{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import utils\n",
    "from utils import bandits, snowden, web\n",
    "reload(utils)\n",
    "reload(utils.web)\n",
    "reload(utils.bandits)\n",
    "reload(utils.snowden)\n",
    "from utils.bandits import exact_gittins\n",
    "\n",
    "from templates import neural as neural_agent\n",
    "reload(neural_agent)\n",
    "\n",
    "neural_agent_class = neural_agent.NeuralAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets = snowden.collect_dataset_from_dir(neural_agent_class, 'sessions/nagiss/20201226/', 1/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25987, 36), (1999, 36))"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0].X.shape, datasets[1].X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean before resampling: 0.31915957978989495\n",
      "mean after resampling: 0.5\n",
      "mean before resampling: 0.3166583291645823\n",
      "mean after resampling: 0.5\n"
     ]
    }
   ],
   "source": [
    "train = snowden.resample_eq(datasets[0])\n",
    "val = snowden.resample_eq(datasets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16588, 36), (1266, 36))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.X.shape, val.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorboardX\n",
      "  Downloading tensorboardX-2.1-py2.py3-none-any.whl (308 kB)\n",
      "\u001b[K     |████████████████████████████████| 308 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/site-packages (from tensorboardX) (1.19.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.9/site-packages (from tensorboardX) (1.15.0)\n",
      "Collecting protobuf>=3.8.0\n",
      "  Downloading protobuf-3.14.0-py2.py3-none-any.whl (173 kB)\n",
      "\u001b[K     |████████████████████████████████| 173 kB 7.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.9/site-packages (from tensorboardX) (1.15.0)\n",
      "Installing collected packages: protobuf, tensorboardX\n",
      "Successfully installed protobuf-3.14.0 tensorboardX-2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dataset(train, val,H=72):\n",
    "    train.X = train.X.reshape(-1, 100, H)\n",
    "    val.X = val.X.reshape(-1, 100, H)\n",
    "\n",
    "    train.actions = train.actions.reshape(-1)\n",
    "    val.actions = val.actions.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(train, val, model_ff, epochs=5, batch_size=64, shuffle=True, freq=10,lr=1e-3,criterion = nn.MSELoss()): \n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    ids_nn = np.arange(train.y.shape[0])\n",
    "    \n",
    "    reshape_to_last = lambda x: torch.reshape(x, [np.prod(x.shape[:-1]), x.shape[-1]])\n",
    "\n",
    "    optimizer = optim.Adam(model_ff.parameters(), lr=lr)\n",
    "\n",
    "    time_for_print_loss = lambda i: (i + 1) % freq == 0\n",
    "    \n",
    "    n_iter = 0\n",
    "    \n",
    "\n",
    "    for epoch in np.arange(epochs):\n",
    "        np.random.shuffle(ids_nn)\n",
    "\n",
    "        model_ff.train(True)\n",
    "\n",
    "        for b in np.arange(0, train.y.shape[0], batch_size):\n",
    "            X_batch = torch.FloatTensor(train.X[ids_nn[b:b+batch_size]])\n",
    "            X_batch = reshape_to_last(X_batch)\n",
    "            y_batch = torch.FloatTensor(train.y[ids_nn[b:b+batch_size]])\n",
    "            a_batch = torch.LongTensor(train.actions[ids_nn[b:b+batch_size]])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred_logits = model_ff(X_batch)\n",
    "\n",
    "            loss = criterion(y_pred_logits, y_batch, a_batch, X_batch)\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if (b // batch_size + 1) % freq == 0:\n",
    "                print('train loss in %d epoch in %d batch: %.5f' %\n",
    "                  (epoch + 1, b // batch_size + 1, loss.item()))\n",
    "                \n",
    "                writer.add_scalar('data/train_loss', loss.item(), n_iter)\n",
    "                writer.add_scalar('data/epoch', epoch + 1, n_iter)\n",
    "                writer.add_scalar('data/batch', b // batch_size + 1, n_iter)\n",
    "\n",
    "                val_loss = 0\n",
    "                its = 0\n",
    "                model_ff.train(False)\n",
    "                for b in np.arange(0, val.y.shape[0], batch_size):\n",
    "                    its += 1\n",
    "                    X_batch = torch.FloatTensor(val.X[b:b+batch_size])\n",
    "                    X_batch = reshape_to_last(X_batch)\n",
    "\n",
    "                    y_batch = torch.FloatTensor(val.y[b:b+batch_size])\n",
    "                    a_batch = torch.LongTensor(val.actions[b:b+batch_size])\n",
    "                    with torch.no_grad():\n",
    "                        y_pred_logits = model_ff(X_batch)\n",
    "                    loss = criterion(y_pred_logits, y_batch, a_batch, X_batch)\n",
    "                    val_loss += loss.item()\n",
    "                val_loss /= its\n",
    "                print('val loss in %d epoch: %.5f' % (epoch + 1, val_loss))\n",
    "                \n",
    "                writer.add_scalar('data/val_loss', val_loss, n_iter)\n",
    "                n_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNWithCusomFeatures(nn.Module):\n",
    "    def __init__(self, INPUT_F, DROP_P, H):\n",
    "        super().__init__()\n",
    "        INPUT_F_C = INPUT_F + 2 * INPUT_F\n",
    "        self.model_ff =  nn.Sequential(\n",
    "            nn.BatchNorm1d(INPUT_F_C),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(INPUT_F_C, H),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, 1)\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lg = torch.log(1 + torch.abs(x))\n",
    "        sn = torch.sin(x)\n",
    "        input_x = torch.cat([x, lg, sn], axis=1)\n",
    "        return self.model_ff(input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 40 batch: 0.695\n",
      "train loss in 1 epoch in 80 batch: 0.680\n",
      "train loss in 1 epoch in 120 batch: 0.696\n",
      "train loss in 1 epoch in 160 batch: 0.661\n",
      "train loss in 1 epoch in 200 batch: 0.702\n",
      "train loss in 1 epoch in 240 batch: 0.665\n",
      "train loss in 1 epoch in 280 batch: 0.645\n",
      "train loss in 1 epoch in 320 batch: 0.680\n",
      "train loss in 1 epoch in 360 batch: 0.683\n",
      "train loss in 1 epoch in 400 batch: 0.670\n",
      "train loss in 1 epoch in 440 batch: 0.689\n",
      "train loss in 1 epoch in 480 batch: 0.630\n",
      "val loss in 1 epoch: 0.657\n",
      "train loss in 2 epoch in 40 batch: 0.648\n",
      "train loss in 2 epoch in 80 batch: 0.672\n",
      "train loss in 2 epoch in 120 batch: 0.593\n",
      "train loss in 2 epoch in 160 batch: 0.685\n",
      "train loss in 2 epoch in 200 batch: 0.696\n",
      "train loss in 2 epoch in 240 batch: 0.631\n",
      "train loss in 2 epoch in 280 batch: 0.621\n",
      "train loss in 2 epoch in 320 batch: 0.661\n",
      "train loss in 2 epoch in 360 batch: 0.696\n",
      "train loss in 2 epoch in 400 batch: 0.569\n",
      "train loss in 2 epoch in 440 batch: 0.673\n",
      "train loss in 2 epoch in 480 batch: 0.736\n",
      "val loss in 2 epoch: 0.644\n",
      "train loss in 3 epoch in 40 batch: 0.713\n",
      "train loss in 3 epoch in 80 batch: 0.654\n",
      "train loss in 3 epoch in 120 batch: 0.570\n",
      "train loss in 3 epoch in 160 batch: 0.605\n",
      "train loss in 3 epoch in 200 batch: 0.610\n",
      "train loss in 3 epoch in 240 batch: 0.625\n",
      "train loss in 3 epoch in 280 batch: 0.605\n",
      "train loss in 3 epoch in 320 batch: 0.574\n",
      "train loss in 3 epoch in 360 batch: 0.659\n",
      "train loss in 3 epoch in 400 batch: 0.626\n",
      "train loss in 3 epoch in 440 batch: 0.681\n",
      "train loss in 3 epoch in 480 batch: 0.601\n",
      "val loss in 3 epoch: 0.639\n",
      "train loss in 4 epoch in 40 batch: 0.565\n",
      "train loss in 4 epoch in 80 batch: 0.577\n",
      "train loss in 4 epoch in 120 batch: 0.671\n",
      "train loss in 4 epoch in 160 batch: 0.648\n",
      "train loss in 4 epoch in 200 batch: 0.619\n",
      "train loss in 4 epoch in 240 batch: 0.769\n",
      "train loss in 4 epoch in 280 batch: 0.708\n",
      "train loss in 4 epoch in 320 batch: 0.683\n",
      "train loss in 4 epoch in 360 batch: 0.598\n",
      "train loss in 4 epoch in 400 batch: 0.592\n",
      "train loss in 4 epoch in 440 batch: 0.583\n",
      "train loss in 4 epoch in 480 batch: 0.679\n",
      "val loss in 4 epoch: 0.638\n",
      "train loss in 5 epoch in 40 batch: 0.617\n",
      "train loss in 5 epoch in 80 batch: 0.671\n",
      "train loss in 5 epoch in 120 batch: 0.603\n",
      "train loss in 5 epoch in 160 batch: 0.592\n",
      "train loss in 5 epoch in 200 batch: 0.620\n",
      "train loss in 5 epoch in 240 batch: 0.635\n",
      "train loss in 5 epoch in 280 batch: 0.701\n",
      "train loss in 5 epoch in 320 batch: 0.629\n",
      "train loss in 5 epoch in 360 batch: 0.612\n",
      "train loss in 5 epoch in 400 batch: 0.629\n",
      "train loss in 5 epoch in 440 batch: 0.667\n",
      "train loss in 5 epoch in 480 batch: 0.591\n",
      "val loss in 5 epoch: 0.637\n",
      "train loss in 6 epoch in 40 batch: 0.551\n",
      "train loss in 6 epoch in 80 batch: 0.655\n",
      "train loss in 6 epoch in 120 batch: 0.557\n",
      "train loss in 6 epoch in 160 batch: 0.637\n",
      "train loss in 6 epoch in 200 batch: 0.636\n",
      "train loss in 6 epoch in 240 batch: 0.588\n",
      "train loss in 6 epoch in 280 batch: 0.606\n",
      "train loss in 6 epoch in 320 batch: 0.660\n",
      "train loss in 6 epoch in 360 batch: 0.591\n",
      "train loss in 6 epoch in 400 batch: 0.587\n",
      "train loss in 6 epoch in 440 batch: 0.636\n",
      "train loss in 6 epoch in 480 batch: 0.658\n",
      "val loss in 6 epoch: 0.637\n",
      "train loss in 7 epoch in 40 batch: 0.651\n",
      "train loss in 7 epoch in 80 batch: 0.632\n",
      "train loss in 7 epoch in 120 batch: 0.636\n",
      "train loss in 7 epoch in 160 batch: 0.696\n",
      "train loss in 7 epoch in 200 batch: 0.652\n",
      "train loss in 7 epoch in 240 batch: 0.659\n",
      "train loss in 7 epoch in 280 batch: 0.660\n",
      "train loss in 7 epoch in 320 batch: 0.551\n",
      "train loss in 7 epoch in 360 batch: 0.624\n",
      "train loss in 7 epoch in 400 batch: 0.703\n",
      "train loss in 7 epoch in 440 batch: 0.631\n",
      "train loss in 7 epoch in 480 batch: 0.634\n",
      "val loss in 7 epoch: 0.636\n"
     ]
    }
   ],
   "source": [
    "INPUT_F = 36\n",
    "H = 256\n",
    "DROP_P = 0.05\n",
    "\n",
    "model = NNWithCusomFeatures(INPUT_F, DROP_P, H)\n",
    "\n",
    "learn(train, val, model, freq=40, batch_size=32,lr=1e-5, epochs=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 40 batch: 0.246\n",
      "train loss in 1 epoch in 80 batch: 0.244\n",
      "train loss in 1 epoch in 120 batch: 0.248\n",
      "train loss in 1 epoch in 160 batch: 0.233\n",
      "train loss in 1 epoch in 200 batch: 0.242\n",
      "train loss in 1 epoch in 240 batch: 0.238\n",
      "train loss in 1 epoch in 280 batch: 0.239\n",
      "train loss in 1 epoch in 320 batch: 0.235\n",
      "train loss in 1 epoch in 360 batch: 0.245\n",
      "train loss in 1 epoch in 400 batch: 0.240\n",
      "train loss in 1 epoch in 440 batch: 0.248\n",
      "train loss in 1 epoch in 480 batch: 0.216\n",
      "val loss in 1 epoch: 0.234\n",
      "train loss in 2 epoch in 40 batch: 0.234\n",
      "train loss in 2 epoch in 80 batch: 0.233\n",
      "train loss in 2 epoch in 120 batch: 0.199\n",
      "train loss in 2 epoch in 160 batch: 0.247\n",
      "train loss in 2 epoch in 200 batch: 0.256\n",
      "train loss in 2 epoch in 240 batch: 0.219\n",
      "train loss in 2 epoch in 280 batch: 0.221\n",
      "train loss in 2 epoch in 320 batch: 0.227\n",
      "train loss in 2 epoch in 360 batch: 0.239\n",
      "train loss in 2 epoch in 400 batch: 0.194\n",
      "train loss in 2 epoch in 440 batch: 0.240\n",
      "train loss in 2 epoch in 480 batch: 0.272\n",
      "val loss in 2 epoch: 0.227\n",
      "train loss in 3 epoch in 40 batch: 0.251\n",
      "train loss in 3 epoch in 80 batch: 0.228\n",
      "train loss in 3 epoch in 120 batch: 0.198\n",
      "train loss in 3 epoch in 160 batch: 0.208\n",
      "train loss in 3 epoch in 200 batch: 0.209\n",
      "train loss in 3 epoch in 240 batch: 0.221\n",
      "train loss in 3 epoch in 280 batch: 0.213\n",
      "train loss in 3 epoch in 320 batch: 0.201\n",
      "train loss in 3 epoch in 360 batch: 0.231\n",
      "train loss in 3 epoch in 400 batch: 0.220\n",
      "train loss in 3 epoch in 440 batch: 0.244\n",
      "train loss in 3 epoch in 480 batch: 0.213\n",
      "val loss in 3 epoch: 0.224\n",
      "train loss in 4 epoch in 40 batch: 0.194\n",
      "train loss in 4 epoch in 80 batch: 0.192\n",
      "train loss in 4 epoch in 120 batch: 0.241\n",
      "train loss in 4 epoch in 160 batch: 0.233\n",
      "train loss in 4 epoch in 200 batch: 0.214\n",
      "train loss in 4 epoch in 240 batch: 0.279\n",
      "train loss in 4 epoch in 280 batch: 0.260\n",
      "train loss in 4 epoch in 320 batch: 0.244\n",
      "train loss in 4 epoch in 360 batch: 0.205\n",
      "train loss in 4 epoch in 400 batch: 0.205\n",
      "train loss in 4 epoch in 440 batch: 0.196\n",
      "train loss in 4 epoch in 480 batch: 0.236\n",
      "val loss in 4 epoch: 0.223\n",
      "train loss in 5 epoch in 40 batch: 0.213\n",
      "train loss in 5 epoch in 80 batch: 0.243\n",
      "train loss in 5 epoch in 120 batch: 0.207\n",
      "train loss in 5 epoch in 160 batch: 0.204\n",
      "train loss in 5 epoch in 200 batch: 0.217\n",
      "train loss in 5 epoch in 240 batch: 0.230\n",
      "train loss in 5 epoch in 280 batch: 0.260\n",
      "train loss in 5 epoch in 320 batch: 0.225\n",
      "train loss in 5 epoch in 360 batch: 0.216\n",
      "train loss in 5 epoch in 400 batch: 0.223\n",
      "train loss in 5 epoch in 440 batch: 0.238\n",
      "train loss in 5 epoch in 480 batch: 0.196\n",
      "val loss in 5 epoch: 0.223\n"
     ]
    }
   ],
   "source": [
    "# mse loss\n",
    "INPUT_F = 36\n",
    "H = 256\n",
    "DROP_P = 0.05\n",
    "\n",
    "model = NNWithCusomFeatures(INPUT_F, DROP_P, H)\n",
    "\n",
    "learn(train, val, model, freq=40, batch_size=32,lr=1e-5, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "632.0 611.0 tmp/b_0.9817500612110714.py tmp/b_0.6737446183327064.py\n",
      "676.0 642.0 tmp/b_0.9817500612110714.py tmp/b_0.6737446183327064.py\n",
      "599.0 543.0 tmp/b_0.9817500612110714.py tmp/b_0.6737446183327064.py\n",
      "624.0 621.0 tmp/b_0.9817500612110714.py tmp/b_0.6737446183327064.py\n",
      "606.0 639.0 tmp/b_0.9817500612110714.py tmp/b_0.6737446183327064.py\n",
      "607.0 591.0 tmp/b_0.9817500612110714.py tmp/b_0.6737446183327064.py\n",
      "609.0 552.0 tmp/b_0.9817500612110714.py tmp/b_0.6737446183327064.py\n",
      "626.0 606.0 tmp/b_0.9817500612110714.py tmp/b_0.6737446183327064.py\n",
      "630.0 603.0 tmp/b_0.9817500612110714.py tmp/b_0.6737446183327064.py\n",
      "615.0 615.0 tmp/b_0.9817500612110714.py tmp/b_0.6737446183327064.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.012154502623372141,\n",
       " 20.1,\n",
       " 25.34738645304482,\n",
       " 0.9,\n",
       " 'tmp/b_0.9817500612110714.py',\n",
       " 'tmp/b_0.6737446183327064.py',\n",
       " array([632., 676., 599., 624., 606., 607., 609., 626., 630., 615.]),\n",
       " array([611., 642., 543., 621., 639., 591., 552., 606., 603., 615.]))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v1\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss=True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=utils.bandits.neural),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "678.0 661.0 tmp/b_0.054213267519393926.py tmp/b_0.236142134925876.py\n",
      "700.0 695.0 tmp/b_0.054213267519393926.py tmp/b_0.236142134925876.py\n",
      "664.0 660.0 tmp/b_0.054213267519393926.py tmp/b_0.236142134925876.py\n",
      "597.0 615.0 tmp/b_0.054213267519393926.py tmp/b_0.236142134925876.py\n",
      "659.0 696.0 tmp/b_0.054213267519393926.py tmp/b_0.236142134925876.py\n",
      "601.0 581.0 tmp/b_0.054213267519393926.py tmp/b_0.236142134925876.py\n",
      "626.0 623.0 tmp/b_0.054213267519393926.py tmp/b_0.236142134925876.py\n",
      "536.0 607.0 tmp/b_0.054213267519393926.py tmp/b_0.236142134925876.py\n",
      "580.0 550.0 tmp/b_0.054213267519393926.py tmp/b_0.236142134925876.py\n",
      "636.0 562.0 tmp/b_0.054213267519393926.py tmp/b_0.236142134925876.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8180618145466889,\n",
       " 2.7,\n",
       " 37.11616898334202,\n",
       " 0.7,\n",
       " 'tmp/b_0.054213267519393926.py',\n",
       " 'tmp/b_0.236142134925876.py',\n",
       " array([678., 700., 664., 597., 659., 601., 626., 536., 580., 636.]),\n",
       " array([661., 695., 660., 615., 696., 581., 623., 607., 550., 562.]))"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v2\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss=True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text= utils.bandits.neural),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657.0 685.0 tmp/b_0.4681855452475341.py tmp/b_0.815911715993058.py\n",
      "717.0 717.0 tmp/b_0.4681855452475341.py tmp/b_0.815911715993058.py\n",
      "691.0 721.0 tmp/b_0.4681855452475341.py tmp/b_0.815911715993058.py\n",
      "629.0 671.0 tmp/b_0.4681855452475341.py tmp/b_0.815911715993058.py\n",
      "624.0 662.0 tmp/b_0.4681855452475341.py tmp/b_0.815911715993058.py\n",
      "677.0 627.0 tmp/b_0.4681855452475341.py tmp/b_0.815911715993058.py\n",
      "579.0 572.0 tmp/b_0.4681855452475341.py tmp/b_0.815911715993058.py\n",
      "594.0 621.0 tmp/b_0.4681855452475341.py tmp/b_0.815911715993058.py\n",
      "617.0 593.0 tmp/b_0.4681855452475341.py tmp/b_0.815911715993058.py\n",
      "629.0 634.0 tmp/b_0.4681855452475341.py tmp/b_0.815911715993058.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.31907279631279994,\n",
       " -8.9,\n",
       " 28.246946737656444,\n",
       " 0.4,\n",
       " 'tmp/b_0.4681855452475341.py',\n",
       " 'tmp/b_0.815911715993058.py',\n",
       " array([657., 717., 691., 629., 624., 677., 579., 594., 617., 629.]),\n",
       " array([685., 717., 721., 671., 662., 627., 572., 621., 593., 634.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v2\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss=True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text= utils.bandits.neural),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all nagiss games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sessions/nagiss/20201226_v2/ ['sessions/nagiss/20201226_v2/7994638.json', 'sessions/nagiss/20201226_v2/7991093.json', 'sessions/nagiss/20201226_v2/7988820.json', 'sessions/nagiss/20201226_v2/8386716.json', 'sessions/nagiss/20201226_v2/8038792.json', 'sessions/nagiss/20201226_v2/8233786.json', 'sessions/nagiss/20201226_v2/8073493.json', 'sessions/nagiss/20201226_v2/8012231.json', 'sessions/nagiss/20201226_v2/7995268.json', 'sessions/nagiss/20201226_v2/8046220.json', 'sessions/nagiss/20201226_v2/8321309.json', 'sessions/nagiss/20201226_v2/8102725.json', 'sessions/nagiss/20201226_v2/8146617.json', 'sessions/nagiss/20201226_v2/7986639.json', 'sessions/nagiss/20201226_v2/8029737.json', 'sessions/nagiss/20201226_v2/8089900.json', 'sessions/nagiss/20201226_v2/8354985.json', 'sessions/nagiss/20201226_v2/7995644.json', 'sessions/nagiss/20201226_v2/7998867.json', 'sessions/nagiss/20201226_v2/8257767.json', 'sessions/nagiss/20201226_v2/7992085.json', 'sessions/nagiss/20201226_v2/7987842.json', 'sessions/nagiss/20201226_v2/8172893.json', 'sessions/nagiss/20201226_v2/8201003.json', 'sessions/nagiss/20201226_v2/8009354.json', 'sessions/nagiss/20201226_v2/7988519.json', 'sessions/nagiss/20201226_v2/7987183.json', 'sessions/nagiss/20201226_v2/7992694.json', 'sessions/nagiss/20201226_v2/7991811.json', 'sessions/nagiss/20201226_v2/8464420.json', 'sessions/nagiss/20201226_v2/8186631.json', 'sessions/nagiss/20201226_v2/8054877.json', 'sessions/nagiss/20201226_v2/8249308.json', 'sessions/nagiss/20201226_v2/8273034.json', 'sessions/nagiss/20201226_v2/7987567.json', 'sessions/nagiss/20201226_v2/8371128.json', 'sessions/nagiss/20201226_v2/8060412.json', 'sessions/nagiss/20201226_v2/8541194.json', 'sessions/nagiss/20201226_v2/8481316.json', 'sessions/nagiss/20201226_v2/7994974.json', 'sessions/nagiss/20201226_v2/8132606.json', 'sessions/nagiss/20201226_v2/7989777.json', 'sessions/nagiss/20201226_v2/7992382.json', 'sessions/nagiss/20201226_v2/8160880.json', 'sessions/nagiss/20201226_v2/8005498.json', 'sessions/nagiss/20201226_v2/8337440.json', 'sessions/nagiss/20201226_v2/8008069.json', 'sessions/nagiss/20201226_v2/8304463.json', 'sessions/nagiss/20201226_v2/7994017.json', 'sessions/nagiss/20201226_v2/7995939.json', 'sessions/nagiss/20201226_v2/8510303.json', 'sessions/nagiss/20201226_v2/8433263.json', 'sessions/nagiss/20201226_v2/7996231.json', 'sessions/nagiss/20201226_v2/7996548.json', 'sessions/nagiss/20201226_v2/8449014.json', 'sessions/nagiss/20201226_v2/7988184.json', 'sessions/nagiss/20201226_v2/8215761.json', 'sessions/nagiss/20201226_v2/7993014.json', 'sessions/nagiss/20201226_v2/7989458.json', 'sessions/nagiss/20201226_v2/7990100.json', 'sessions/nagiss/20201226_v2/8288389.json', 'sessions/nagiss/20201226_v2/7991414.json', 'sessions/nagiss/20201226_v2/7994342.json', 'sessions/nagiss/20201226_v2/8417594.json', 'sessions/nagiss/20201226_v2/7991731.json', 'sessions/nagiss/20201226_v2/8402256.json', 'sessions/nagiss/20201226_v2/8002300.json', 'sessions/nagiss/20201226_v2/8495892.json', 'sessions/nagiss/20201226_v2/8557211.json', 'sessions/nagiss/20201226_v2/8073192.json', 'sessions/nagiss/20201226_v2/8118585.json', 'sessions/nagiss/20201226_v2/8016051.json', 'sessions/nagiss/20201226_v2/7993371.json', 'sessions/nagiss/20201226_v2/7989156.json', 'sessions/nagiss/20201226_v2/7990750.json', 'sessions/nagiss/20201226_v2/8021586.json', 'sessions/nagiss/20201226_v2/7993701.json', 'sessions/nagiss/20201226_v2/7990443.json', 'sessions/nagiss/20201226_v2/8079696.json', 'sessions/nagiss/20201226_v2/7987514.json', 'sessions/nagiss/20201226_v2/8525948.json']\n",
      "error while parse session sessions/nagiss/20201226_v2/8354985.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/7991811.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/8054877.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/8005498.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/7995939.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/7991414.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/8002300.json : \n"
     ]
    }
   ],
   "source": [
    "datasets = snowden.collect_dataset_from_dir(neural_agent_class, 'sessions/nagiss/20201226_v2/', 1/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((133933, 36), (13993, 36))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0].X.shape, datasets[1].X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean before resampling: 0.31643433657127074\n",
      "mean after resampling: 0.5\n",
      "mean before resampling: 0.3148002572714929\n",
      "mean after resampling: 0.5\n"
     ]
    }
   ],
   "source": [
    "train = snowden.resample_eq(datasets[0])\n",
    "val = snowden.resample_eq(datasets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84762, 36), (8810, 36))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.X.shape, val.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class NNWithCusomFeatures(nn.Module):\n",
    "#     def __init__(self, INPUT_F, DROP_P, H):\n",
    "#         super().__init__()\n",
    "#         INPUT_F_C = INPUT_F + 2 * INPUT_F\n",
    "#         self.model_ff_1 =  nn.Sequential(\n",
    "#             nn.BatchNorm1d(INPUT_F_C),\n",
    "#             nn.Dropout(DROP_P),\n",
    "#             nn.Linear(INPUT_F_C, H),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Dropout(DROP_P)\n",
    "#         )\n",
    "#         self.model_ff_2 =  nn.Sequential(\n",
    "#             nn.Linear(H, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         lg = torch.log(1 + torch.abs(x))\n",
    "#         sn = torch.sin(x)\n",
    "#         input_x = torch.cat([x, lg, sn], axis=1)\n",
    "#         out1 =  self.model_ff_1(input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 500 batch: 0.24326\n",
      "train loss in 1 epoch in 1000 batch: 0.23294\n",
      "train loss in 1 epoch in 1500 batch: 0.25006\n",
      "train loss in 1 epoch in 2000 batch: 0.20686\n",
      "train loss in 1 epoch in 2500 batch: 0.21678\n",
      "val loss in 1 epoch: 0.22331\n",
      "train loss in 2 epoch in 500 batch: 0.26177\n",
      "train loss in 2 epoch in 1000 batch: 0.19144\n",
      "train loss in 2 epoch in 1500 batch: 0.17022\n",
      "train loss in 2 epoch in 2000 batch: 0.23671\n",
      "train loss in 2 epoch in 2500 batch: 0.22735\n",
      "val loss in 2 epoch: 0.22240\n",
      "train loss in 3 epoch in 500 batch: 0.18364\n",
      "train loss in 3 epoch in 1000 batch: 0.25060\n",
      "train loss in 3 epoch in 1500 batch: 0.25504\n",
      "train loss in 3 epoch in 2000 batch: 0.21704\n",
      "train loss in 3 epoch in 2500 batch: 0.20117\n",
      "val loss in 3 epoch: 0.22189\n",
      "train loss in 4 epoch in 500 batch: 0.17385\n",
      "train loss in 4 epoch in 1000 batch: 0.22882\n",
      "train loss in 4 epoch in 1500 batch: 0.22496\n",
      "train loss in 4 epoch in 2000 batch: 0.20567\n",
      "train loss in 4 epoch in 2500 batch: 0.22413\n",
      "val loss in 4 epoch: 0.22164\n",
      "train loss in 5 epoch in 500 batch: 0.18633\n",
      "train loss in 5 epoch in 1000 batch: 0.23591\n",
      "train loss in 5 epoch in 1500 batch: 0.19090\n",
      "train loss in 5 epoch in 2000 batch: 0.20379\n",
      "train loss in 5 epoch in 2500 batch: 0.21904\n",
      "val loss in 5 epoch: 0.22131\n"
     ]
    }
   ],
   "source": [
    "# mse loss\n",
    "INPUT_F = 36\n",
    "H = 256\n",
    "DROP_P = 0.05\n",
    "\n",
    "model = NNWithCusomFeatures(INPUT_F, DROP_P, H)\n",
    "\n",
    "learn(train, val, model, freq=500, batch_size=32,lr=1e-5, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "619.0 538.0 tmp/b_0.5394941534468733.py tmp/b_0.9677390896108925.py\n",
      "552.0 574.0 tmp/b_0.5394941534468733.py tmp/b_0.9677390896108925.py\n",
      "641.0 652.0 tmp/b_0.5394941534468733.py tmp/b_0.9677390896108925.py\n",
      "631.0 601.0 tmp/b_0.5394941534468733.py tmp/b_0.9677390896108925.py\n",
      "703.0 651.0 tmp/b_0.5394941534468733.py tmp/b_0.9677390896108925.py\n",
      "643.0 666.0 tmp/b_0.5394941534468733.py tmp/b_0.9677390896108925.py\n",
      "617.0 628.0 tmp/b_0.5394941534468733.py tmp/b_0.9677390896108925.py\n",
      "607.0 574.0 tmp/b_0.5394941534468733.py tmp/b_0.9677390896108925.py\n",
      "638.0 660.0 tmp/b_0.5394941534468733.py tmp/b_0.9677390896108925.py\n",
      "618.0 632.0 tmp/b_0.5394941534468733.py tmp/b_0.9677390896108925.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4021809806045401,\n",
       " 9.3,\n",
       " 35.105697543276364,\n",
       " 0.4,\n",
       " 'tmp/b_0.5394941534468733.py',\n",
       " 'tmp/b_0.9677390896108925.py',\n",
       " array([619., 552., 641., 631., 703., 643., 617., 607., 638., 618.]),\n",
       " array([538., 574., 652., 601., 651., 666., 628., 574., 660., 632.]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v3\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss=True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=utils.bandits.neural),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sessions/nagiss/20201226_v2/ ['sessions/nagiss/20201226_v2/7994638.json', 'sessions/nagiss/20201226_v2/7991093.json', 'sessions/nagiss/20201226_v2/7988820.json', 'sessions/nagiss/20201226_v2/8386716.json', 'sessions/nagiss/20201226_v2/8038792.json', 'sessions/nagiss/20201226_v2/8233786.json', 'sessions/nagiss/20201226_v2/8073493.json', 'sessions/nagiss/20201226_v2/8012231.json', 'sessions/nagiss/20201226_v2/7995268.json', 'sessions/nagiss/20201226_v2/8046220.json', 'sessions/nagiss/20201226_v2/8321309.json', 'sessions/nagiss/20201226_v2/8102725.json', 'sessions/nagiss/20201226_v2/8146617.json', 'sessions/nagiss/20201226_v2/7986639.json', 'sessions/nagiss/20201226_v2/8029737.json', 'sessions/nagiss/20201226_v2/8089900.json', 'sessions/nagiss/20201226_v2/8354985.json', 'sessions/nagiss/20201226_v2/7995644.json', 'sessions/nagiss/20201226_v2/7998867.json', 'sessions/nagiss/20201226_v2/8257767.json', 'sessions/nagiss/20201226_v2/7992085.json', 'sessions/nagiss/20201226_v2/7987842.json', 'sessions/nagiss/20201226_v2/8172893.json', 'sessions/nagiss/20201226_v2/8201003.json', 'sessions/nagiss/20201226_v2/8009354.json', 'sessions/nagiss/20201226_v2/7988519.json', 'sessions/nagiss/20201226_v2/7987183.json', 'sessions/nagiss/20201226_v2/7992694.json', 'sessions/nagiss/20201226_v2/7991811.json', 'sessions/nagiss/20201226_v2/8464420.json', 'sessions/nagiss/20201226_v2/8186631.json', 'sessions/nagiss/20201226_v2/8054877.json', 'sessions/nagiss/20201226_v2/8249308.json', 'sessions/nagiss/20201226_v2/8273034.json', 'sessions/nagiss/20201226_v2/7987567.json', 'sessions/nagiss/20201226_v2/8371128.json', 'sessions/nagiss/20201226_v2/8060412.json', 'sessions/nagiss/20201226_v2/8541194.json', 'sessions/nagiss/20201226_v2/8481316.json', 'sessions/nagiss/20201226_v2/7994974.json', 'sessions/nagiss/20201226_v2/8132606.json', 'sessions/nagiss/20201226_v2/7989777.json', 'sessions/nagiss/20201226_v2/7992382.json', 'sessions/nagiss/20201226_v2/8160880.json', 'sessions/nagiss/20201226_v2/8005498.json', 'sessions/nagiss/20201226_v2/8337440.json', 'sessions/nagiss/20201226_v2/8008069.json', 'sessions/nagiss/20201226_v2/8304463.json', 'sessions/nagiss/20201226_v2/7994017.json', 'sessions/nagiss/20201226_v2/7995939.json', 'sessions/nagiss/20201226_v2/8510303.json', 'sessions/nagiss/20201226_v2/8433263.json', 'sessions/nagiss/20201226_v2/7996231.json', 'sessions/nagiss/20201226_v2/7996548.json', 'sessions/nagiss/20201226_v2/8449014.json', 'sessions/nagiss/20201226_v2/7988184.json', 'sessions/nagiss/20201226_v2/8215761.json', 'sessions/nagiss/20201226_v2/7993014.json', 'sessions/nagiss/20201226_v2/7989458.json', 'sessions/nagiss/20201226_v2/7990100.json', 'sessions/nagiss/20201226_v2/8288389.json', 'sessions/nagiss/20201226_v2/7991414.json', 'sessions/nagiss/20201226_v2/7994342.json', 'sessions/nagiss/20201226_v2/8417594.json', 'sessions/nagiss/20201226_v2/7991731.json', 'sessions/nagiss/20201226_v2/8402256.json', 'sessions/nagiss/20201226_v2/8002300.json', 'sessions/nagiss/20201226_v2/8495892.json', 'sessions/nagiss/20201226_v2/8557211.json', 'sessions/nagiss/20201226_v2/8073192.json', 'sessions/nagiss/20201226_v2/8118585.json', 'sessions/nagiss/20201226_v2/8016051.json', 'sessions/nagiss/20201226_v2/7993371.json', 'sessions/nagiss/20201226_v2/7989156.json', 'sessions/nagiss/20201226_v2/7990750.json', 'sessions/nagiss/20201226_v2/8021586.json', 'sessions/nagiss/20201226_v2/7993701.json', 'sessions/nagiss/20201226_v2/7990443.json', 'sessions/nagiss/20201226_v2/8079696.json', 'sessions/nagiss/20201226_v2/7987514.json', 'sessions/nagiss/20201226_v2/8525948.json', 'sessions/nagiss/20201226_v2/.ipynb_checkpoints/8002300-checkpoint.json']\n",
      "error while parse session sessions/nagiss/20201226_v2/8354985.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/7991811.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/8054877.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/8005498.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/7995939.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/7991414.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/8002300.json : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error while parse session sessions/nagiss/20201226_v2/.ipynb_checkpoints/8002300-checkpoint.json : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [12:54<00:00, 11.56s/it]\n",
      "100%|██████████| 7/7 [01:19<00:00, 11.38s/it]\n"
     ]
    }
   ],
   "source": [
    "#with mean features\n",
    "\n",
    "datasets = snowden.collect_dataset_from_dir(neural_agent_class, 'sessions/nagiss/20201226_v2/', 1/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((133933, 72), (13993, 72))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0].X.shape, datasets[1].X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean before resampling: 0.3157548923715589\n",
      "mean after resampling: 0.5\n",
      "mean before resampling: 0.3213035088973058\n",
      "mean after resampling: 0.5\n"
     ]
    }
   ],
   "source": [
    "train = snowden.resample_eq(datasets[0])\n",
    "val = snowden.resample_eq(datasets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 50 batch: 0.22536\n",
      "train loss in 1 epoch in 100 batch: 0.22169\n",
      "train loss in 1 epoch in 150 batch: 0.21847\n",
      "val loss in 1 epoch: 0.22192\n",
      "train loss in 2 epoch in 50 batch: 0.21630\n",
      "train loss in 2 epoch in 100 batch: 0.22580\n",
      "train loss in 2 epoch in 150 batch: 0.22800\n",
      "val loss in 2 epoch: 0.22128\n",
      "train loss in 3 epoch in 50 batch: 0.21413\n",
      "train loss in 3 epoch in 100 batch: 0.22134\n",
      "train loss in 3 epoch in 150 batch: 0.22260\n",
      "val loss in 3 epoch: 0.22139\n",
      "train loss in 4 epoch in 50 batch: 0.21507\n",
      "train loss in 4 epoch in 100 batch: 0.21997\n",
      "train loss in 4 epoch in 150 batch: 0.22291\n",
      "val loss in 4 epoch: 0.22118\n",
      "train loss in 5 epoch in 50 batch: 0.22766\n",
      "train loss in 5 epoch in 100 batch: 0.22133\n",
      "train loss in 5 epoch in 150 batch: 0.21585\n",
      "val loss in 5 epoch: 0.22130\n",
      "train loss in 6 epoch in 50 batch: 0.22414\n",
      "train loss in 6 epoch in 100 batch: 0.21673\n",
      "train loss in 6 epoch in 150 batch: 0.21959\n",
      "val loss in 6 epoch: 0.22122\n",
      "train loss in 7 epoch in 50 batch: 0.21426\n",
      "train loss in 7 epoch in 100 batch: 0.22093\n",
      "train loss in 7 epoch in 150 batch: 0.23346\n",
      "val loss in 7 epoch: 0.22143\n",
      "train loss in 8 epoch in 50 batch: 0.21513\n",
      "train loss in 8 epoch in 100 batch: 0.22357\n",
      "train loss in 8 epoch in 150 batch: 0.21298\n",
      "val loss in 8 epoch: 0.22112\n",
      "train loss in 9 epoch in 50 batch: 0.22280\n",
      "train loss in 9 epoch in 100 batch: 0.23197\n",
      "train loss in 9 epoch in 150 batch: 0.23310\n",
      "val loss in 9 epoch: 0.22148\n",
      "train loss in 10 epoch in 50 batch: 0.21593\n",
      "train loss in 10 epoch in 100 batch: 0.21463\n",
      "train loss in 10 epoch in 150 batch: 0.22527\n",
      "val loss in 10 epoch: 0.22109\n"
     ]
    }
   ],
   "source": [
    "# mse loss\n",
    "INPUT_F = 72\n",
    "H = 256\n",
    "DROP_P = 0.05\n",
    "\n",
    "model = NNWithCusomFeatures(INPUT_F, DROP_P, H)\n",
    "\n",
    "learn(train, val, model, freq=50, batch_size=512,lr=1e-4, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "364.0 742.0 tmp/b_0.624265024014975.py tmp/b_0.8223440142039435.py\n",
      "395.0 686.0 tmp/b_0.624265024014975.py tmp/b_0.8223440142039435.py\n",
      "323.0 666.0 tmp/b_0.624265024014975.py tmp/b_0.8223440142039435.py\n",
      "390.0 753.0 tmp/b_0.624265024014975.py tmp/b_0.8223440142039435.py\n",
      "394.0 618.0 tmp/b_0.624265024014975.py tmp/b_0.8223440142039435.py\n",
      "366.0 663.0 tmp/b_0.624265024014975.py tmp/b_0.8223440142039435.py\n",
      "374.0 620.0 tmp/b_0.624265024014975.py tmp/b_0.8223440142039435.py\n",
      "398.0 731.0 tmp/b_0.624265024014975.py tmp/b_0.8223440142039435.py\n",
      "380.0 682.0 tmp/b_0.624265024014975.py tmp/b_0.8223440142039435.py\n",
      "386.0 745.0 tmp/b_0.624265024014975.py tmp/b_0.8223440142039435.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.2900685246041295e-93,\n",
       " -313.6,\n",
       " 48.3822281421598,\n",
       " 0.0,\n",
       " 'tmp/b_0.624265024014975.py',\n",
       " 'tmp/b_0.8223440142039435.py',\n",
       " array([364., 395., 323., 390., 394., 366., 374., 398., 380., 386.]),\n",
       " array([742., 686., 666., 753., 618., 663., 620., 731., 682., 745.]))"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v4\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,True\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=False\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # v4 vs default + mean features\n",
    "# utils.bandits.compare(\n",
    "#     utils.bandits.Agent(text=neural_with_nagiss),\n",
    "#     utils.bandits.Agent(text=utils.bandits.neural),\n",
    "#     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "637.0 624.0 tmp/b_0.00783945348032078.py tmp/b_0.08316848165655322.py\n",
      "652.0 693.0 tmp/b_0.00783945348032078.py tmp/b_0.08316848165655322.py\n",
      "600.0 628.0 tmp/b_0.00783945348032078.py tmp/b_0.08316848165655322.py\n",
      "597.0 611.0 tmp/b_0.00783945348032078.py tmp/b_0.08316848165655322.py\n",
      "646.0 634.0 tmp/b_0.00783945348032078.py tmp/b_0.08316848165655322.py\n",
      "624.0 627.0 tmp/b_0.00783945348032078.py tmp/b_0.08316848165655322.py\n",
      "595.0 599.0 tmp/b_0.00783945348032078.py tmp/b_0.08316848165655322.py\n",
      "605.0 596.0 tmp/b_0.00783945348032078.py tmp/b_0.08316848165655322.py\n",
      "664.0 672.0 tmp/b_0.00783945348032078.py tmp/b_0.08316848165655322.py\n",
      "628.0 608.0 tmp/b_0.00783945348032078.py tmp/b_0.08316848165655322.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.44715964885505466,\n",
       " -4.4,\n",
       " 18.30409790183608,\n",
       " 0.4,\n",
       " 'tmp/b_0.00783945348032078.py',\n",
       " 'tmp/b_0.08316848165655322.py',\n",
       " array([637., 652., 600., 597., 646., 624., 595., 605., 664., 628.]),\n",
       " array([624., 693., 628., 611., 634., 627., 599., 596., 672., 608.]))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v3 as feature\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_feature_nagiss=True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=utils.bandits.neural),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617.0 640.0 tmp/b_0.2329175121645335.py tmp/b_0.9947346263005474.py\n",
      "644.0 619.0 tmp/b_0.2329175121645335.py tmp/b_0.9947346263005474.py\n",
      "679.0 694.0 tmp/b_0.2329175121645335.py tmp/b_0.9947346263005474.py\n",
      "645.0 617.0 tmp/b_0.2329175121645335.py tmp/b_0.9947346263005474.py\n",
      "601.0 629.0 tmp/b_0.2329175121645335.py tmp/b_0.9947346263005474.py\n",
      "628.0 617.0 tmp/b_0.2329175121645335.py tmp/b_0.9947346263005474.py\n",
      "639.0 634.0 tmp/b_0.2329175121645335.py tmp/b_0.9947346263005474.py\n",
      "651.0 658.0 tmp/b_0.2329175121645335.py tmp/b_0.9947346263005474.py\n",
      "736.0 647.0 tmp/b_0.2329175121645335.py tmp/b_0.9947346263005474.py\n",
      "586.0 588.0 tmp/b_0.2329175121645335.py tmp/b_0.9947346263005474.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.41522101422787383,\n",
       " 8.3,\n",
       " 32.21505859066533,\n",
       " 0.5,\n",
       " 'tmp/b_0.2329175121645335.py',\n",
       " 'tmp/b_0.9947346263005474.py',\n",
       " array([617., 644., 679., 645., 601., 628., 639., 651., 736., 586.]),\n",
       " array([640., 619., 694., 617., 629., 617., 634., 658., 647., 588.]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v2 as feature\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_feature_nagiss=True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=utils.bandits.neural),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "627.0 664.0 tmp/b_0.6232887914692327.py tmp/b_0.7812314428145001.py\n",
      "563.0 601.0 tmp/b_0.6232887914692327.py tmp/b_0.7812314428145001.py\n",
      "579.0 583.0 tmp/b_0.6232887914692327.py tmp/b_0.7812314428145001.py\n",
      "673.0 630.0 tmp/b_0.6232887914692327.py tmp/b_0.7812314428145001.py\n",
      "648.0 693.0 tmp/b_0.6232887914692327.py tmp/b_0.7812314428145001.py\n",
      "633.0 632.0 tmp/b_0.6232887914692327.py tmp/b_0.7812314428145001.py\n",
      "625.0 616.0 tmp/b_0.6232887914692327.py tmp/b_0.7812314428145001.py\n",
      "607.0 577.0 tmp/b_0.6232887914692327.py tmp/b_0.7812314428145001.py\n",
      "625.0 655.0 tmp/b_0.6232887914692327.py tmp/b_0.7812314428145001.py\n",
      "617.0 651.0 tmp/b_0.6232887914692327.py tmp/b_0.7812314428145001.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.25859498040269646,\n",
       " -10.5,\n",
       " 29.391325250828686,\n",
       " 0.4,\n",
       " 'tmp/b_0.6232887914692327.py',\n",
       " 'tmp/b_0.7812314428145001.py',\n",
       " array([627., 563., 579., 673., 648., 633., 625., 607., 625., 617.]),\n",
       " array([664., 601., 583., 630., 693., 632., 616., 577., 655., 651.]))"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v4 as feature\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_feature_nagiss,use_mean=True,True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=utils.bandits.neural),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538.0 550.0 tmp/b_0.03981686451690292.py tmp/b_0.8435219654502392.py\n",
      "633.0 611.0 tmp/b_0.03981686451690292.py tmp/b_0.8435219654502392.py\n",
      "589.0 594.0 tmp/b_0.03981686451690292.py tmp/b_0.8435219654502392.py\n",
      "614.0 638.0 tmp/b_0.03981686451690292.py tmp/b_0.8435219654502392.py\n",
      "631.0 664.0 tmp/b_0.03981686451690292.py tmp/b_0.8435219654502392.py\n",
      "656.0 644.0 tmp/b_0.03981686451690292.py tmp/b_0.8435219654502392.py\n",
      "529.0 583.0 tmp/b_0.03981686451690292.py tmp/b_0.8435219654502392.py\n",
      "610.0 635.0 tmp/b_0.03981686451690292.py tmp/b_0.8435219654502392.py\n",
      "641.0 625.0 tmp/b_0.03981686451690292.py tmp/b_0.8435219654502392.py\n",
      "664.0 632.0 tmp/b_0.03981686451690292.py tmp/b_0.8435219654502392.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.38850022498604,\n",
       " -7.1,\n",
       " 26.036320784627,\n",
       " 0.4,\n",
       " 'tmp/b_0.03981686451690292.py',\n",
       " 'tmp/b_0.8435219654502392.py',\n",
       " array([538., 633., 589., 614., 631., 656., 529., 610., 641., 664.]),\n",
       " array([550., 611., 594., 638., 664., 644., 583., 635., 625., 632.]))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v2 as feature vs only v2\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_feature_nagiss=True\", \"\")\n",
    "neural_with_only_nagiss = utils.bandits.neural.format(\"use_only_nagiss=True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_with_only_nagiss),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dataset(datasets[0], 'test_save_nagiss_train.npy')\n",
    "save_dataset(datasets[1], 'test_save_nagiss_val.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit on Temporal Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.54513832e+00,  5.82159002e-01,  2.12729732e+00,  2.00000000e+00,\n",
       "        1.99800000e+03,  1.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        7.26338676e-01,  8.22673418e-01,  1.55853035e+00,  7.48466609e-01,\n",
       "        4.76706051e-01,  9.21358662e-01,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  1.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  1.44497329e+00,  1.08565598e+00,  2.28020948e+00,\n",
       "        2.72875138e+00,  6.66666667e-01,  6.66666667e-01,  6.66666667e-01,\n",
       "        9.90000000e-01, -1.00000000e-02,  9.80000000e-01,  0.00000000e+00,\n",
       "        0.00000000e+00,  9.90000000e-01, -1.00000000e-02, -2.00000000e-02,\n",
       "        2.42604406e-01,  5.24751214e-02, -1.16985524e-01,  3.17383712e-01,\n",
       "       -2.61674838e-02,  1.74026792e-01, -1.00000000e-02, -1.00000000e-02,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  9.90000000e-01,\n",
       "       -1.00000000e-02,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "       -1.00000000e-02, -1.00000000e-02,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00, -2.05848684e-02,  1.11009769e-01, -8.90838930e-01,\n",
       "       -1.34384845e+00,  1.64166667e-01,  1.64166667e-01,  1.64166667e-01])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0].X[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNWithCusomFeatures2(nn.Module):\n",
    "    def __init__(self, INPUT_F, DROP_P, H):\n",
    "        super().__init__()\n",
    "        INPUT_F_C = INPUT_F + 2 * INPUT_F\n",
    "        self.model_ff =  nn.Sequential(\n",
    "            nn.BatchNorm1d(INPUT_F_C),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(INPUT_F_C, H),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(H, H),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        lg = torch.log(1 + torch.abs(x))\n",
    "        sn = torch.sin(x)\n",
    "        input_x = torch.cat([x, lg, sn], axis=1)\n",
    "        return self.model_ff(input_x)\n",
    "\n",
    "class TDModel(nn.Module):\n",
    "    def __init__(self, INPUT_F, DROP_P, H):\n",
    "        super().__init__()\n",
    "        self.model = NNWithCusomFeatures(INPUT_F, DROP_P, H)\n",
    "\n",
    "    def forward(self, x):\n",
    "        left = x[:, : INPUT_F]\n",
    "        right = x[:, INPUT_F :]\n",
    "        return self.model(left) - self.model(right).detach()\n",
    "    \n",
    "class DoubleTDModel(nn.Module):\n",
    "    def __init__(self, INPUT_F, DROP_P, H):\n",
    "        super().__init__()\n",
    "        self.modelA = NNWithCusomFeatures2(INPUT_F, DROP_P, H)\n",
    "        self.modelB = NNWithCusomFeatures2(INPUT_F, DROP_P, H)\n",
    "\n",
    "    def forward(self, x):\n",
    "        left = x[:, : INPUT_F]\n",
    "        right = x[:, INPUT_F :]\n",
    "        ab = self.modelA(left) - self.modelB(right)\n",
    "        ba = self.modelB(left) - self.modelA(right)\n",
    "        return (ab + ba) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_td_dataset(d : utils.snowden.Dataset):\n",
    "    left = np.concatenate([d.X, np.zeros((1, d.X.shape[1]))], axis=0)\n",
    "    right = np.concatenate([np.zeros((1, d.X.shape[1])), d.X], axis=0)\n",
    "    y = np.concatenate([d.y, [0]])\n",
    "    X = np.concatenate([left, right], axis=1)\n",
    "    return utils.snowden.Dataset(X, y, d.sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_train = make_td_dataset(datasets[0])\n",
    "td_val = make_td_dataset(datasets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_train.save_dataset('datasets/td_train_nagiss_20201226')\n",
    "td_val.save_dataset('datasets/td_val_nagiss_20201226')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 500 batch: 0.16636\n",
      "train loss in 1 epoch in 1000 batch: 0.16476\n",
      "train loss in 1 epoch in 1500 batch: 0.17953\n",
      "train loss in 1 epoch in 2000 batch: 0.30793\n",
      "train loss in 1 epoch in 2500 batch: 0.22230\n",
      "train loss in 1 epoch in 3000 batch: 0.20506\n",
      "train loss in 1 epoch in 3500 batch: 0.14479\n",
      "train loss in 1 epoch in 4000 batch: 0.18252\n",
      "val loss in 1 epoch: 0.19520\n"
     ]
    }
   ],
   "source": [
    "# mse loss\n",
    "INPUT_F = 72\n",
    "H = 256\n",
    "DROP_P = 0.05\n",
    "\n",
    "model = NNWithCusomFeatures(INPUT_F, DROP_P, H)\n",
    "\n",
    "learn(datasets[0], datasets[1], model, freq=500, batch_size=32,lr=1e-5, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 50 batch: 0.33739\n",
      "train loss in 1 epoch in 100 batch: 0.29898\n",
      "train loss in 1 epoch in 150 batch: 0.30539\n",
      "train loss in 1 epoch in 200 batch: 0.33259\n",
      "train loss in 1 epoch in 250 batch: 0.33338\n",
      "val loss in 1 epoch: 0.31717\n",
      "train loss in 2 epoch in 50 batch: 0.30092\n",
      "train loss in 2 epoch in 100 batch: 0.32005\n",
      "train loss in 2 epoch in 150 batch: 0.31823\n",
      "train loss in 2 epoch in 200 batch: 0.31090\n",
      "train loss in 2 epoch in 250 batch: 0.32401\n",
      "val loss in 2 epoch: 0.31707\n",
      "train loss in 3 epoch in 50 batch: 0.30618\n",
      "train loss in 3 epoch in 100 batch: 0.33236\n",
      "train loss in 3 epoch in 150 batch: 0.33858\n",
      "train loss in 3 epoch in 200 batch: 0.31367\n",
      "train loss in 3 epoch in 250 batch: 0.31520\n",
      "val loss in 3 epoch: 0.31703\n",
      "train loss in 4 epoch in 50 batch: 0.30974\n",
      "train loss in 4 epoch in 100 batch: 0.33966\n",
      "train loss in 4 epoch in 150 batch: 0.32230\n",
      "train loss in 4 epoch in 200 batch: 0.32780\n",
      "train loss in 4 epoch in 250 batch: 0.31112\n",
      "val loss in 4 epoch: 0.31701\n",
      "train loss in 5 epoch in 50 batch: 0.30343\n",
      "train loss in 5 epoch in 100 batch: 0.31349\n",
      "train loss in 5 epoch in 150 batch: 0.34429\n",
      "train loss in 5 epoch in 200 batch: 0.32564\n",
      "train loss in 5 epoch in 250 batch: 0.33413\n",
      "val loss in 5 epoch: 0.31698\n",
      "train loss in 6 epoch in 50 batch: 0.30016\n",
      "train loss in 6 epoch in 100 batch: 0.32984\n",
      "train loss in 6 epoch in 150 batch: 0.31350\n",
      "train loss in 6 epoch in 200 batch: 0.32597\n",
      "train loss in 6 epoch in 250 batch: 0.26691\n",
      "val loss in 6 epoch: 0.31696\n",
      "train loss in 7 epoch in 50 batch: 0.30450\n",
      "train loss in 7 epoch in 100 batch: 0.29120\n",
      "train loss in 7 epoch in 150 batch: 0.33111\n",
      "train loss in 7 epoch in 200 batch: 0.36213\n",
      "train loss in 7 epoch in 250 batch: 0.32340\n",
      "val loss in 7 epoch: 0.31695\n",
      "train loss in 8 epoch in 50 batch: 0.30356\n",
      "train loss in 8 epoch in 100 batch: 0.29629\n",
      "train loss in 8 epoch in 150 batch: 0.31225\n",
      "train loss in 8 epoch in 200 batch: 0.30248\n",
      "train loss in 8 epoch in 250 batch: 0.32090\n",
      "val loss in 8 epoch: 0.31695\n",
      "train loss in 9 epoch in 50 batch: 0.31959\n",
      "train loss in 9 epoch in 100 batch: 0.32671\n",
      "train loss in 9 epoch in 150 batch: 0.30573\n",
      "train loss in 9 epoch in 200 batch: 0.33003\n",
      "train loss in 9 epoch in 250 batch: 0.31635\n",
      "val loss in 9 epoch: 0.31694\n",
      "train loss in 10 epoch in 50 batch: 0.29896\n",
      "train loss in 10 epoch in 100 batch: 0.33301\n",
      "train loss in 10 epoch in 150 batch: 0.30595\n",
      "train loss in 10 epoch in 200 batch: 0.33117\n",
      "train loss in 10 epoch in 250 batch: 0.29733\n",
      "val loss in 10 epoch: 0.31694\n"
     ]
    }
   ],
   "source": [
    "# mse loss\n",
    "INPUT_F = 72\n",
    "H = 256\n",
    "DROP_P = 0.1\n",
    "\n",
    "tdmodel = TDModel(INPUT_F, DROP_P, H)\n",
    "\n",
    "learn(td_train, td_val, tdmodel, freq=50, batch_size=512,lr=1e-5, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(tdmodel.model.state_dict(), 'models/nagiss_v5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 50 batch: 0.33434\n",
      "train loss in 1 epoch in 100 batch: 0.29501\n",
      "train loss in 1 epoch in 150 batch: 0.30367\n",
      "train loss in 1 epoch in 200 batch: 0.33173\n",
      "train loss in 1 epoch in 250 batch: 0.32840\n",
      "val loss in 1 epoch: 0.31808\n",
      "train loss in 2 epoch in 50 batch: 0.30069\n",
      "train loss in 2 epoch in 100 batch: 0.32157\n",
      "train loss in 2 epoch in 150 batch: 0.32148\n",
      "train loss in 2 epoch in 200 batch: 0.31430\n",
      "train loss in 2 epoch in 250 batch: 0.32340\n",
      "val loss in 2 epoch: 0.31920\n",
      "train loss in 3 epoch in 50 batch: 0.30729\n",
      "train loss in 3 epoch in 100 batch: 0.33402\n",
      "train loss in 3 epoch in 150 batch: 0.33552\n",
      "train loss in 3 epoch in 200 batch: 0.31429\n",
      "train loss in 3 epoch in 250 batch: 0.31118\n",
      "val loss in 3 epoch: 0.31922\n",
      "train loss in 4 epoch in 50 batch: 0.30944\n",
      "train loss in 4 epoch in 100 batch: 0.34393\n",
      "train loss in 4 epoch in 150 batch: 0.32317\n",
      "train loss in 4 epoch in 200 batch: 0.32753\n",
      "train loss in 4 epoch in 250 batch: 0.31087\n",
      "val loss in 4 epoch: 0.31874\n",
      "train loss in 5 epoch in 50 batch: 0.30523\n",
      "train loss in 5 epoch in 100 batch: 0.31276\n",
      "train loss in 5 epoch in 150 batch: 0.34309\n",
      "train loss in 5 epoch in 200 batch: 0.32451\n",
      "train loss in 5 epoch in 250 batch: 0.33523\n",
      "val loss in 5 epoch: 0.31823\n",
      "train loss in 6 epoch in 50 batch: 0.30357\n",
      "train loss in 6 epoch in 100 batch: 0.32958\n",
      "train loss in 6 epoch in 150 batch: 0.31489\n",
      "train loss in 6 epoch in 200 batch: 0.32606\n",
      "train loss in 6 epoch in 250 batch: 0.27128\n",
      "val loss in 6 epoch: 0.31783\n",
      "train loss in 7 epoch in 50 batch: 0.30124\n",
      "train loss in 7 epoch in 100 batch: 0.28973\n",
      "train loss in 7 epoch in 150 batch: 0.33052\n",
      "train loss in 7 epoch in 200 batch: 0.35898\n",
      "train loss in 7 epoch in 250 batch: 0.32137\n",
      "val loss in 7 epoch: 0.31759\n",
      "train loss in 8 epoch in 50 batch: 0.30171\n",
      "train loss in 8 epoch in 100 batch: 0.29348\n",
      "train loss in 8 epoch in 150 batch: 0.31309\n",
      "train loss in 8 epoch in 200 batch: 0.30257\n",
      "train loss in 8 epoch in 250 batch: 0.31792\n",
      "val loss in 8 epoch: 0.31746\n",
      "train loss in 9 epoch in 50 batch: 0.32322\n",
      "train loss in 9 epoch in 100 batch: 0.32424\n",
      "train loss in 9 epoch in 150 batch: 0.30275\n",
      "train loss in 9 epoch in 200 batch: 0.32998\n",
      "train loss in 9 epoch in 250 batch: 0.31879\n",
      "val loss in 9 epoch: 0.31738\n",
      "train loss in 10 epoch in 50 batch: 0.29692\n",
      "train loss in 10 epoch in 100 batch: 0.33546\n",
      "train loss in 10 epoch in 150 batch: 0.30751\n",
      "train loss in 10 epoch in 200 batch: 0.32876\n",
      "train loss in 10 epoch in 250 batch: 0.29608\n",
      "val loss in 10 epoch: 0.31735\n"
     ]
    }
   ],
   "source": [
    "# mse loss\n",
    "INPUT_F = 72\n",
    "H = 256\n",
    "DROP_P = 0.01\n",
    "\n",
    "tdmodel = TDModel(INPUT_F, DROP_P, H)\n",
    "\n",
    "learn(td_train, td_val, tdmodel, freq=50, batch_size=512,lr=1e-5, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "579.0 653.0 tmp/b_0.5780403019658866.py tmp/b_0.7839141865156775.py\n",
      "585.0 608.0 tmp/b_0.5780403019658866.py tmp/b_0.7839141865156775.py\n",
      "589.0 628.0 tmp/b_0.5780403019658866.py tmp/b_0.7839141865156775.py\n",
      "590.0 651.0 tmp/b_0.5780403019658866.py tmp/b_0.7839141865156775.py\n",
      "573.0 644.0 tmp/b_0.5780403019658866.py tmp/b_0.7839141865156775.py\n",
      "573.0 657.0 tmp/b_0.5780403019658866.py tmp/b_0.7839141865156775.py\n",
      "590.0 641.0 tmp/b_0.5780403019658866.py tmp/b_0.7839141865156775.py\n",
      "568.0 701.0 tmp/b_0.5780403019658866.py tmp/b_0.7839141865156775.py\n",
      "595.0 642.0 tmp/b_0.5780403019658866.py tmp/b_0.7839141865156775.py\n",
      "566.0 629.0 tmp/b_0.5780403019658866.py tmp/b_0.7839141865156775.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6.64840967158245e-13,\n",
       " -64.6,\n",
       " 28.42604439594085,\n",
       " 0.0,\n",
       " 'tmp/b_0.5780403019658866.py',\n",
       " 'tmp/b_0.7839141865156775.py',\n",
       " array([579., 585., 589., 590., 573., 573., 590., 568., 595., 566.]),\n",
       " array([653., 608., 628., 651., 644., 657., 641., 701., 642., 629.]))"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v5 only\n",
    "# neural_with_nagiss = utils.bandits.neural.format(\"use_feature_nagiss=True\", \"\")\n",
    "neural_with_only_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_only_nagiss),\n",
    "    utils.bandits.Agent(text=utils.bandits.neural),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674.0 678.0 tmp/b_0.3813700667284041.py tmp/b_0.7124470702854484.py\n",
      "632.0 639.0 tmp/b_0.3813700667284041.py tmp/b_0.7124470702854484.py\n",
      "648.0 679.0 tmp/b_0.3813700667284041.py tmp/b_0.7124470702854484.py\n",
      "611.0 585.0 tmp/b_0.3813700667284041.py tmp/b_0.7124470702854484.py\n",
      "705.0 660.0 tmp/b_0.3813700667284041.py tmp/b_0.7124470702854484.py\n",
      "659.0 625.0 tmp/b_0.3813700667284041.py tmp/b_0.7124470702854484.py\n",
      "586.0 558.0 tmp/b_0.3813700667284041.py tmp/b_0.7124470702854484.py\n",
      "694.0 668.0 tmp/b_0.3813700667284041.py tmp/b_0.7124470702854484.py\n",
      "720.0 681.0 tmp/b_0.3813700667284041.py tmp/b_0.7124470702854484.py\n",
      "660.0 639.0 tmp/b_0.3813700667284041.py tmp/b_0.7124470702854484.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.013848867420041112,\n",
       " 17.7,\n",
       " 22.742251427684113,\n",
       " 0.7,\n",
       " 'tmp/b_0.3813700667284041.py',\n",
       " 'tmp/b_0.7124470702854484.py',\n",
       " array([674., 632., 648., 611., 705., 659., 586., 694., 720., 660.]),\n",
       " array([678., 639., 679., 585., 660., 625., 558., 668., 681., 639.]))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v5 features\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_feature_nagiss,use_mean=True,True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=utils.bandits.neural),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top 10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions = [18775092, 18808528, 18645426,\n",
    "               18841795, 18844737, 18817525,\n",
    "               18689820, 18818826, 18667443, 18814531]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for sub in submissions:\n",
    "    utils.web.download_all_sessions_for_submission(sub, 'sessions/20201227/{}'.format(sub), delta=0.1)\n",
    "    print('{} OK'.format(sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sessions/20201227/ ['sessions/20201227/18817525/8314372.json', 'sessions/20201227/18817525/8397408.json', 'sessions/20201227/18817525/8415499.json', 'sessions/20201227/18817525/8318832.json', 'sessions/20201227/18817525/8315742.json', 'sessions/20201227/18817525/8324361.json', 'sessions/20201227/18817525/8317114.json', 'sessions/20201227/18817525/8519185.json', 'sessions/20201227/18817525/8461310.json', 'sessions/20201227/18817525/8641350.json', 'sessions/20201227/18817525/8321949.json', 'sessions/20201227/18817525/8625587.json', 'sessions/20201227/18817525/8314040.json', 'sessions/20201227/18817525/8570033.json', 'sessions/20201227/18817525/8657196.json', 'sessions/20201227/18817525/8318147.json', 'sessions/20201227/18817525/8621966.json', 'sessions/20201227/18817525/8626309.json', 'sessions/20201227/18817525/8323329.json', 'sessions/20201227/18817525/8360828.json', 'sessions/20201227/18817525/8316778.json', 'sessions/20201227/18817525/8343671.json', 'sessions/20201227/18817525/8376704.json', 'sessions/20201227/18817525/8602626.json', 'sessions/20201227/18817525/8537881.json', 'sessions/20201227/18817525/8327822.json', 'sessions/20201227/18817525/8313350.json', 'sessions/20201227/18817525/8615439.json', 'sessions/20201227/18817525/8557561.json', 'sessions/20201227/18817525/8331283.json', 'sessions/20201227/18817525/8447156.json', 'sessions/20201227/18817525/8322945.json', 'sessions/20201227/18817525/8316427.json', 'sessions/20201227/18817525/8501879.json', 'sessions/20201227/18817525/8322656.json', 'sessions/20201227/18817525/8319863.json', 'sessions/20201227/18817525/8312794.json', 'sessions/20201227/18817525/8349912.json', 'sessions/20201227/18817525/8313015.json', 'sessions/20201227/18817525/8411347.json', 'sessions/20201227/18817525/8336463.json', 'sessions/20201227/18817525/8320536.json', 'sessions/20201227/18817525/8320218.json', 'sessions/20201227/18817525/8321560.json', 'sessions/20201227/18817525/8315398.json', 'sessions/20201227/18817525/8319207.json', 'sessions/20201227/18817525/8557918.json', 'sessions/20201227/18817525/8503611.json', 'sessions/20201227/18817525/8317798.json', 'sessions/20201227/18817525/8408939.json', 'sessions/20201227/18817525/8673069.json', 'sessions/20201227/18817525/8488087.json', 'sessions/20201227/18817525/8323696.json', 'sessions/20201227/18817525/8324775.json', 'sessions/20201227/18817525/8323675.json', 'sessions/20201227/18817525/8319508.json', 'sessions/20201227/18817525/8386986.json', 'sessions/20201227/18817525/8477516.json', 'sessions/20201227/18817525/8449014.json', 'sessions/20201227/18817525/8448994.json', 'sessions/20201227/18817525/8322320.json', 'sessions/20201227/18817525/8313703.json', 'sessions/20201227/18817525/8315058.json', 'sessions/20201227/18817525/8455991.json', 'sessions/20201227/18817525/8427728.json', 'sessions/20201227/18817525/8547861.json', 'sessions/20201227/18817525/8323998.json', 'sessions/20201227/18817525/8436027.json', 'sessions/20201227/18817525/8412344.json', 'sessions/20201227/18817525/8316087.json', 'sessions/20201227/18817525/8370744.json', 'sessions/20201227/18817525/8672331.json', 'sessions/20201227/18817525/8586077.json', 'sessions/20201227/18817525/8322609.json', 'sessions/20201227/18817525/8320874.json', 'sessions/20201227/18817525/8321224.json', 'sessions/20201227/18817525/8561456.json', 'sessions/20201227/18817525/8318485.json', 'sessions/20201227/18817525/8314737.json', 'sessions/20201227/18817525/8531233.json', 'sessions/20201227/18817525/8317463.json', 'sessions/20201227/18844737/8610406.json', 'sessions/20201227/18844737/8640259.json', 'sessions/20201227/18844737/8614717.json', 'sessions/20201227/18844737/8635210.json', 'sessions/20201227/18844737/8605760.json', 'sessions/20201227/18844737/8611518.json', 'sessions/20201227/18844737/8652838.json', 'sessions/20201227/18844737/8608980.json', 'sessions/20201227/18844737/8613342.json', 'sessions/20201227/18844737/8606843.json', 'sessions/20201227/18844737/8604346.json', 'sessions/20201227/18844737/8616156.json', 'sessions/20201227/18844737/8605039.json', 'sessions/20201227/18844737/8623449.json', 'sessions/20201227/18844737/8609695.json', 'sessions/20201227/18844737/8602935.json', 'sessions/20201227/18844737/8615065.json', 'sessions/20201227/18844737/8614789.json', 'sessions/20201227/18844737/8612562.json', 'sessions/20201227/18844737/8683531.json', 'sessions/20201227/18844737/8603963.json', 'sessions/20201227/18844737/8615439.json', 'sessions/20201227/18844737/8607193.json', 'sessions/20201227/18844737/8607940.json', 'sessions/20201227/18844737/8604679.json', 'sessions/20201227/18844737/8609354.json', 'sessions/20201227/18844737/8603630.json', 'sessions/20201227/18844737/8612922.json', 'sessions/20201227/18844737/8602522.json', 'sessions/20201227/18844737/8614031.json', 'sessions/20201227/18844737/8610064.json', 'sessions/20201227/18844737/8610766.json', 'sessions/20201227/18844737/8674173.json', 'sessions/20201227/18844737/8611136.json', 'sessions/20201227/18844737/8619101.json', 'sessions/20201227/18844737/8613652.json', 'sessions/20201227/18844737/8611842.json', 'sessions/20201227/18844737/8601946.json', 'sessions/20201227/18844737/8608264.json', 'sessions/20201227/18844737/8612215.json', 'sessions/20201227/18844737/8628742.json', 'sessions/20201227/18844737/8663711.json', 'sessions/20201227/18844737/8603239.json', 'sessions/20201227/18844737/8639492.json', 'sessions/20201227/18844737/8605396.json', 'sessions/20201227/18844737/8608623.json', 'sessions/20201227/18844737/8615787.json', 'sessions/20201227/18844737/8614362.json', 'sessions/20201227/18844737/8602166.json', 'sessions/20201227/18844737/8606122.json', 'sessions/20201227/18844737/8606477.json', 'sessions/20201227/18844737/8651105.json', 'sessions/20201227/18844737/8607552.json', 'sessions/20201227/18775092/8667645.json', 'sessions/20201227/18775092/7994638.json', 'sessions/20201227/18775092/7991093.json', 'sessions/20201227/18775092/7988820.json', 'sessions/20201227/18775092/8386716.json', 'sessions/20201227/18775092/8038792.json', 'sessions/20201227/18775092/8652496.json', 'sessions/20201227/18775092/8233786.json', 'sessions/20201227/18775092/8073493.json', 'sessions/20201227/18775092/8012231.json', 'sessions/20201227/18775092/7995268.json', 'sessions/20201227/18775092/8046220.json', 'sessions/20201227/18775092/8321309.json', 'sessions/20201227/18775092/8102725.json', 'sessions/20201227/18775092/8636292.json', 'sessions/20201227/18775092/8146617.json', 'sessions/20201227/18775092/7986639.json', 'sessions/20201227/18775092/8029737.json', 'sessions/20201227/18775092/8089900.json', 'sessions/20201227/18775092/8354985.json', 'sessions/20201227/18775092/7995644.json', 'sessions/20201227/18775092/7998867.json', 'sessions/20201227/18775092/8257767.json', 'sessions/20201227/18775092/7992085.json', 'sessions/20201227/18775092/7987842.json', 'sessions/20201227/18775092/8172893.json', 'sessions/20201227/18775092/8201003.json', 'sessions/20201227/18775092/8009354.json', 'sessions/20201227/18775092/7988519.json', 'sessions/20201227/18775092/8573496.json', 'sessions/20201227/18775092/7987183.json', 'sessions/20201227/18775092/7992694.json', 'sessions/20201227/18775092/7991811.json', 'sessions/20201227/18775092/8464420.json', 'sessions/20201227/18775092/8186631.json', 'sessions/20201227/18775092/8054877.json', 'sessions/20201227/18775092/8249308.json', 'sessions/20201227/18775092/8273034.json', 'sessions/20201227/18775092/7987567.json', 'sessions/20201227/18775092/8371128.json', 'sessions/20201227/18775092/8060412.json', 'sessions/20201227/18775092/8541194.json', 'sessions/20201227/18775092/8481316.json', 'sessions/20201227/18775092/8683270.json', 'sessions/20201227/18775092/7994974.json', 'sessions/20201227/18775092/8132606.json', 'sessions/20201227/18775092/7989777.json', 'sessions/20201227/18775092/8621563.json', 'sessions/20201227/18775092/8606240.json', 'sessions/20201227/18775092/7992382.json', 'sessions/20201227/18775092/8160880.json', 'sessions/20201227/18775092/8005498.json', 'sessions/20201227/18775092/8337440.json', 'sessions/20201227/18775092/8008069.json', 'sessions/20201227/18775092/8304463.json', 'sessions/20201227/18775092/7994017.json', 'sessions/20201227/18775092/7995939.json', 'sessions/20201227/18775092/8510303.json', 'sessions/20201227/18775092/8433263.json', 'sessions/20201227/18775092/7996231.json', 'sessions/20201227/18775092/7996548.json', 'sessions/20201227/18775092/8449014.json', 'sessions/20201227/18775092/7988184.json', 'sessions/20201227/18775092/8215761.json', 'sessions/20201227/18775092/7993014.json', 'sessions/20201227/18775092/7989458.json', 'sessions/20201227/18775092/7990100.json', 'sessions/20201227/18775092/8288389.json', 'sessions/20201227/18775092/7991414.json', 'sessions/20201227/18775092/7994342.json', 'sessions/20201227/18775092/8417594.json', 'sessions/20201227/18775092/7991731.json', 'sessions/20201227/18775092/8402256.json', 'sessions/20201227/18775092/8002300.json', 'sessions/20201227/18775092/8495892.json', 'sessions/20201227/18775092/8557211.json', 'sessions/20201227/18775092/8073192.json', 'sessions/20201227/18775092/8589678.json', 'sessions/20201227/18775092/8118585.json', 'sessions/20201227/18775092/8016051.json', 'sessions/20201227/18775092/7993371.json', 'sessions/20201227/18775092/7989156.json', 'sessions/20201227/18775092/7990750.json', 'sessions/20201227/18775092/8021586.json', 'sessions/20201227/18775092/7993701.json', 'sessions/20201227/18775092/7990443.json', 'sessions/20201227/18775092/8079696.json', 'sessions/20201227/18775092/7987514.json', 'sessions/20201227/18775092/8525948.json', 'sessions/20201227/18645426/7539606.json', 'sessions/20201227/18645426/7263146.json', 'sessions/20201227/18645426/7727094.json', 'sessions/20201227/18645426/7278835.json', 'sessions/20201227/18645426/7497464.json', 'sessions/20201227/18645426/8632737.json', 'sessions/20201227/18645426/8305518.json', 'sessions/20201227/18645426/8486009.json', 'sessions/20201227/18645426/8624030.json', 'sessions/20201227/18645426/8102369.json', 'sessions/20201227/18645426/8427675.json', 'sessions/20201227/18645426/8173174.json', 'sessions/20201227/18645426/7407093.json', 'sessions/20201227/18645426/8523702.json', 'sessions/20201227/18645426/7249077.json', 'sessions/20201227/18645426/7547582.json', 'sessions/20201227/18645426/8382144.json', 'sessions/20201227/18645426/8652838.json', 'sessions/20201227/18645426/8636714.json', 'sessions/20201227/18645426/7523247.json', 'sessions/20201227/18645426/8020321.json', 'sessions/20201227/18645426/7759014.json', 'sessions/20201227/18645426/7976974.json', 'sessions/20201227/18645426/8189917.json', 'sessions/20201227/18645426/8254711.json', 'sessions/20201227/18645426/7832948.json', 'sessions/20201227/18645426/7967365.json', 'sessions/20201227/18645426/8070620.json', 'sessions/20201227/18645426/8561497.json', 'sessions/20201227/18645426/7251237.json', 'sessions/20201227/18645426/7303776.json', 'sessions/20201227/18645426/7600944.json', 'sessions/20201227/18645426/7249543.json', 'sessions/20201227/18645426/7250277.json', 'sessions/20201227/18645426/7595239.json', 'sessions/20201227/18645426/8253060.json', 'sessions/20201227/18645426/7649449.json', 'sessions/20201227/18645426/8271264.json', 'sessions/20201227/18645426/8400557.json', 'sessions/20201227/18645426/7919984.json', 'sessions/20201227/18645426/8208428.json', 'sessions/20201227/18645426/7651829.json', 'sessions/20201227/18645426/8236468.json', 'sessions/20201227/18645426/7863794.json', 'sessions/20201227/18645426/7473602.json', 'sessions/20201227/18645426/8363951.json', 'sessions/20201227/18645426/7658389.json', 'sessions/20201227/18645426/8587157.json', 'sessions/20201227/18645426/7880088.json', 'sessions/20201227/18645426/8536137.json', 'sessions/20201227/18645426/8307535.json', 'sessions/20201227/18645426/7643072.json', 'sessions/20201227/18645426/7849426.json', 'sessions/20201227/18645426/7339288.json', 'sessions/20201227/18645426/8681805.json', 'sessions/20201227/18645426/8500439.json', 'sessions/20201227/18645426/8037180.json', 'sessions/20201227/18645426/7936835.json', 'sessions/20201227/18645426/7250508.json', 'sessions/20201227/18645426/7578073.json', 'sessions/20201227/18645426/8580282.json', 'sessions/20201227/18645426/7772533.json', 'sessions/20201227/18645426/8672339.json', 'sessions/20201227/18645426/7382579.json', 'sessions/20201227/18645426/7944655.json', 'sessions/20201227/18645426/7707517.json', 'sessions/20201227/18645426/7806441.json', 'sessions/20201227/18645426/7563222.json', 'sessions/20201227/18645426/8593322.json', 'sessions/20201227/18645426/7250751.json', 'sessions/20201227/18645426/7825861.json', 'sessions/20201227/18645426/8086252.json', 'sessions/20201227/18645426/8167172.json', 'sessions/20201227/18645426/7995666.json', 'sessions/20201227/18645426/7790578.json', 'sessions/20201227/18645426/8379778.json', 'sessions/20201227/18645426/8544267.json', 'sessions/20201227/18645426/7712872.json', 'sessions/20201227/18645426/7249301.json', 'sessions/20201227/18645426/7592279.json', 'sessions/20201227/18645426/8197668.json', 'sessions/20201227/18645426/8211715.json', 'sessions/20201227/18645426/8443341.json', 'sessions/20201227/18645426/7249784.json', 'sessions/20201227/18645426/7448073.json', 'sessions/20201227/18645426/7655126.json', 'sessions/20201227/18645426/8621563.json', 'sessions/20201227/18645426/7645520.json', 'sessions/20201227/18645426/8337440.json', 'sessions/20201227/18645426/8052903.json', 'sessions/20201227/18645426/8291300.json', 'sessions/20201227/18645426/7363626.json', 'sessions/20201227/18645426/7351868.json', 'sessions/20201227/18645426/8612613.json', 'sessions/20201227/18645426/7902040.json', 'sessions/20201227/18645426/7615514.json', 'sessions/20201227/18645426/7629617.json', 'sessions/20201227/18645426/8609423.json', 'sessions/20201227/18645426/8220835.json', 'sessions/20201227/18645426/7742847.json', 'sessions/20201227/18645426/7285168.json', 'sessions/20201227/18645426/8532244.json', 'sessions/20201227/18645426/8275009.json', 'sessions/20201227/18645426/7412607.json', 'sessions/20201227/18645426/8317490.json', 'sessions/20201227/18645426/7702081.json', 'sessions/20201227/18645426/8004482.json', 'sessions/20201227/18645426/7250029.json', 'sessions/20201227/18645426/7851234.json', 'sessions/20201227/18645426/8117581.json', 'sessions/20201227/18645426/7937763.json', 'sessions/20201227/18645426/7738334.json', 'sessions/20201227/18645426/7539612.json', 'sessions/20201227/18645426/8453828.json', 'sessions/20201227/18645426/7904243.json', 'sessions/20201227/18645426/8412391.json', 'sessions/20201227/18645426/7673931.json', 'sessions/20201227/18645426/7879489.json', 'sessions/20201227/18645426/8151302.json', 'sessions/20201227/18645426/7810457.json', 'sessions/20201227/18645426/8134347.json', 'sessions/20201227/18645426/8397410.json', 'sessions/20201227/18645426/8545017.json', 'sessions/20201227/18645426/8354975.json', 'sessions/20201227/18645426/7255606.json', 'sessions/20201227/18645426/8515648.json', 'sessions/20201227/18645426/7314592.json', 'sessions/20201227/18645426/7893592.json', 'sessions/20201227/18645426/7787500.json', 'sessions/20201227/18645426/8206673.json', 'sessions/20201227/18645426/8657511.json', 'sessions/20201227/18645426/7262670.json', 'sessions/20201227/18645426/7421907.json', 'sessions/20201227/18645426/8470076.json', 'sessions/20201227/18645426/8328176.json', 'sessions/20201227/18645426/7963200.json', 'sessions/20201227/18645426/7687639.json', 'sessions/20201227/18645426/8573943.json', 'sessions/20201227/18645426/8189309.json', 'sessions/20201227/18645426/7250998.json', 'sessions/20201227/18814531/8479278.json', 'sessions/20201227/18814531/8292933.json', 'sessions/20201227/18814531/8540484.json', 'sessions/20201227/18814531/8296726.json', 'sessions/20201227/18814531/8342965.json', 'sessions/20201227/18814531/8466586.json', 'sessions/20201227/18814531/8291237.json', 'sessions/20201227/18814531/8304216.json', 'sessions/20201227/18814531/8355274.json', 'sessions/20201227/18814531/8298980.json', 'sessions/20201227/18814531/8609348.json', 'sessions/20201227/18814531/8289523.json', 'sessions/20201227/18814531/8289173.json', 'sessions/20201227/18814531/8296005.json', 'sessions/20201227/18814531/8295042.json', 'sessions/20201227/18814531/8451791.json', 'sessions/20201227/18814531/8337055.json', 'sessions/20201227/18814531/8299667.json', 'sessions/20201227/18814531/8562805.json', 'sessions/20201227/18814531/8290554.json', 'sessions/20201227/18814531/8380778.json', 'sessions/20201227/18814531/8363253.json', 'sessions/20201227/18814531/8298293.json', 'sessions/20201227/18814531/8329200.json', 'sessions/20201227/18814531/8299314.json', 'sessions/20201227/18814531/8450703.json', 'sessions/20201227/18814531/8602542.json', 'sessions/20201227/18814531/8293611.json', 'sessions/20201227/18814531/8298661.json', 'sessions/20201227/18814531/8308907.json', 'sessions/20201227/18814531/8411388.json', 'sessions/20201227/18814531/8671610.json', 'sessions/20201227/18814531/8296351.json', 'sessions/20201227/18814531/8384645.json', 'sessions/20201227/18814531/8287370.json', 'sessions/20201227/18814531/8294399.json', 'sessions/20201227/18814531/8288268.json', 'sessions/20201227/18814531/8452177.json', 'sessions/20201227/18814531/8396417.json', 'sessions/20201227/18814531/8525492.json', 'sessions/20201227/18814531/8293275.json', 'sessions/20201227/18814531/8300397.json', 'sessions/20201227/18814531/8292258.json', 'sessions/20201227/18814531/8556407.json', 'sessions/20201227/18814531/8297938.json', 'sessions/20201227/18814531/8630535.json', 'sessions/20201227/18814531/8423891.json', 'sessions/20201227/18814531/8493425.json', 'sessions/20201227/18814531/8297056.json', 'sessions/20201227/18814531/8341910.json', 'sessions/20201227/18814531/8295705.json', 'sessions/20201227/18814531/8576403.json', 'sessions/20201227/18814531/8437406.json', 'sessions/20201227/18814531/8289874.json', 'sessions/20201227/18814531/8291910.json', 'sessions/20201227/18814531/8291571.json', 'sessions/20201227/18814531/8624412.json', 'sessions/20201227/18814531/8505064.json', 'sessions/20201227/18814531/8403381.json', 'sessions/20201227/18814531/8321625.json', 'sessions/20201227/18814531/8290198.json', 'sessions/20201227/18814531/8295375.json', 'sessions/20201227/18814531/8294646.json', 'sessions/20201227/18814531/8592516.json', 'sessions/20201227/18814531/8297601.json', 'sessions/20201227/18814531/8514605.json', 'sessions/20201227/18814531/8287586.json', 'sessions/20201227/18814531/8652837.json', 'sessions/20201227/18814531/8636625.json', 'sessions/20201227/18814531/8292655.json', 'sessions/20201227/18814531/8316822.json', 'sessions/20201227/18814531/8636691.json', 'sessions/20201227/18814531/8290920.json', 'sessions/20201227/18814531/8287948.json', 'sessions/20201227/18814531/8293970.json', 'sessions/20201227/18814531/8667996.json', 'sessions/20201227/18814531/8288832.json', 'sessions/20201227/18814531/8369801.json', 'sessions/20201227/18814531/8299999.json', 'sessions/20201227/18841795/8578859.json', 'sessions/20201227/18841795/8575284.json', 'sessions/20201227/18841795/8646749.json', 'sessions/20201227/18841795/8573501.json', 'sessions/20201227/18841795/8681382.json', 'sessions/20201227/18841795/8568855.json', 'sessions/20201227/18841795/8579593.json', 'sessions/20201227/18841795/8569205.json', 'sessions/20201227/18841795/8579205.json', 'sessions/20201227/18841795/8635210.json', 'sessions/20201227/18841795/8652496.json', 'sessions/20201227/18841795/8588207.json', 'sessions/20201227/18841795/8574565.json', 'sessions/20201227/18841795/8583236.json', 'sessions/20201227/18841795/8573845.json', 'sessions/20201227/18841795/8579926.json', 'sessions/20201227/18841795/8628751.json', 'sessions/20201227/18841795/8576725.json', 'sessions/20201227/18841795/8583155.json', 'sessions/20201227/18841795/8660807.json', 'sessions/20201227/18841795/8570628.json', 'sessions/20201227/18841795/8577417.json', 'sessions/20201227/18841795/8581781.json', 'sessions/20201227/18841795/8580650.json', 'sessions/20201227/18841795/8571346.json', 'sessions/20201227/18841795/8580282.json', 'sessions/20201227/18841795/8576378.json', 'sessions/20201227/18841795/8577181.json', 'sessions/20201227/18841795/8572415.json', 'sessions/20201227/18841795/8610175.json', 'sessions/20201227/18841795/8570275.json', 'sessions/20201227/18841795/8579202.json', 'sessions/20201227/18841795/8572119.json', 'sessions/20201227/18841795/8595781.json', 'sessions/20201227/18841795/8582087.json', 'sessions/20201227/18841795/8578518.json', 'sessions/20201227/18841795/8668019.json', 'sessions/20201227/18841795/8570997.json', 'sessions/20201227/18841795/8575628.json', 'sessions/20201227/18841795/8574982.json', 'sessions/20201227/18841795/8568630.json', 'sessions/20201227/18841795/8569918.json', 'sessions/20201227/18841795/8575983.json', 'sessions/20201227/18841795/8574218.json', 'sessions/20201227/18841795/8577770.json', 'sessions/20201227/18841795/8581013.json', 'sessions/20201227/18841795/8618673.json', 'sessions/20201227/18841795/8582486.json', 'sessions/20201227/18841795/8569564.json', 'sessions/20201227/18841795/8604723.json', 'sessions/20201227/18841795/8679643.json', 'sessions/20201227/18841795/8626605.json', 'sessions/20201227/18841795/8572774.json', 'sessions/20201227/18841795/8582832.json', 'sessions/20201227/18841795/8578155.json', 'sessions/20201227/18841795/8573164.json', 'sessions/20201227/18841795/8581360.json', 'sessions/20201227/18841795/8571712.json', 'sessions/20201227/18689820/8318214.json', 'sessions/20201227/18689820/7595238.json', 'sessions/20201227/18689820/8310366.json', 'sessions/20201227/18689820/7954234.json', 'sessions/20201227/18689820/7694256.json', 'sessions/20201227/18689820/7987891.json', 'sessions/20201227/18689820/7535473.json', 'sessions/20201227/18689820/7768857.json', 'sessions/20201227/18689820/8402624.json', 'sessions/20201227/18689820/8257079.json', 'sessions/20201227/18689820/7797939.json', 'sessions/20201227/18689820/7823421.json', 'sessions/20201227/18689820/8206430.json', 'sessions/20201227/18689820/7984618.json', 'sessions/20201227/18689820/8551104.json', 'sessions/20201227/18689820/7486296.json', 'sessions/20201227/18689820/8519146.json', 'sessions/20201227/18689820/8245245.json', 'sessions/20201227/18689820/7523769.json', 'sessions/20201227/18689820/7671518.json', 'sessions/20201227/18689820/7560832.json', 'sessions/20201227/18689820/8621268.json', 'sessions/20201227/18689820/8191326.json', 'sessions/20201227/18689820/8019037.json', 'sessions/20201227/18689820/7838186.json', 'sessions/20201227/18689820/7643686.json', 'sessions/20201227/18689820/8277067.json', 'sessions/20201227/18689820/8657211.json', 'sessions/20201227/18689820/8467590.json', 'sessions/20201227/18689820/8040691.json', 'sessions/20201227/18689820/7904295.json', 'sessions/20201227/18689820/8412019.json', 'sessions/20201227/18689820/8470058.json', 'sessions/20201227/18689820/8520932.json', 'sessions/20201227/18689820/8160190.json', 'sessions/20201227/18689820/7565885.json', 'sessions/20201227/18689820/8641367.json', 'sessions/20201227/18689820/7781663.json', 'sessions/20201227/18689820/8212716.json', 'sessions/20201227/18689820/8371501.json', 'sessions/20201227/18689820/8030019.json', 'sessions/20201227/18689820/7643117.json', 'sessions/20201227/18689820/8247897.json', 'sessions/20201227/18689820/8368063.json', 'sessions/20201227/18689820/7631357.json', 'sessions/20201227/18689820/7548734.json', 'sessions/20201227/18689820/8153248.json', 'sessions/20201227/18689820/7449334.json', 'sessions/20201227/18689820/8119535.json', 'sessions/20201227/18689820/8500838.json', 'sessions/20201227/18689820/8365309.json', 'sessions/20201227/18689820/8616516.json', 'sessions/20201227/18689820/7597629.json', 'sessions/20201227/18689820/7620635.json', 'sessions/20201227/18689820/7451115.json', 'sessions/20201227/18689820/7658097.json', 'sessions/20201227/18689820/7568507.json', 'sessions/20201227/18689820/7784152.json', 'sessions/20201227/18689820/7857531.json', 'sessions/20201227/18689820/8427035.json', 'sessions/20201227/18689820/7720737.json', 'sessions/20201227/18689820/7450358.json', 'sessions/20201227/18689820/8502234.json', 'sessions/20201227/18689820/8387086.json', 'sessions/20201227/18689820/8293671.json', 'sessions/20201227/18689820/7631696.json', 'sessions/20201227/18689820/7598840.json', 'sessions/20201227/18689820/8563967.json', 'sessions/20201227/18689820/7791754.json', 'sessions/20201227/18689820/7864993.json', 'sessions/20201227/18689820/8107067.json', 'sessions/20201227/18689820/7478670.json', 'sessions/20201227/18689820/7556440.json', 'sessions/20201227/18689820/7575320.json', 'sessions/20201227/18689820/7911860.json', 'sessions/20201227/18689820/7768865.json', 'sessions/20201227/18689820/7504598.json', 'sessions/20201227/18689820/8672336.json', 'sessions/20201227/18689820/7782292.json', 'sessions/20201227/18689820/7711065.json', 'sessions/20201227/18689820/7905497.json', 'sessions/20201227/18689820/8227963.json', 'sessions/20201227/18689820/7752939.json', 'sessions/20201227/18689820/8136258.json', 'sessions/20201227/18689820/7450855.json', 'sessions/20201227/18689820/7451878.json', 'sessions/20201227/18689820/7734700.json', 'sessions/20201227/18689820/8486975.json', 'sessions/20201227/18689820/7608664.json', 'sessions/20201227/18689820/8273329.json', 'sessions/20201227/18689820/7808580.json', 'sessions/20201227/18689820/7450090.json', 'sessions/20201227/18689820/7450593.json', 'sessions/20201227/18689820/8591151.json', 'sessions/20201227/18689820/7449835.json', 'sessions/20201227/18689820/8573504.json', 'sessions/20201227/18689820/8620113.json', 'sessions/20201227/18689820/7900525.json', 'sessions/20201227/18689820/8503260.json', 'sessions/20201227/18689820/7451619.json', 'sessions/20201227/18689820/8074190.json', 'sessions/20201227/18689820/7827365.json', 'sessions/20201227/18689820/7964465.json', 'sessions/20201227/18689820/7682831.json', 'sessions/20201227/18689820/7449585.json', 'sessions/20201227/18689820/7449074.json', 'sessions/20201227/18689820/7587505.json', 'sessions/20201227/18689820/7465766.json', 'sessions/20201227/18689820/8056496.json', 'sessions/20201227/18689820/8436050.json', 'sessions/20201227/18689820/7854396.json', 'sessions/20201227/18689820/7974012.json', 'sessions/20201227/18689820/7709310.json', 'sessions/20201227/18689820/7739499.json', 'sessions/20201227/18689820/8604781.json', 'sessions/20201227/18689820/8106996.json', 'sessions/20201227/18689820/7581015.json', 'sessions/20201227/18689820/7928567.json', 'sessions/20201227/18689820/7925355.json', 'sessions/20201227/18689820/8334359.json', 'sessions/20201227/18689820/8536144.json', 'sessions/20201227/18689820/8002290.json', 'sessions/20201227/18689820/8175149.json', 'sessions/20201227/18689820/8451726.json', 'sessions/20201227/18689820/7828940.json', 'sessions/20201227/18689820/7452137.json', 'sessions/20201227/18689820/8574930.json', 'sessions/20201227/18689820/7904864.json', 'sessions/20201227/18689820/8486339.json', 'sessions/20201227/18689820/8157853.json', 'sessions/20201227/18689820/8093217.json', 'sessions/20201227/18689820/8013212.json', 'sessions/20201227/18689820/8371197.json', 'sessions/20201227/18689820/8079376.json', 'sessions/20201227/18689820/8635989.json', 'sessions/20201227/18689820/7946898.json', 'sessions/20201227/18689820/8254070.json', 'sessions/20201227/18689820/7448598.json', 'sessions/20201227/18689820/7489615.json', 'sessions/20201227/18689820/7542536.json', 'sessions/20201227/18689820/7883875.json', 'sessions/20201227/18689820/8013530.json', 'sessions/20201227/18689820/8348167.json', 'sessions/20201227/18667443/7752643.json', 'sessions/20201227/18667443/7653326.json', 'sessions/20201227/18667443/7352607.json', 'sessions/20201227/18667443/7983978.json', 'sessions/20201227/18667443/7669132.json', 'sessions/20201227/18667443/7706566.json', 'sessions/20201227/18667443/7986310.json', 'sessions/20201227/18667443/7950410.json', 'sessions/20201227/18667443/8537517.json', 'sessions/20201227/18667443/7811680.json', 'sessions/20201227/18667443/8411655.json', 'sessions/20201227/18667443/8651393.json', 'sessions/20201227/18667443/8208068.json', 'sessions/20201227/18667443/8065012.json', 'sessions/20201227/18667443/7708718.json', 'sessions/20201227/18667443/8666577.json', 'sessions/20201227/18667443/8171466.json', 'sessions/20201227/18667443/8045888.json', 'sessions/20201227/18667443/8077783.json', 'sessions/20201227/18667443/7815084.json', 'sessions/20201227/18667443/8519486.json', 'sessions/20201227/18667443/8454635.json', 'sessions/20201227/18667443/8280124.json', 'sessions/20201227/18667443/8587465.json', 'sessions/20201227/18667443/7840122.json', 'sessions/20201227/18667443/7706855.json', 'sessions/20201227/18667443/7824577.json', 'sessions/20201227/18667443/7351122.json', 'sessions/20201227/18667443/7561167.json', 'sessions/20201227/18667443/7560890.json', 'sessions/20201227/18667443/7879185.json', 'sessions/20201227/18667443/7962501.json', 'sessions/20201227/18667443/8176896.json', 'sessions/20201227/18667443/7350882.json', 'sessions/20201227/18667443/7789300.json', 'sessions/20201227/18667443/7702949.json', 'sessions/20201227/18667443/8474991.json', 'sessions/20201227/18667443/8463350.json', 'sessions/20201227/18667443/7704195.json', 'sessions/20201227/18667443/7407353.json', 'sessions/20201227/18667443/7373107.json', 'sessions/20201227/18667443/8102725.json', 'sessions/20201227/18667443/7842231.json', 'sessions/20201227/18667443/8636292.json', 'sessions/20201227/18667443/7352375.json', 'sessions/20201227/18667443/7934560.json', 'sessions/20201227/18667443/7794831.json', 'sessions/20201227/18667443/8467260.json', 'sessions/20201227/18667443/7351369.json', 'sessions/20201227/18667443/8314071.json', 'sessions/20201227/18667443/7608348.json', 'sessions/20201227/18667443/8685778.json', 'sessions/20201227/18667443/8030039.json', 'sessions/20201227/18667443/7598533.json', 'sessions/20201227/18667443/7756294.json', 'sessions/20201227/18667443/7907387.json', 'sessions/20201227/18667443/7908025.json', 'sessions/20201227/18667443/8642465.json', 'sessions/20201227/18667443/8465837.json', 'sessions/20201227/18667443/7859372.json', 'sessions/20201227/18667443/7680183.json', 'sessions/20201227/18667443/8504952.json', 'sessions/20201227/18667443/8426653.json', 'sessions/20201227/18667443/7726525.json', 'sessions/20201227/18667443/7922883.json', 'sessions/20201227/18667443/7616137.json', 'sessions/20201227/18667443/7774081.json', 'sessions/20201227/18667443/8135284.json', 'sessions/20201227/18667443/7632242.json', 'sessions/20201227/18667443/7669457.json', 'sessions/20201227/18667443/7350631.json', 'sessions/20201227/18667443/7467800.json', 'sessions/20201227/18667443/8309635.json', 'sessions/20201227/18667443/7887915.json', 'sessions/20201227/18667443/8482015.json', 'sessions/20201227/18667443/7425934.json', 'sessions/20201227/18667443/7350167.json', 'sessions/20201227/18667443/8541194.json', 'sessions/20201227/18667443/8152284.json', 'sessions/20201227/18667443/7497196.json', 'sessions/20201227/18667443/7589655.json', 'sessions/20201227/18667443/7699084.json', 'sessions/20201227/18667443/7963159.json', 'sessions/20201227/18667443/8244581.json', 'sessions/20201227/18667443/8306886.json', 'sessions/20201227/18667443/8530517.json', 'sessions/20201227/18667443/8636992.json', 'sessions/20201227/18667443/8223598.json', 'sessions/20201227/18667443/8191913.json', 'sessions/20201227/18667443/8369105.json', 'sessions/20201227/18667443/7570895.json', 'sessions/20201227/18667443/7692410.json', 'sessions/20201227/18667443/8116969.json', 'sessions/20201227/18667443/8446505.json', 'sessions/20201227/18667443/8087634.json', 'sessions/20201227/18667443/7741969.json', 'sessions/20201227/18667443/8222561.json', 'sessions/20201227/18667443/7353101.json', 'sessions/20201227/18667443/7534288.json', 'sessions/20201227/18667443/8352346.json', 'sessions/20201227/18667443/8060798.json', 'sessions/20201227/18667443/8389429.json', 'sessions/20201227/18667443/7390049.json', 'sessions/20201227/18667443/8490530.json', 'sessions/20201227/18667443/8264911.json', 'sessions/20201227/18667443/8680715.json', 'sessions/20201227/18667443/8449262.json', 'sessions/20201227/18667443/7546139.json', 'sessions/20201227/18667443/7351872.json', 'sessions/20201227/18667443/7978581.json', 'sessions/20201227/18667443/8156268.json', 'sessions/20201227/18667443/8618029.json', 'sessions/20201227/18667443/8632704.json', 'sessions/20201227/18667443/7578354.json', 'sessions/20201227/18667443/8430770.json', 'sessions/20201227/18667443/7919381.json', 'sessions/20201227/18667443/8674148.json', 'sessions/20201227/18667443/7644307.json', 'sessions/20201227/18667443/8253052.json', 'sessions/20201227/18667443/8215798.json', 'sessions/20201227/18667443/7943429.json', 'sessions/20201227/18667443/8280198.json', 'sessions/20201227/18667443/7445549.json', 'sessions/20201227/18667443/8564696.json', 'sessions/20201227/18667443/7654822.json', 'sessions/20201227/18667443/7517146.json', 'sessions/20201227/18667443/7352111.json', 'sessions/20201227/18667443/7964747.json', 'sessions/20201227/18667443/8524134.json', 'sessions/20201227/18667443/7352853.json', 'sessions/20201227/18667443/8297066.json', 'sessions/20201227/18667443/8639533.json', 'sessions/20201227/18667443/8580726.json', 'sessions/20201227/18667443/8346119.json', 'sessions/20201227/18667443/7629309.json', 'sessions/20201227/18667443/8402256.json', 'sessions/20201227/18667443/8473587.json', 'sessions/20201227/18667443/8241843.json', 'sessions/20201227/18667443/7557056.json', 'sessions/20201227/18667443/7702394.json', 'sessions/20201227/18667443/8603730.json', 'sessions/20201227/18667443/8079032.json', 'sessions/20201227/18667443/8013228.json', 'sessions/20201227/18667443/7933015.json', 'sessions/20201227/18667443/7892655.json', 'sessions/20201227/18667443/8400250.json', 'sessions/20201227/18667443/7375111.json', 'sessions/20201227/18667443/7627814.json', 'sessions/20201227/18667443/8564992.json', 'sessions/20201227/18667443/7351614.json', 'sessions/20201227/18667443/7709891.json', 'sessions/20201227/18667443/8384661.json', 'sessions/20201227/18667443/8547899.json', 'sessions/20201227/18667443/7997191.json', 'sessions/20201227/18667443/7842238.json', 'sessions/20201227/18667443/8133972.json', 'sessions/20201227/18667443/7477920.json', 'sessions/20201227/18667443/8330252.json', 'sessions/20201227/18667443/7710809.json', 'sessions/20201227/18818826/8337039.json', 'sessions/20201227/18818826/8486642.json', 'sessions/20201227/18818826/8331933.json', 'sessions/20201227/18818826/8339457.json', 'sessions/20201227/18818826/8583221.json', 'sessions/20201227/18818826/8532646.json', 'sessions/20201227/18818826/8340165.json', 'sessions/20201227/18818826/8449695.json', 'sessions/20201227/18818826/8334967.json', 'sessions/20201227/18818826/8358353.json', 'sessions/20201227/18818826/8390882.json', 'sessions/20201227/18818826/8636328.json', 'sessions/20201227/18818826/8629434.json', 'sessions/20201227/18818826/8550743.json', 'sessions/20201227/18818826/8419264.json', 'sessions/20201227/18818826/8334321.json', 'sessions/20201227/18818826/8333260.json', 'sessions/20201227/18818826/8567514.json', 'sessions/20201227/18818826/8406164.json', 'sessions/20201227/18818826/8349587.json', 'sessions/20201227/18818826/8332925.json', 'sessions/20201227/18818826/8331523.json', 'sessions/20201227/18818826/8329806.json', 'sessions/20201227/18818826/8330164.json', 'sessions/20201227/18818826/8340909.json', 'sessions/20201227/18818826/8341524.json', 'sessions/20201227/18818826/8365919.json', 'sessions/20201227/18818826/8336356.json', 'sessions/20201227/18818826/8336004.json', 'sessions/20201227/18818826/8607202.json', 'sessions/20201227/18818826/8416948.json', 'sessions/20201227/18818826/8488769.json', 'sessions/20201227/18818826/8484588.json', 'sessions/20201227/18818826/8394327.json', 'sessions/20201227/18818826/8599689.json', 'sessions/20201227/18818826/8330503.json', 'sessions/20201227/18818826/8452791.json', 'sessions/20201227/18818826/8331191.json', 'sessions/20201227/18818826/8335664.json', 'sessions/20201227/18818826/8657554.json', 'sessions/20201227/18818826/8469361.json', 'sessions/20201227/18818826/8429007.json', 'sessions/20201227/18818826/8517775.json', 'sessions/20201227/18818826/8502577.json', 'sessions/20201227/18818826/8683572.json', 'sessions/20201227/18818826/8339107.json', 'sessions/20201227/18818826/8454963.json', 'sessions/20201227/18818826/8332589.json', 'sessions/20201227/18818826/8622251.json', 'sessions/20201227/18818826/8339812.json', 'sessions/20201227/18818826/8337733.json', 'sessions/20201227/18818826/8417967.json', 'sessions/20201227/18818826/8535774.json', 'sessions/20201227/18818826/8338166.json', 'sessions/20201227/18818826/8672655.json', 'sessions/20201227/18818826/8334626.json', 'sessions/20201227/18818826/8395685.json', 'sessions/20201227/18818826/8341177.json', 'sessions/20201227/18818826/8338771.json', 'sessions/20201227/18818826/8329588.json', 'sessions/20201227/18818826/8342603.json', 'sessions/20201227/18818826/8338419.json', 'sessions/20201227/18818826/8340540.json', 'sessions/20201227/18818826/8341900.json', 'sessions/20201227/18818826/8333941.json', 'sessions/20201227/18818826/8342906.json', 'sessions/20201227/18818826/8332223.json', 'sessions/20201227/18818826/8453465.json', 'sessions/20201227/18818826/8641692.json', 'sessions/20201227/18818826/8382192.json', 'sessions/20201227/18818826/8335334.json', 'sessions/20201227/18818826/8330835.json', 'sessions/20201227/18818826/8333599.json', 'sessions/20201227/18818826/8336715.json', 'sessions/20201227/18818826/8373923.json', 'sessions/20201227/18818826/8342287.json', 'sessions/20201227/18818826/8337391.json', 'sessions/20201227/18818826/8361836.json', 'sessions/20201227/18818826/8439873.json', 'sessions/20201227/18818826/8632364.json', 'sessions/20201227/18808528/8243535.json', 'sessions/20201227/18808528/8237418.json', 'sessions/20201227/18808528/8255009.json', 'sessions/20201227/18808528/8240122.json', 'sessions/20201227/18808528/8235059.json', 'sessions/20201227/18808528/8239168.json', 'sessions/20201227/18808528/8537864.json', 'sessions/20201227/18808528/8520905.json', 'sessions/20201227/18808528/8545700.json', 'sessions/20201227/18808528/8233705.json', 'sessions/20201227/18808528/8322688.json', 'sessions/20201227/18808528/8405841.json', 'sessions/20201227/18808528/8231294.json', 'sessions/20201227/18808528/8643478.json', 'sessions/20201227/18808528/8422134.json', 'sessions/20201227/18808528/8245946.json', 'sessions/20201227/18808528/8488061.json', 'sessions/20201227/18808528/8238105.json', 'sessions/20201227/18808528/8355983.json', 'sessions/20201227/18808528/8253373.json', 'sessions/20201227/18808528/8291955.json', 'sessions/20201227/18808528/8234075.json', 'sessions/20201227/18808528/8606204.json', 'sessions/20201227/18808528/8234366.json', 'sessions/20201227/18808528/8232318.json', 'sessions/20201227/18808528/8238437.json', 'sessions/20201227/18808528/8307539.json', 'sessions/20201227/18808528/8534700.json', 'sessions/20201227/18808528/8263849.json', 'sessions/20201227/18808528/8451718.json', 'sessions/20201227/18808528/8236393.json', 'sessions/20201227/18808528/8502242.json', 'sessions/20201227/18808528/8288335.json', 'sessions/20201227/18808528/8236760.json', 'sessions/20201227/18808528/8628002.json', 'sessions/20201227/18808528/8241489.json', 'sessions/20201227/18808528/8410262.json', 'sessions/20201227/18808528/8451806.json', 'sessions/20201227/18808528/8235382.json', 'sessions/20201227/18808528/8513521.json', 'sessions/20201227/18808528/8241128.json', 'sessions/20201227/18808528/8234706.json', 'sessions/20201227/18808528/8655379.json', 'sessions/20201227/18808528/8279182.json', 'sessions/20201227/18808528/8236078.json', 'sessions/20201227/18808528/8230952.json', 'sessions/20201227/18808528/8340932.json', 'sessions/20201227/18808528/8483891.json', 'sessions/20201227/18808528/8467604.json', 'sessions/20201227/18808528/8233015.json', 'sessions/20201227/18808528/8231643.json', 'sessions/20201227/18808528/8309600.json', 'sessions/20201227/18808528/8237109.json', 'sessions/20201227/18808528/8301436.json', 'sessions/20201227/18808528/8382930.json', 'sessions/20201227/18808528/8230399.json', 'sessions/20201227/18808528/8231971.json', 'sessions/20201227/18808528/8239842.json', 'sessions/20201227/18808528/8408943.json', 'sessions/20201227/18808528/8237759.json', 'sessions/20201227/18808528/8573504.json', 'sessions/20201227/18808528/8433263.json', 'sessions/20201227/18808528/8241813.json', 'sessions/20201227/18808528/8670473.json', 'sessions/20201227/18808528/8239458.json', 'sessions/20201227/18808528/8233346.json', 'sessions/20201227/18808528/8238859.json', 'sessions/20201227/18808528/8248960.json', 'sessions/20201227/18808528/8589618.json', 'sessions/20201227/18808528/8272354.json', 'sessions/20201227/18808528/8240462.json', 'sessions/20201227/18808528/8392215.json', 'sessions/20201227/18808528/8517818.json', 'sessions/20201227/18808528/8672389.json', 'sessions/20201227/18808528/8557211.json', 'sessions/20201227/18808528/8243886.json', 'sessions/20201227/18808528/8240802.json', 'sessions/20201227/18808528/8369732.json', 'sessions/20201227/18808528/8261830.json', 'sessions/20201227/18808528/8328140.json', 'sessions/20201227/18808528/8232658.json', 'sessions/20201227/18808528/8235719.json', 'sessions/20201227/18808528/8502251.json', 'sessions/20201227/18808528/8626599.json', 'sessions/20201227/18808528/8242188.json', 'sessions/20201227/18808528/8612286.json', 'sessions/20201227/18808528/8436694.json']\n",
      "error while parse session sessions/20201227/18775092/8354985.json : \n",
      "error while parse session sessions/20201227/18775092/7991811.json : \n",
      "error while parse session sessions/20201227/18775092/8054877.json : \n",
      "error while parse session sessions/20201227/18775092/8005498.json : \n",
      "error while parse session sessions/20201227/18775092/7995939.json : \n",
      "error while parse session sessions/20201227/18775092/7991414.json : \n",
      "error while parse session sessions/20201227/18775092/8002300.json : \n",
      "error while parse session sessions/20201227/18689820/8030019.json : \n",
      "error while parse session sessions/20201227/18689820/8616516.json : \n",
      "error while parse session sessions/20201227/18689820/8591151.json : \n",
      "error while parse session sessions/20201227/18689820/8604781.json : \n",
      "error while parse session sessions/20201227/18689820/8536144.json : \n",
      "error while parse session sessions/20201227/18689820/7904864.json : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 765/765 [37:44<00:00,  2.96s/it]\n",
      "100%|██████████| 191/191 [08:34<00:00,  2.69s/it]\n"
     ]
    }
   ],
   "source": [
    "datasets_top10 = snowden.collect_dataset_from_dir(\n",
    "    neural_agent_class,\n",
    "    'sessions/20201227/',\n",
    "    val_ratio=2/10,\n",
    "    n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "train10 = make_td_dataset(datasets_top10[0])\n",
    "val10 = make_td_dataset(datasets_top10[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "train10.save_dataset('datasets/td_train_top_10_20201227')\n",
    "val10.save_dataset('datasets/td_val_top_10_20201227')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1529236, 144)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train10.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 500 batch: 0.31017\n",
      "val loss in 1 epoch: 0.32012\n",
      "train loss in 1 epoch in 1000 batch: 0.33459\n",
      "val loss in 1 epoch: 0.31990\n",
      "train loss in 2 epoch in 500 batch: 0.29971\n",
      "val loss in 2 epoch: 0.31826\n",
      "train loss in 2 epoch in 1000 batch: 0.31261\n",
      "val loss in 2 epoch: 0.31808\n",
      "train loss in 3 epoch in 500 batch: 0.30555\n",
      "val loss in 3 epoch: 0.31801\n",
      "train loss in 3 epoch in 1000 batch: 0.31830\n",
      "val loss in 3 epoch: 0.31801\n",
      "train loss in 4 epoch in 500 batch: 0.32305\n",
      "val loss in 4 epoch: 0.31802\n",
      "train loss in 4 epoch in 1000 batch: 0.31833\n",
      "val loss in 4 epoch: 0.31803\n",
      "train loss in 5 epoch in 500 batch: 0.29391\n",
      "val loss in 5 epoch: 0.31804\n",
      "train loss in 5 epoch in 1000 batch: 0.32989\n",
      "val loss in 5 epoch: 0.31804\n",
      "train loss in 6 epoch in 500 batch: 0.32324\n",
      "val loss in 6 epoch: 0.31805\n",
      "train loss in 6 epoch in 1000 batch: 0.29496\n",
      "val loss in 6 epoch: 0.31805\n",
      "train loss in 7 epoch in 500 batch: 0.30279\n",
      "val loss in 7 epoch: 0.31806\n",
      "train loss in 7 epoch in 1000 batch: 0.32812\n",
      "val loss in 7 epoch: 0.31806\n",
      "train loss in 8 epoch in 500 batch: 0.31834\n",
      "val loss in 8 epoch: 0.31806\n",
      "train loss in 8 epoch in 1000 batch: 0.31055\n",
      "val loss in 8 epoch: 0.31806\n",
      "train loss in 9 epoch in 500 batch: 0.31836\n",
      "val loss in 9 epoch: 0.31806\n",
      "train loss in 9 epoch in 1000 batch: 0.31250\n",
      "val loss in 9 epoch: 0.31806\n",
      "train loss in 10 epoch in 500 batch: 0.29492\n",
      "val loss in 10 epoch: 0.31806\n",
      "train loss in 10 epoch in 1000 batch: 0.32324\n",
      "val loss in 10 epoch: 0.31806\n"
     ]
    }
   ],
   "source": [
    "# mse loss\n",
    "INPUT_F = 72\n",
    "H = 256\n",
    "DROP_P = 0.01\n",
    "\n",
    "model = TDModel(INPUT_F, DROP_P, H)\n",
    "\n",
    "learn(train10, val10, model, freq=500, batch_size=1024,lr=1e-5, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.model.state_dict(), 'models/nagiss_v7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean before resampling: 0.3173560636527414\n",
      "mean after resampling: 0.5\n",
      "mean before resampling: 0.31813026932314326\n",
      "mean after resampling: 0.5\n"
     ]
    }
   ],
   "source": [
    "train_top10 = snowden.resample_eq(datasets_top10[0])\n",
    "val_top10 = snowden.resample_eq(datasets_top10[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 500 batch: 0.22432\n",
      "val loss in 1 epoch: 0.22639\n",
      "train loss in 1 epoch in 1000 batch: 0.22962\n",
      "val loss in 1 epoch: 0.22300\n",
      "train loss in 1 epoch in 1500 batch: 0.23037\n",
      "val loss in 1 epoch: 0.22201\n",
      "train loss in 1 epoch in 2000 batch: 0.22566\n",
      "val loss in 1 epoch: 0.22158\n",
      "train loss in 1 epoch in 2500 batch: 0.22240\n",
      "val loss in 1 epoch: 0.22137\n",
      "train loss in 1 epoch in 3000 batch: 0.22608\n",
      "val loss in 1 epoch: 0.22123\n",
      "train loss in 1 epoch in 3500 batch: 0.23233\n",
      "val loss in 1 epoch: 0.22114\n",
      "train loss in 2 epoch in 500 batch: 0.21473\n",
      "val loss in 2 epoch: 0.22107\n",
      "train loss in 2 epoch in 1000 batch: 0.22262\n",
      "val loss in 2 epoch: 0.22105\n",
      "train loss in 2 epoch in 1500 batch: 0.24129\n",
      "val loss in 2 epoch: 0.22099\n",
      "train loss in 2 epoch in 2000 batch: 0.23092\n",
      "val loss in 2 epoch: 0.22098\n",
      "train loss in 2 epoch in 2500 batch: 0.26149\n",
      "val loss in 2 epoch: 0.22096\n",
      "train loss in 2 epoch in 3000 batch: 0.20146\n",
      "val loss in 2 epoch: 0.22094\n",
      "train loss in 2 epoch in 3500 batch: 0.21546\n",
      "val loss in 2 epoch: 0.22092\n",
      "train loss in 3 epoch in 500 batch: 0.22889\n",
      "val loss in 3 epoch: 0.22096\n",
      "train loss in 3 epoch in 1000 batch: 0.21815\n",
      "val loss in 3 epoch: 0.22089\n",
      "train loss in 3 epoch in 1500 batch: 0.22937\n",
      "val loss in 3 epoch: 0.22090\n",
      "train loss in 3 epoch in 2000 batch: 0.21618\n",
      "val loss in 3 epoch: 0.22088\n",
      "train loss in 3 epoch in 2500 batch: 0.21777\n",
      "val loss in 3 epoch: 0.22087\n",
      "train loss in 3 epoch in 3000 batch: 0.22356\n",
      "val loss in 3 epoch: 0.22085\n",
      "train loss in 3 epoch in 3500 batch: 0.21060\n",
      "val loss in 3 epoch: 0.22085\n",
      "train loss in 4 epoch in 500 batch: 0.21423\n",
      "val loss in 4 epoch: 0.22085\n",
      "train loss in 4 epoch in 1000 batch: 0.22720\n",
      "val loss in 4 epoch: 0.22085\n",
      "train loss in 4 epoch in 1500 batch: 0.23480\n",
      "val loss in 4 epoch: 0.22085\n",
      "train loss in 4 epoch in 2000 batch: 0.22925\n",
      "val loss in 4 epoch: 0.22084\n",
      "train loss in 4 epoch in 2500 batch: 0.23832\n",
      "val loss in 4 epoch: 0.22080\n",
      "train loss in 4 epoch in 3000 batch: 0.22606\n",
      "val loss in 4 epoch: 0.22082\n",
      "train loss in 4 epoch in 3500 batch: 0.21509\n",
      "val loss in 4 epoch: 0.22080\n",
      "train loss in 5 epoch in 500 batch: 0.23412\n",
      "val loss in 5 epoch: 0.22079\n",
      "train loss in 5 epoch in 1000 batch: 0.22845\n",
      "val loss in 5 epoch: 0.22078\n",
      "train loss in 5 epoch in 1500 batch: 0.21019\n",
      "val loss in 5 epoch: 0.22078\n",
      "train loss in 5 epoch in 2000 batch: 0.22251\n",
      "val loss in 5 epoch: 0.22078\n",
      "train loss in 5 epoch in 2500 batch: 0.22220\n",
      "val loss in 5 epoch: 0.22077\n",
      "train loss in 5 epoch in 3000 batch: 0.22535\n",
      "val loss in 5 epoch: 0.22078\n",
      "train loss in 5 epoch in 3500 batch: 0.22404\n",
      "val loss in 5 epoch: 0.22076\n",
      "train loss in 6 epoch in 500 batch: 0.22602\n",
      "val loss in 6 epoch: 0.22078\n",
      "train loss in 6 epoch in 1000 batch: 0.22982\n",
      "val loss in 6 epoch: 0.22076\n",
      "train loss in 6 epoch in 1500 batch: 0.21321\n",
      "val loss in 6 epoch: 0.22075\n",
      "train loss in 6 epoch in 2000 batch: 0.19024\n",
      "val loss in 6 epoch: 0.22075\n",
      "train loss in 6 epoch in 2500 batch: 0.21453\n",
      "val loss in 6 epoch: 0.22075\n",
      "train loss in 6 epoch in 3000 batch: 0.21286\n",
      "val loss in 6 epoch: 0.22075\n",
      "train loss in 6 epoch in 3500 batch: 0.23923\n",
      "val loss in 6 epoch: 0.22076\n",
      "train loss in 7 epoch in 500 batch: 0.22891\n",
      "val loss in 7 epoch: 0.22074\n",
      "train loss in 7 epoch in 1000 batch: 0.24740\n",
      "val loss in 7 epoch: 0.22073\n",
      "train loss in 7 epoch in 1500 batch: 0.23805\n",
      "val loss in 7 epoch: 0.22074\n",
      "train loss in 7 epoch in 2000 batch: 0.22206\n",
      "val loss in 7 epoch: 0.22073\n",
      "train loss in 7 epoch in 2500 batch: 0.21476\n",
      "val loss in 7 epoch: 0.22074\n",
      "train loss in 7 epoch in 3000 batch: 0.22108\n",
      "val loss in 7 epoch: 0.22075\n",
      "train loss in 7 epoch in 3500 batch: 0.22214\n",
      "val loss in 7 epoch: 0.22072\n",
      "train loss in 8 epoch in 500 batch: 0.23528\n",
      "val loss in 8 epoch: 0.22073\n",
      "train loss in 8 epoch in 1000 batch: 0.23089\n",
      "val loss in 8 epoch: 0.22074\n",
      "train loss in 8 epoch in 1500 batch: 0.21275\n",
      "val loss in 8 epoch: 0.22072\n",
      "train loss in 8 epoch in 2000 batch: 0.21285\n",
      "val loss in 8 epoch: 0.22072\n",
      "train loss in 8 epoch in 2500 batch: 0.23114\n",
      "val loss in 8 epoch: 0.22072\n",
      "train loss in 8 epoch in 3000 batch: 0.21439\n",
      "val loss in 8 epoch: 0.22071\n",
      "train loss in 8 epoch in 3500 batch: 0.21583\n",
      "val loss in 8 epoch: 0.22071\n",
      "train loss in 9 epoch in 500 batch: 0.21554\n",
      "val loss in 9 epoch: 0.22070\n",
      "train loss in 9 epoch in 1000 batch: 0.23273\n",
      "val loss in 9 epoch: 0.22072\n",
      "train loss in 9 epoch in 1500 batch: 0.22058\n",
      "val loss in 9 epoch: 0.22070\n",
      "train loss in 9 epoch in 2000 batch: 0.22258\n",
      "val loss in 9 epoch: 0.22070\n",
      "train loss in 9 epoch in 2500 batch: 0.23532\n",
      "val loss in 9 epoch: 0.22069\n",
      "train loss in 9 epoch in 3000 batch: 0.22795\n",
      "val loss in 9 epoch: 0.22072\n",
      "train loss in 9 epoch in 3500 batch: 0.20389\n",
      "val loss in 9 epoch: 0.22071\n",
      "train loss in 10 epoch in 500 batch: 0.22921\n",
      "val loss in 10 epoch: 0.22070\n",
      "train loss in 10 epoch in 1000 batch: 0.21494\n",
      "val loss in 10 epoch: 0.22070\n",
      "train loss in 10 epoch in 1500 batch: 0.21625\n",
      "val loss in 10 epoch: 0.22070\n",
      "train loss in 10 epoch in 2000 batch: 0.21935\n",
      "val loss in 10 epoch: 0.22070\n",
      "train loss in 10 epoch in 2500 batch: 0.20828\n",
      "val loss in 10 epoch: 0.22069\n",
      "train loss in 10 epoch in 3000 batch: 0.21921\n",
      "val loss in 10 epoch: 0.22069\n",
      "train loss in 10 epoch in 3500 batch: 0.22478\n",
      "val loss in 10 epoch: 0.22069\n"
     ]
    }
   ],
   "source": [
    "# mse loss\n",
    "INPUT_F = 72\n",
    "H = 256\n",
    "DROP_P = 0.01\n",
    "\n",
    "model = NNWithCusomFeatures(INPUT_F, DROP_P, H)\n",
    "\n",
    "learn(train_top10, val_top10, model, freq=500, batch_size=256,lr=1e-5, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "667.0 635.0 tmp/b_0.5915544147234911.py tmp/b_0.6092418235378909.py\n",
      "677.0 631.0 tmp/b_0.5915544147234911.py tmp/b_0.6092418235378909.py\n",
      "639.0 649.0 tmp/b_0.5915544147234911.py tmp/b_0.6092418235378909.py\n",
      "598.0 611.0 tmp/b_0.5915544147234911.py tmp/b_0.6092418235378909.py\n",
      "580.0 571.0 tmp/b_0.5915544147234911.py tmp/b_0.6092418235378909.py\n",
      "563.0 533.0 tmp/b_0.5915544147234911.py tmp/b_0.6092418235378909.py\n",
      "614.0 625.0 tmp/b_0.5915544147234911.py tmp/b_0.6092418235378909.py\n",
      "643.0 620.0 tmp/b_0.5915544147234911.py tmp/b_0.6092418235378909.py\n",
      "613.0 616.0 tmp/b_0.5915544147234911.py tmp/b_0.6092418235378909.py\n",
      "584.0 604.0 tmp/b_0.5915544147234911.py tmp/b_0.6092418235378909.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.22897157536219315,\n",
       " 8.3,\n",
       " 21.817653402692052,\n",
       " 0.5,\n",
       " 'tmp/b_0.5915544147234911.py',\n",
       " 'tmp/b_0.6092418235378909.py',\n",
       " array([667., 677., 639., 598., 580., 563., 614., 643., 613., 584.]),\n",
       " array([635., 631., 649., 611., 571., 533., 625., 620., 616., 604.]))"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v6 feature\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_feature_nagiss,use_mean=True,True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=utils.bandits.neural),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601.0 621.0 tmp/b_0.041844375173799464.py tmp/b_0.928490600352431.py\n",
      "595.0 565.0 tmp/b_0.041844375173799464.py tmp/b_0.928490600352431.py\n",
      "599.0 715.0 tmp/b_0.041844375173799464.py tmp/b_0.928490600352431.py\n",
      "521.0 627.0 tmp/b_0.041844375173799464.py tmp/b_0.928490600352431.py\n",
      "601.0 633.0 tmp/b_0.041844375173799464.py tmp/b_0.928490600352431.py\n",
      "540.0 651.0 tmp/b_0.041844375173799464.py tmp/b_0.928490600352431.py\n",
      "487.0 603.0 tmp/b_0.041844375173799464.py tmp/b_0.928490600352431.py\n",
      "606.0 609.0 tmp/b_0.041844375173799464.py tmp/b_0.928490600352431.py\n",
      "564.0 610.0 tmp/b_0.041844375173799464.py tmp/b_0.928490600352431.py\n",
      "572.0 612.0 tmp/b_0.041844375173799464.py tmp/b_0.928490600352431.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0004145997283126856,\n",
       " -56.0,\n",
       " 50.157751145760116,\n",
       " 0.1,\n",
       " 'tmp/b_0.041844375173799464.py',\n",
       " 'tmp/b_0.928490600352431.py',\n",
       " array([601., 595., 599., 521., 601., 540., 487., 606., 564., 572.]),\n",
       " array([621., 565., 715., 627., 633., 651., 603., 609., 610., 612.]))"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v6 only\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=utils.bandits.neural),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gittins', (2.2338113477826215e-05, 41.9, 31.2488399784696)]\n",
      "['gittins_with_random', (0.04874871594922682, 26.2, 42.039980970499975)]\n",
      "['gittins_with_count_my', (0.040665651939938506, 21.5, 33.21520736048475)]\n",
      "['gittins_with_count_rival_drift', (0.14417327148436906, 20.7, 44.82198121457819)]\n",
      "['gittins_with_my_and_count_rival_drift', (0.17334983983393915, 13.4, 31.122981862283055)]\n",
      "['gittins_bb', (0.3269775255193714, -13.3, 42.9069924371308)]\n",
      "['gittins_bb_delta', (0.671288128512241, -4.0, 29.80603965641863)]\n",
      "['softmax_ucb', (7.764515993879431e-16, 60.3, 23.664530420018902)]\n",
      "['multiarmed_bandit_agent', (1.0, 0.0, 19.662146373170962)]\n",
      "['upper_confidence', (4.041218931197669e-47, 159.2, 34.91933561796387)]\n",
      "['ucb_decay', (1.0455956308323003e-06, 57.1, 36.97958896472485)]\n",
      "['bayesian_ucb', (0.40061517620458387, -8.1, 30.47441549890662)]\n",
      "['thompson', (0.7564081166054748, -2.8, 28.54400112107621)]\n",
      "['neural', (0.7840180300583126, -2.9, 33.45878061137316)]\n",
      "<string>:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "<string>:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "<string>:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "<string>:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "<string>:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "<string>:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "<string>:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "<string>:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "<string>:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "<string>:82: RuntimeWarning: invalid value encountered in true_divide\n",
      "['optimized_ucb', (2.4172973961320224e-11, 81.2, 38.4494473302283)]\n",
      "['exact_gittins', (0.00113454255850436, 23.1, 22.443039009902378)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['gittins', (2.2338113477826215e-05, 41.9, 31.2488399784696)],\n",
       " ['gittins_with_random', (0.04874871594922682, 26.2, 42.039980970499975)],\n",
       " ['gittins_with_count_my', (0.040665651939938506, 21.5, 33.21520736048475)],\n",
       " ['gittins_with_count_rival_drift',\n",
       "  (0.14417327148436906, 20.7, 44.82198121457819)],\n",
       " ['gittins_with_my_and_count_rival_drift',\n",
       "  (0.17334983983393915, 13.4, 31.122981862283055)],\n",
       " ['gittins_bb', (0.3269775255193714, -13.3, 42.9069924371308)],\n",
       " ['gittins_bb_delta', (0.671288128512241, -4.0, 29.80603965641863)],\n",
       " ['softmax_ucb', (7.764515993879431e-16, 60.3, 23.664530420018902)],\n",
       " ['multiarmed_bandit_agent', (1.0, 0.0, 19.662146373170962)],\n",
       " ['upper_confidence', (4.041218931197669e-47, 159.2, 34.91933561796387)],\n",
       " ['ucb_decay', (1.0455956308323003e-06, 57.1, 36.97958896472485)],\n",
       " ['bayesian_ucb', (0.40061517620458387, -8.1, 30.47441549890662)],\n",
       " ['thompson', (0.7564081166054748, -2.8, 28.54400112107621)],\n",
       " ['neural', (0.7840180300583126, -2.9, 33.45878061137316)],\n",
       " ['optimized_ucb', (2.4172973961320224e-11, 81.2, 38.4494473302283)],\n",
       " ['exact_gittins', (0.00113454255850436, 23.1, 22.443039009902378)]]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.bandits.bench(\n",
    "    utils.bandits.Agent(\n",
    "        text=utils.bandits.neural.format(\"use_feature_nagiss,use_mean=True,True\", \"\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sessions/nagiss/20201226/ ['sessions/nagiss/20201226/8389416.json', 'sessions/nagiss/20201226/8528396.json', 'sessions/nagiss/20201226/8415103.json', 'sessions/nagiss/20201226/8354920.json', 'sessions/nagiss/20201226/8499335.json', 'sessions/nagiss/20201226/8441958.json', 'sessions/nagiss/20201226/8377070.json', 'sessions/nagiss/20201226/8513521.json', 'sessions/nagiss/20201226/8364900.json', 'sessions/nagiss/20201226/8428032.json', 'sessions/nagiss/20201226/8485987.json', 'sessions/nagiss/20201226/8455991.json', 'sessions/nagiss/20201226/8471153.json', 'sessions/nagiss/20201226/8401973.json', 'sessions/nagiss/20201226/.ipynb_checkpoints/8354920-checkpoint.json']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n",
      "  7%|▋         | 1/14 [00:10<02:21, 10.87s/it]\u001b[A\n",
      " 14%|█▍        | 2/14 [00:22<02:11, 10.98s/it]\u001b[A\n",
      " 21%|██▏       | 3/14 [00:33<02:01, 11.09s/it]\u001b[A\n",
      " 29%|██▊       | 4/14 [00:46<01:55, 11.56s/it]\u001b[A\n",
      " 36%|███▌      | 5/14 [00:57<01:43, 11.53s/it]\u001b[A\n",
      " 43%|████▎     | 6/14 [01:09<01:33, 11.67s/it]\u001b[A\n",
      " 50%|█████     | 7/14 [01:22<01:25, 12.18s/it]\u001b[A\n",
      " 57%|█████▋    | 8/14 [01:35<01:13, 12.20s/it]\u001b[A\n",
      " 64%|██████▍   | 9/14 [01:47<01:01, 12.25s/it]\u001b[A\n",
      " 71%|███████▏  | 10/14 [01:58<00:47, 11.84s/it]\u001b[A\n",
      " 79%|███████▊  | 11/14 [02:08<00:34, 11.38s/it]\u001b[A\n",
      " 86%|████████▌ | 12/14 [02:18<00:21, 10.98s/it]\u001b[A\n",
      " 93%|█████████▎| 13/14 [02:29<00:10, 10.94s/it]\u001b[A\n",
      "100%|██████████| 14/14 [02:40<00:00, 11.45s/it]\u001b[A\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████| 1/1 [00:11<00:00, 11.04s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "datasets = snowden.collect_dataset_from_dir(neural_agent_class, 'sessions/nagiss/20201226/', 1/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets[0].save_dataset('datasets/all_actions_train_nagiss_20201226')\n",
    "# datasets[1].save_dataset('datasets/all_actions_val_nagiss_20201226')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = snowden.Dataset.load_dataset('datasets/all_actions_train_nagiss_20201226.npy')\n",
    "val = snowden.Dataset.load_dataset('datasets/all_actions_val_nagiss_20201226.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 1999)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1999, 100, 72), (1999,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val.X.shape, val.actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 200 batch: 2.44914\n",
      "val loss in 1 epoch: 3.19189\n",
      "train loss in 1 epoch in 400 batch: 2.12746\n",
      "val loss in 1 epoch: 3.11925\n",
      "train loss in 2 epoch in 200 batch: 2.52760\n",
      "val loss in 2 epoch: 3.08872\n",
      "train loss in 2 epoch in 400 batch: 2.57668\n",
      "val loss in 2 epoch: 3.01324\n",
      "train loss in 3 epoch in 200 batch: 2.47165\n",
      "val loss in 3 epoch: 2.98740\n",
      "train loss in 3 epoch in 400 batch: 3.53690\n",
      "val loss in 3 epoch: 2.99072\n",
      "train loss in 4 epoch in 200 batch: 2.38290\n",
      "val loss in 4 epoch: 2.94655\n",
      "train loss in 4 epoch in 400 batch: 1.71899\n",
      "val loss in 4 epoch: 2.95541\n",
      "train loss in 5 epoch in 200 batch: 3.12862\n",
      "val loss in 5 epoch: 2.96041\n",
      "train loss in 5 epoch in 400 batch: 2.82046\n",
      "val loss in 5 epoch: 2.93376\n",
      "train loss in 6 epoch in 200 batch: 2.19779\n",
      "val loss in 6 epoch: 2.91537\n",
      "train loss in 6 epoch in 400 batch: 3.29692\n",
      "val loss in 6 epoch: 2.89947\n",
      "train loss in 7 epoch in 200 batch: 1.82034\n",
      "val loss in 7 epoch: 2.90066\n",
      "train loss in 7 epoch in 400 batch: 2.36200\n",
      "val loss in 7 epoch: 2.86088\n",
      "train loss in 8 epoch in 200 batch: 2.04600\n",
      "val loss in 8 epoch: 2.92264\n",
      "train loss in 8 epoch in 400 batch: 2.86133\n",
      "val loss in 8 epoch: 2.87558\n",
      "train loss in 9 epoch in 200 batch: 2.33739\n",
      "val loss in 9 epoch: 2.84630\n",
      "train loss in 9 epoch in 400 batch: 2.01285\n",
      "val loss in 9 epoch: 2.86971\n",
      "train loss in 10 epoch in 200 batch: 3.03405\n",
      "val loss in 10 epoch: 2.88198\n",
      "train loss in 10 epoch in 400 batch: 2.90262\n",
      "val loss in 10 epoch: 2.87311\n"
     ]
    }
   ],
   "source": [
    "# mse loss\n",
    "INPUT_F = 72\n",
    "H = 256\n",
    "DROP_P = 0.1\n",
    "\n",
    "model = NNWithCusomFeatures(INPUT_F, DROP_P, H)\n",
    "\n",
    "def policy_gradient(logits_batch, rewards_batch, actions_batch):\n",
    "    loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    logits_batch = torch.reshape(logits_batch, (-1, 100))\n",
    "    policy = loss(logits_batch, actions_batch)\n",
    "    out = torch.sum(policy * rewards_batch) / torch.sum(rewards_batch)\n",
    "    return out\n",
    "\n",
    "learn(train, val, model, freq=200, batch_size=64,lr=1e-4, epochs=10, criterion=policy_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657.0 673.0 tmp/b_0.5539695728453294.py tmp/b_0.3664986483537218.py\n",
      "662.0 651.0 tmp/b_0.5539695728453294.py tmp/b_0.3664986483537218.py\n",
      "602.0 600.0 tmp/b_0.5539695728453294.py tmp/b_0.3664986483537218.py\n",
      "556.0 565.0 tmp/b_0.5539695728453294.py tmp/b_0.3664986483537218.py\n",
      "640.0 594.0 tmp/b_0.5539695728453294.py tmp/b_0.3664986483537218.py\n",
      "645.0 669.0 tmp/b_0.5539695728453294.py tmp/b_0.3664986483537218.py\n",
      "622.0 661.0 tmp/b_0.5539695728453294.py tmp/b_0.3664986483537218.py\n",
      "562.0 509.0 tmp/b_0.5539695728453294.py tmp/b_0.3664986483537218.py\n",
      "629.0 607.0 tmp/b_0.5539695728453294.py tmp/b_0.3664986483537218.py\n",
      "625.0 653.0 tmp/b_0.5539695728453294.py tmp/b_0.3664986483537218.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.847144391522459,\n",
       " 1.8,\n",
       " 29.52896882723811,\n",
       " 0.5,\n",
       " 'tmp/b_0.5539695728453294.py',\n",
       " 'tmp/b_0.3664986483537218.py',\n",
       " array([657., 662., 602., 556., 640., 645., 622., 562., 629., 625.]),\n",
       " array([673., 651., 600., 565., 594., 669., 661., 509., 607., 653.]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v7\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,True\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=False\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # v7 + eps greedy - bad\n",
    "# neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,True\", \"\")\n",
    "# neural_default = utils.bandits.neural.format(\"use_mean=False\", \"\")\n",
    "# utils.bandits.compare(\n",
    "#     utils.bandits.Agent(text=neural_with_nagiss),\n",
    "#     utils.bandits.Agent(text=neural_default),\n",
    "#     verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.bandits.compare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607.0 641.0 tmp/b_0.7818590056214438.py tmp/b_0.7584076114252468.py\n",
      "678.0 689.0 tmp/b_0.7818590056214438.py tmp/b_0.7584076114252468.py\n",
      "622.0 642.0 tmp/b_0.7818590056214438.py tmp/b_0.7584076114252468.py\n",
      "652.0 639.0 tmp/b_0.7818590056214438.py tmp/b_0.7584076114252468.py\n",
      "621.0 587.0 tmp/b_0.7818590056214438.py tmp/b_0.7584076114252468.py\n",
      "584.0 584.0 tmp/b_0.7818590056214438.py tmp/b_0.7584076114252468.py\n",
      "666.0 689.0 tmp/b_0.7818590056214438.py tmp/b_0.7584076114252468.py\n",
      "597.0 624.0 tmp/b_0.7818590056214438.py tmp/b_0.7584076114252468.py\n",
      "636.0 607.0 tmp/b_0.7818590056214438.py tmp/b_0.7584076114252468.py\n",
      "566.0 620.0 tmp/b_0.7818590056214438.py tmp/b_0.7584076114252468.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.2714717561979586,\n",
       " -9.3,\n",
       " 26.74341040331244,\n",
       " 0.4,\n",
       " 'tmp/b_0.7818590056214438.py',\n",
       " 'tmp/b_0.7584076114252468.py',\n",
       " array([607., 678., 622., 652., 621., 584., 666., 597., 636., 566.]),\n",
       " array([641., 689., 642., 639., 587., 584., 689., 624., 607., 620.]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v7\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_feature_nagiss,use_mean=True,True\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=False\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nagiss_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sessions/nagiss/20201226_v2/ ['sessions/nagiss/20201226_v2/7994638.json', 'sessions/nagiss/20201226_v2/7991093.json', 'sessions/nagiss/20201226_v2/7988820.json', 'sessions/nagiss/20201226_v2/8386716.json', 'sessions/nagiss/20201226_v2/8038792.json', 'sessions/nagiss/20201226_v2/8233786.json', 'sessions/nagiss/20201226_v2/8073493.json', 'sessions/nagiss/20201226_v2/8012231.json', 'sessions/nagiss/20201226_v2/7995268.json', 'sessions/nagiss/20201226_v2/8046220.json', 'sessions/nagiss/20201226_v2/8321309.json', 'sessions/nagiss/20201226_v2/8102725.json', 'sessions/nagiss/20201226_v2/8146617.json', 'sessions/nagiss/20201226_v2/7986639.json', 'sessions/nagiss/20201226_v2/8029737.json', 'sessions/nagiss/20201226_v2/8089900.json', 'sessions/nagiss/20201226_v2/8354985.json', 'sessions/nagiss/20201226_v2/7995644.json', 'sessions/nagiss/20201226_v2/7998867.json', 'sessions/nagiss/20201226_v2/8257767.json', 'sessions/nagiss/20201226_v2/7992085.json', 'sessions/nagiss/20201226_v2/7987842.json', 'sessions/nagiss/20201226_v2/8172893.json', 'sessions/nagiss/20201226_v2/8201003.json', 'sessions/nagiss/20201226_v2/8009354.json', 'sessions/nagiss/20201226_v2/7988519.json', 'sessions/nagiss/20201226_v2/7987183.json', 'sessions/nagiss/20201226_v2/7992694.json', 'sessions/nagiss/20201226_v2/7991811.json', 'sessions/nagiss/20201226_v2/8464420.json', 'sessions/nagiss/20201226_v2/8186631.json', 'sessions/nagiss/20201226_v2/8054877.json', 'sessions/nagiss/20201226_v2/8249308.json', 'sessions/nagiss/20201226_v2/8273034.json', 'sessions/nagiss/20201226_v2/7987567.json', 'sessions/nagiss/20201226_v2/8371128.json', 'sessions/nagiss/20201226_v2/8060412.json', 'sessions/nagiss/20201226_v2/8541194.json', 'sessions/nagiss/20201226_v2/8481316.json', 'sessions/nagiss/20201226_v2/7994974.json', 'sessions/nagiss/20201226_v2/8132606.json', 'sessions/nagiss/20201226_v2/7989777.json', 'sessions/nagiss/20201226_v2/7992382.json', 'sessions/nagiss/20201226_v2/8160880.json', 'sessions/nagiss/20201226_v2/8005498.json', 'sessions/nagiss/20201226_v2/8337440.json', 'sessions/nagiss/20201226_v2/8008069.json', 'sessions/nagiss/20201226_v2/8304463.json', 'sessions/nagiss/20201226_v2/7994017.json', 'sessions/nagiss/20201226_v2/7995939.json', 'sessions/nagiss/20201226_v2/8510303.json', 'sessions/nagiss/20201226_v2/8433263.json', 'sessions/nagiss/20201226_v2/7996231.json', 'sessions/nagiss/20201226_v2/7996548.json', 'sessions/nagiss/20201226_v2/8449014.json', 'sessions/nagiss/20201226_v2/7988184.json', 'sessions/nagiss/20201226_v2/8215761.json', 'sessions/nagiss/20201226_v2/7993014.json', 'sessions/nagiss/20201226_v2/7989458.json', 'sessions/nagiss/20201226_v2/7990100.json', 'sessions/nagiss/20201226_v2/8288389.json', 'sessions/nagiss/20201226_v2/7991414.json', 'sessions/nagiss/20201226_v2/7994342.json', 'sessions/nagiss/20201226_v2/8417594.json', 'sessions/nagiss/20201226_v2/7991731.json', 'sessions/nagiss/20201226_v2/8402256.json', 'sessions/nagiss/20201226_v2/8002300.json', 'sessions/nagiss/20201226_v2/8495892.json', 'sessions/nagiss/20201226_v2/8557211.json', 'sessions/nagiss/20201226_v2/8073192.json', 'sessions/nagiss/20201226_v2/8118585.json', 'sessions/nagiss/20201226_v2/8016051.json', 'sessions/nagiss/20201226_v2/7993371.json', 'sessions/nagiss/20201226_v2/7989156.json', 'sessions/nagiss/20201226_v2/7990750.json', 'sessions/nagiss/20201226_v2/8021586.json', 'sessions/nagiss/20201226_v2/7993701.json', 'sessions/nagiss/20201226_v2/7990443.json', 'sessions/nagiss/20201226_v2/8079696.json', 'sessions/nagiss/20201226_v2/7987514.json', 'sessions/nagiss/20201226_v2/8525948.json', 'sessions/nagiss/20201226_v2/.ipynb_checkpoints/8002300-checkpoint.json']\n",
      "error while parse session sessions/nagiss/20201226_v2/8354985.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/7991811.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/8054877.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/8005498.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/7995939.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/7991414.json : \n",
      "error while parse session sessions/nagiss/20201226_v2/8002300.json : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error while parse session sessions/nagiss/20201226_v2/.ipynb_checkpoints/8002300-checkpoint.json : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [02:47<00:00,  2.79s/it]\n",
      "100%|██████████| 14/14 [00:00<00:00, 1799.52it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets_nagiss_big = snowden.collect_dataset_from_dir(\n",
    "    neural_agent_class,\n",
    "    'sessions/nagiss/20201226_v2/',\n",
    "    val_ratio=2/10,\n",
    "    n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets_nagiss_big[0].save_dataset('datasets/all_actions_nagiss_v2_train_top_10')\n",
    "# datasets_nagiss_big[1].save_dataset('datasets/all_actions_nagiss_v2_val_top_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,val = datasets_nagiss_big[0],datasets_nagiss_big[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_shapes(train,val,H=72):\n",
    "    train.X = train.X.reshape(-1, 100, H)\n",
    "    val.X = val.X.reshape(-1, 100, H)\n",
    "\n",
    "    train.actions = train.actions.reshape(-1)\n",
    "    val.actions = val.actions.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNWithCusomFeatures2(nn.Module):\n",
    "    def __init__(self, INPUT_F, DROP_P, H):\n",
    "        super().__init__()\n",
    "        INPUT_F_C = INPUT_F + 2 * INPUT_F\n",
    "        self.model_ff =  nn.Sequential(\n",
    "            nn.BatchNorm1d(INPUT_F_C),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(INPUT_F_C, H),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(H, H),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, 1)\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[:, :36]   # v9\n",
    "        lg = torch.log(1 + torch.abs(x))\n",
    "        sn = torch.sin(x)\n",
    "        input_x = torch.cat([x, lg, sn], axis=1)\n",
    "        return self.model_ff(input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 50 batch: 4.14671\n",
      "val loss in 1 epoch: 4.21345\n",
      "train loss in 1 epoch in 100 batch: 3.45924\n",
      "val loss in 1 epoch: 3.56803\n",
      "train loss in 1 epoch in 150 batch: 3.17914\n",
      "val loss in 1 epoch: 3.18435\n",
      "train loss in 1 epoch in 200 batch: 2.83735\n",
      "val loss in 1 epoch: 3.09126\n",
      "train loss in 1 epoch in 250 batch: 2.93774\n",
      "val loss in 1 epoch: 3.05854\n",
      "train loss in 1 epoch in 300 batch: 3.13401\n",
      "val loss in 1 epoch: 3.03785\n",
      "train loss in 1 epoch in 350 batch: 2.78217\n",
      "val loss in 1 epoch: 3.02176\n",
      "train loss in 1 epoch in 400 batch: 2.97525\n",
      "val loss in 1 epoch: 3.00114\n",
      "train loss in 1 epoch in 450 batch: 3.14768\n",
      "val loss in 1 epoch: 2.99071\n",
      "train loss in 2 epoch in 50 batch: 2.68735\n",
      "val loss in 2 epoch: 2.95458\n",
      "train loss in 2 epoch in 100 batch: 2.76762\n",
      "val loss in 2 epoch: 2.94053\n",
      "train loss in 2 epoch in 150 batch: 3.29673\n",
      "val loss in 2 epoch: 2.92879\n",
      "train loss in 2 epoch in 200 batch: 2.46205\n",
      "val loss in 2 epoch: 2.92022\n",
      "train loss in 2 epoch in 250 batch: 2.95611\n",
      "val loss in 2 epoch: 2.90676\n",
      "train loss in 2 epoch in 300 batch: 2.87811\n",
      "val loss in 2 epoch: 2.90001\n",
      "train loss in 2 epoch in 350 batch: 2.62381\n",
      "val loss in 2 epoch: 2.89620\n",
      "train loss in 2 epoch in 400 batch: 2.59864\n",
      "val loss in 2 epoch: 2.89234\n",
      "train loss in 2 epoch in 450 batch: 2.55766\n",
      "val loss in 2 epoch: 2.87984\n",
      "train loss in 3 epoch in 50 batch: 2.66218\n",
      "val loss in 3 epoch: 2.87189\n",
      "train loss in 3 epoch in 100 batch: 2.55154\n",
      "val loss in 3 epoch: 2.87238\n",
      "train loss in 3 epoch in 150 batch: 2.83693\n",
      "val loss in 3 epoch: 2.87353\n",
      "train loss in 3 epoch in 200 batch: 2.76055\n",
      "val loss in 3 epoch: 2.87172\n",
      "train loss in 3 epoch in 250 batch: 2.77981\n",
      "val loss in 3 epoch: 2.86739\n",
      "train loss in 3 epoch in 300 batch: 2.61338\n",
      "val loss in 3 epoch: 2.86895\n",
      "train loss in 3 epoch in 350 batch: 2.31897\n",
      "val loss in 3 epoch: 2.85340\n",
      "train loss in 3 epoch in 400 batch: 2.87036\n",
      "val loss in 3 epoch: 2.85709\n",
      "train loss in 3 epoch in 450 batch: 2.86394\n",
      "val loss in 3 epoch: 2.85059\n",
      "train loss in 4 epoch in 50 batch: 2.84580\n",
      "val loss in 4 epoch: 2.84780\n",
      "train loss in 4 epoch in 100 batch: 2.74902\n",
      "val loss in 4 epoch: 2.84683\n",
      "train loss in 4 epoch in 150 batch: 2.93341\n",
      "val loss in 4 epoch: 2.84414\n",
      "train loss in 4 epoch in 200 batch: 2.68842\n",
      "val loss in 4 epoch: 2.84373\n",
      "train loss in 4 epoch in 250 batch: 2.19103\n",
      "val loss in 4 epoch: 2.84845\n",
      "train loss in 4 epoch in 300 batch: 2.59012\n",
      "val loss in 4 epoch: 2.84082\n",
      "train loss in 4 epoch in 350 batch: 2.58675\n",
      "val loss in 4 epoch: 2.84094\n",
      "train loss in 4 epoch in 400 batch: 2.59284\n",
      "val loss in 4 epoch: 2.84241\n",
      "train loss in 4 epoch in 450 batch: 3.15915\n",
      "val loss in 4 epoch: 2.84037\n"
     ]
    }
   ],
   "source": [
    "INPUT_F = 72\n",
    "H = 256\n",
    "DROP_P = 0.01\n",
    "\n",
    "model = NNWithCusomFeatures2(INPUT_F, DROP_P, H)\n",
    "\n",
    "def policy_gradient(logits_batch, rewards_batch, actions_batch):\n",
    "    loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    logits_batch = torch.reshape(logits_batch, (-1, 100))\n",
    "    policy = loss(logits_batch, actions_batch)\n",
    "    out = torch.sum(policy * rewards_batch) / torch.sum(rewards_batch)\n",
    "    return out\n",
    "\n",
    "learn(train, val, model, freq=50, batch_size=256,lr=1e-4, epochs=4, criterion=policy_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634.0 678.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "612.0 638.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "752.0 737.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "577.0 592.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "612.0 599.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "528.0 610.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "578.0 599.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "617.0 677.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "591.0 628.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "618.0 722.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0016427853489932535,\n",
       " -36.1,\n",
       " 36.261411996776964,\n",
       " 0.2,\n",
       " 'tmp/b_0.21800793341522473.py',\n",
       " 'tmp/b_0.36094695632683116.py',\n",
       " array([634., 612., 752., 577., 612., 528., 578., 617., 591., 618.]),\n",
       " array([678., 638., 737., 592., 599., 610., 599., 677., 628., 722.]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v8\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,True\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=False\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656.0 670.0 tmp/b_0.8789289661002517.py tmp/b_0.4196354505275198.py\n",
      "574.0 584.0 tmp/b_0.8789289661002517.py tmp/b_0.4196354505275198.py\n",
      "637.0 617.0 tmp/b_0.8789289661002517.py tmp/b_0.4196354505275198.py\n",
      "644.0 635.0 tmp/b_0.8789289661002517.py tmp/b_0.4196354505275198.py\n",
      "583.0 608.0 tmp/b_0.8789289661002517.py tmp/b_0.4196354505275198.py\n",
      "638.0 630.0 tmp/b_0.8789289661002517.py tmp/b_0.4196354505275198.py\n",
      "662.0 635.0 tmp/b_0.8789289661002517.py tmp/b_0.4196354505275198.py\n",
      "588.0 620.0 tmp/b_0.8789289661002517.py tmp/b_0.4196354505275198.py\n",
      "595.0 629.0 tmp/b_0.8789289661002517.py tmp/b_0.4196354505275198.py\n",
      "656.0 678.0 tmp/b_0.8789289661002517.py tmp/b_0.4196354505275198.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.26705232859577455,\n",
       " -7.3,\n",
       " 20.7992788336519,\n",
       " 0.4,\n",
       " 'tmp/b_0.8789289661002517.py',\n",
       " 'tmp/b_0.4196354505275198.py',\n",
       " array([656., 574., 637., 644., 583., 638., 662., 588., 595., 656.]),\n",
       " array([670., 584., 617., 635., 608., 630., 635., 620., 629., 678.]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v8\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,True\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=False\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 50 batch: 1.26886\n",
      "val loss in 1 epoch: 1.30705\n",
      "train loss in 1 epoch in 100 batch: 1.15985\n",
      "val loss in 1 epoch: 1.05718\n",
      "train loss in 1 epoch in 150 batch: 1.07963\n",
      "val loss in 1 epoch: 0.92873\n",
      "train loss in 1 epoch in 200 batch: 0.93143\n",
      "val loss in 1 epoch: 0.89737\n",
      "train loss in 1 epoch in 250 batch: 0.93955\n",
      "val loss in 1 epoch: 0.88664\n",
      "train loss in 1 epoch in 300 batch: 1.15136\n",
      "val loss in 1 epoch: 0.88071\n",
      "train loss in 1 epoch in 350 batch: 0.86700\n",
      "val loss in 1 epoch: 0.87526\n",
      "train loss in 1 epoch in 400 batch: 0.97938\n",
      "val loss in 1 epoch: 0.86932\n",
      "train loss in 1 epoch in 450 batch: 0.96049\n",
      "val loss in 1 epoch: 0.86571\n",
      "train loss in 2 epoch in 50 batch: 0.82117\n",
      "val loss in 2 epoch: 0.85503\n",
      "train loss in 2 epoch in 100 batch: 0.74852\n",
      "val loss in 2 epoch: 0.85012\n",
      "train loss in 2 epoch in 150 batch: 1.01651\n",
      "val loss in 2 epoch: 0.84566\n",
      "train loss in 2 epoch in 200 batch: 0.74224\n",
      "val loss in 2 epoch: 0.84266\n",
      "train loss in 2 epoch in 250 batch: 0.97031\n",
      "val loss in 2 epoch: 0.83919\n",
      "train loss in 2 epoch in 300 batch: 0.89095\n",
      "val loss in 2 epoch: 0.83666\n",
      "train loss in 2 epoch in 350 batch: 0.92638\n",
      "val loss in 2 epoch: 0.83462\n",
      "train loss in 2 epoch in 400 batch: 0.82397\n",
      "val loss in 2 epoch: 0.83376\n",
      "train loss in 2 epoch in 450 batch: 0.77903\n",
      "val loss in 2 epoch: 0.82922\n",
      "train loss in 3 epoch in 50 batch: 0.95812\n",
      "val loss in 3 epoch: 0.82665\n",
      "train loss in 3 epoch in 100 batch: 0.86922\n",
      "val loss in 3 epoch: 0.82679\n",
      "train loss in 3 epoch in 150 batch: 0.93104\n",
      "val loss in 3 epoch: 0.82728\n",
      "train loss in 3 epoch in 200 batch: 0.94973\n",
      "val loss in 3 epoch: 0.82643\n",
      "train loss in 3 epoch in 250 batch: 0.84904\n",
      "val loss in 3 epoch: 0.82497\n",
      "train loss in 3 epoch in 300 batch: 0.80590\n",
      "val loss in 3 epoch: 0.82556\n",
      "train loss in 3 epoch in 350 batch: 0.77612\n",
      "val loss in 3 epoch: 0.82027\n",
      "train loss in 3 epoch in 400 batch: 0.94198\n",
      "val loss in 3 epoch: 0.82186\n",
      "train loss in 3 epoch in 450 batch: 0.95297\n",
      "val loss in 3 epoch: 0.81933\n",
      "train loss in 4 epoch in 50 batch: 0.88915\n",
      "val loss in 4 epoch: 0.81874\n",
      "train loss in 4 epoch in 100 batch: 0.92457\n",
      "val loss in 4 epoch: 0.81793\n",
      "train loss in 4 epoch in 150 batch: 0.96206\n",
      "val loss in 4 epoch: 0.81713\n",
      "train loss in 4 epoch in 200 batch: 0.92435\n",
      "val loss in 4 epoch: 0.81761\n",
      "train loss in 4 epoch in 250 batch: 0.62370\n",
      "val loss in 4 epoch: 0.81892\n",
      "train loss in 4 epoch in 300 batch: 0.85895\n",
      "val loss in 4 epoch: 0.81582\n",
      "train loss in 4 epoch in 350 batch: 0.75729\n",
      "val loss in 4 epoch: 0.81566\n",
      "train loss in 4 epoch in 400 batch: 0.84167\n",
      "val loss in 4 epoch: 0.81591\n",
      "train loss in 4 epoch in 450 batch: 1.07330\n",
      "val loss in 4 epoch: 0.81538\n"
     ]
    }
   ],
   "source": [
    "INPUT_F = 72\n",
    "H = 256\n",
    "DROP_P = 0.01\n",
    "\n",
    "model = NNWithCusomFeatures2(INPUT_F, DROP_P, H)\n",
    "\n",
    "def policy_gradient(logits_batch, rewards_batch, actions_batch):\n",
    "    loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    logits_batch = torch.reshape(logits_batch, (-1, 100))\n",
    "    policy = loss(logits_batch, actions_batch)\n",
    "    out = torch.mean(policy * rewards_batch)\n",
    "    return out\n",
    "\n",
    "learn(train, val, model, freq=50, batch_size=256,lr=1e-4, epochs=4, criterion=policy_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "588.0 598.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "635.0 648.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "647.0 671.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "614.0 635.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "642.0 613.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "603.0 609.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "528.0 564.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "675.0 680.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "641.0 660.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "634.0 658.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.014015995659641567,\n",
       " -12.9,\n",
       " 16.603915200939806,\n",
       " 0.1,\n",
       " 'tmp/b_0.21800793341522473.py',\n",
       " 'tmp/b_0.36094695632683116.py',\n",
       " array([588., 635., 647., 614., 642., 603., 528., 675., 641., 634.]),\n",
       " array([598., 648., 671., 635., 613., 609., 564., 680., 660., 658.]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v9\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,True\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=False\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 200 batch: 2.85484\n",
      "val loss in 1 epoch: 3.56275\n",
      "train loss in 1 epoch in 400 batch: 3.92476\n",
      "val loss in 1 epoch: 3.28502\n",
      "train loss in 1 epoch in 600 batch: 3.79547\n",
      "val loss in 1 epoch: 3.25171\n",
      "train loss in 1 epoch in 800 batch: 3.71395\n",
      "val loss in 1 epoch: 3.23124\n",
      "train loss in 1 epoch in 1000 batch: 2.36579\n",
      "val loss in 1 epoch: 3.21198\n",
      "train loss in 1 epoch in 1200 batch: 2.46426\n",
      "val loss in 1 epoch: 3.19612\n",
      "train loss in 1 epoch in 1400 batch: 5.10417\n",
      "val loss in 1 epoch: 3.17131\n",
      "train loss in 1 epoch in 1600 batch: 1.91599\n",
      "val loss in 1 epoch: 3.17259\n",
      "train loss in 1 epoch in 1800 batch: 2.31170\n",
      "val loss in 1 epoch: 3.13965\n",
      "train loss in 1 epoch in 2000 batch: 2.79342\n",
      "val loss in 1 epoch: 3.12516\n",
      "train loss in 1 epoch in 2200 batch: 3.22354\n",
      "val loss in 1 epoch: 3.12646\n",
      "train loss in 1 epoch in 2400 batch: 3.18674\n",
      "val loss in 1 epoch: 3.09653\n",
      "train loss in 1 epoch in 2600 batch: 2.67877\n",
      "val loss in 1 epoch: 3.10167\n",
      "train loss in 1 epoch in 2800 batch: 2.62644\n",
      "val loss in 1 epoch: 3.07481\n",
      "train loss in 1 epoch in 3000 batch: 5.28459\n",
      "val loss in 1 epoch: 3.06820\n",
      "train loss in 1 epoch in 3200 batch: 3.12635\n",
      "val loss in 1 epoch: 3.05405\n",
      "train loss in 1 epoch in 3400 batch: 1.66062\n",
      "val loss in 1 epoch: 3.05678\n",
      "train loss in 1 epoch in 3600 batch: 3.73224\n",
      "val loss in 1 epoch: 3.06174\n",
      "train loss in 1 epoch in 3800 batch: 2.26645\n",
      "val loss in 1 epoch: 3.05120\n",
      "train loss in 1 epoch in 4000 batch: 3.37179\n",
      "val loss in 1 epoch: 3.04364\n",
      "train loss in 1 epoch in 4200 batch: 1.53326\n",
      "val loss in 1 epoch: 3.04373\n",
      "train loss in 1 epoch in 4400 batch: 4.06844\n",
      "val loss in 1 epoch: 3.04908\n",
      "train loss in 1 epoch in 4600 batch: 2.68555\n",
      "val loss in 1 epoch: 3.04139\n",
      "train loss in 1 epoch in 4800 batch: 3.67042\n",
      "val loss in 1 epoch: 3.04198\n",
      "train loss in 1 epoch in 5000 batch: 3.23617\n",
      "val loss in 1 epoch: 3.04024\n",
      "train loss in 1 epoch in 5200 batch: 3.39284\n",
      "val loss in 1 epoch: 3.03599\n",
      "train loss in 1 epoch in 5400 batch: 3.37571\n",
      "val loss in 1 epoch: 3.04163\n",
      "train loss in 1 epoch in 5600 batch: 2.06181\n",
      "val loss in 1 epoch: 3.03330\n",
      "train loss in 1 epoch in 5800 batch: 2.89329\n",
      "val loss in 1 epoch: 3.04458\n",
      "train loss in 1 epoch in 6000 batch: 2.83261\n",
      "val loss in 1 epoch: 3.04391\n",
      "train loss in 1 epoch in 6200 batch: 2.44182\n",
      "val loss in 1 epoch: 3.03107\n",
      "train loss in 1 epoch in 6400 batch: 2.57144\n",
      "val loss in 1 epoch: 3.03652\n",
      "train loss in 1 epoch in 6600 batch: 0.34968\n",
      "val loss in 1 epoch: 3.01502\n",
      "train loss in 1 epoch in 6800 batch: 4.10802\n",
      "val loss in 1 epoch: 3.02310\n",
      "train loss in 1 epoch in 7000 batch: 2.01138\n",
      "val loss in 1 epoch: 3.02649\n",
      "train loss in 1 epoch in 7200 batch: 2.99442\n",
      "val loss in 1 epoch: 3.05696\n",
      "train loss in 1 epoch in 7400 batch: 2.98008\n",
      "val loss in 1 epoch: 3.01897\n",
      "train loss in 2 epoch in 200 batch: 1.87588\n",
      "val loss in 2 epoch: 3.03651\n",
      "train loss in 2 epoch in 400 batch: 4.04170\n",
      "val loss in 2 epoch: 3.02053\n",
      "train loss in 2 epoch in 600 batch: 3.84156\n",
      "val loss in 2 epoch: 3.02249\n",
      "train loss in 2 epoch in 800 batch: 2.76450\n",
      "val loss in 2 epoch: 3.01638\n",
      "train loss in 2 epoch in 1000 batch: 2.62345\n",
      "val loss in 2 epoch: 3.01073\n",
      "train loss in 2 epoch in 1200 batch: 2.32807\n",
      "val loss in 2 epoch: 3.02025\n",
      "train loss in 2 epoch in 1400 batch: 2.79705\n",
      "val loss in 2 epoch: 3.01141\n",
      "train loss in 2 epoch in 1600 batch: 1.14580\n",
      "val loss in 2 epoch: 3.02280\n",
      "train loss in 2 epoch in 1800 batch: 2.89668\n",
      "val loss in 2 epoch: 3.02048\n",
      "train loss in 2 epoch in 2000 batch: 3.74851\n",
      "val loss in 2 epoch: 3.02306\n",
      "train loss in 2 epoch in 2200 batch: 5.18288\n",
      "val loss in 2 epoch: 3.01731\n",
      "train loss in 2 epoch in 2400 batch: 4.10693\n",
      "val loss in 2 epoch: 3.03715\n",
      "train loss in 2 epoch in 2600 batch: 4.03381\n",
      "val loss in 2 epoch: 3.00921\n",
      "train loss in 2 epoch in 2800 batch: 4.33240\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-6001b0743988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolicy_gradient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-16efb3c84b47>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(train, val, model_ff, epochs, batch_size, shuffle, freq, lr, criterion)\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0ma_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                         \u001b[0my_pred_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                     \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-43-beba3fddfe21>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0msn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0minput_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_ff\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1690\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1692\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# v11\n",
    "INPUT_F = 36\n",
    "H = 256\n",
    "DROP_P = 0.1\n",
    "\n",
    "model = NNWithCusomFeatures2(INPUT_F, DROP_P, H)\n",
    "\n",
    "def policy_gradient(logits_batch, rewards_batch, actions_batch):\n",
    "    loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    logits_batch = torch.reshape(logits_batch, (-1, 100))\n",
    "    policy = loss(logits_batch, actions_batch)\n",
    "    out = torch.mean(policy * rewards_batch)\n",
    "    return out\n",
    "\n",
    "learn(train, val, model, freq=200, batch_size=16,lr=1e-4, epochs=4, criterion=policy_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "629.0 627.0 tmp/b_0.5686394484909753.py tmp/b_0.24554733352265246.py\n",
      "626.0 585.0 tmp/b_0.5686394484909753.py tmp/b_0.24554733352265246.py\n",
      "673.0 614.0 tmp/b_0.5686394484909753.py tmp/b_0.24554733352265246.py\n",
      "673.0 674.0 tmp/b_0.5686394484909753.py tmp/b_0.24554733352265246.py\n",
      "648.0 610.0 tmp/b_0.5686394484909753.py tmp/b_0.24554733352265246.py\n",
      "678.0 651.0 tmp/b_0.5686394484909753.py tmp/b_0.24554733352265246.py\n",
      "668.0 655.0 tmp/b_0.5686394484909753.py tmp/b_0.24554733352265246.py\n",
      "629.0 637.0 tmp/b_0.5686394484909753.py tmp/b_0.24554733352265246.py\n",
      "622.0 590.0 tmp/b_0.5686394484909753.py tmp/b_0.24554733352265246.py\n",
      "635.0 621.0 tmp/b_0.5686394484909753.py tmp/b_0.24554733352265246.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0006848808459747456,\n",
       " 21.7,\n",
       " 20.209156340629363,\n",
       " 0.8,\n",
       " 'tmp/b_0.5686394484909753.py',\n",
       " 'tmp/b_0.24554733352265246.py',\n",
       " array([629., 626., 673., 673., 648., 678., 668., 629., 622., 635.]),\n",
       " array([627., 585., 614., 674., 610., 651., 655., 637., 590., 621.]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v11\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,False\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=False\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "635.0 669.0 tmp/b_0.9458669928299895.py tmp/b_0.7455212560064984.py\n",
      "672.0 674.0 tmp/b_0.9458669928299895.py tmp/b_0.7455212560064984.py\n",
      "698.0 630.0 tmp/b_0.9458669928299895.py tmp/b_0.7455212560064984.py\n",
      "638.0 571.0 tmp/b_0.9458669928299895.py tmp/b_0.7455212560064984.py\n",
      "585.0 607.0 tmp/b_0.9458669928299895.py tmp/b_0.7455212560064984.py\n",
      "697.0 645.0 tmp/b_0.9458669928299895.py tmp/b_0.7455212560064984.py\n",
      "628.0 640.0 tmp/b_0.9458669928299895.py tmp/b_0.7455212560064984.py\n",
      "612.0 559.0 tmp/b_0.9458669928299895.py tmp/b_0.7455212560064984.py\n",
      "616.0 644.0 tmp/b_0.9458669928299895.py tmp/b_0.7455212560064984.py\n",
      "654.0 577.0 tmp/b_0.9458669928299895.py tmp/b_0.7455212560064984.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.10576402451141403,\n",
       " 21.9,\n",
       " 42.81460031344448,\n",
       " 0.5,\n",
       " 'tmp/b_0.9458669928299895.py',\n",
       " 'tmp/b_0.7455212560064984.py',\n",
       " array([635., 672., 698., 638., 585., 697., 628., 612., 616., 654.]),\n",
       " array([669., 674., 630., 571., 607., 645., 640., 559., 644., 577.]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v11 vs neural with mean\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,False\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 57,  66,  97,  75,  10,  42,  34,  86, 101,  93,  22,   6,  46,\n",
       "        33,  81, 106,  11, 105,  72,  60, 103,  27,  73,  82,  69,  94,\n",
       "        32,  18,   1,  47,  89,  90,  70,  95,  61,  99,  74,  20,  63,\n",
       "        80,  53,  76,  38,  77,  13,  28,  71,  36,  59,  49,  24,  92,\n",
       "        41,  25,  54, 100,  31,  62,  64,  84,  16,  30,  35, 107,  23,\n",
       "        50,  78,  98,  12,  56,  65,  68,  26,  17,  44, 104,  96,  85,\n",
       "         0,   8,  37,  79,  14,  40, 102,  48,  21,   2,   9,  58,  29,\n",
       "         5,  83,  91,  43,  88,  19,   4,  52,  67,  15,  51,   3,  87,\n",
       "        55,  39,  45,   7])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#analyze v11\n",
    "model.load_state_dict(torch.load(\"models/nagiss_v11\"))\n",
    "model.eval()\n",
    "\n",
    "model.state_dict().keys()\n",
    "\n",
    "np.argsort(np.abs(np.mean(model.state_dict()['model_ff.2.weight'].numpy(),axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 200 batch: 0.49264\n",
      "val loss in 1 epoch: 0.97389\n",
      "train loss in 1 epoch in 400 batch: 0.25059\n",
      "val loss in 1 epoch: 0.90190\n",
      "train loss in 1 epoch in 600 batch: 1.16856\n",
      "val loss in 1 epoch: 0.88345\n",
      "train loss in 1 epoch in 800 batch: 1.58572\n",
      "val loss in 1 epoch: 0.87793\n",
      "train loss in 1 epoch in 1000 batch: 0.72290\n",
      "val loss in 1 epoch: 0.86430\n",
      "train loss in 1 epoch in 1200 batch: 1.10172\n",
      "val loss in 1 epoch: 0.86087\n",
      "train loss in 1 epoch in 1400 batch: 0.61443\n",
      "val loss in 1 epoch: 0.85290\n",
      "train loss in 1 epoch in 1600 batch: 0.70160\n",
      "val loss in 1 epoch: 0.85688\n",
      "train loss in 1 epoch in 1800 batch: 0.77548\n",
      "val loss in 1 epoch: 0.84472\n",
      "train loss in 1 epoch in 2000 batch: 0.36067\n",
      "val loss in 1 epoch: 0.84141\n",
      "train loss in 1 epoch in 2200 batch: 1.35711\n",
      "val loss in 1 epoch: 0.84295\n",
      "train loss in 1 epoch in 2400 batch: 0.79576\n",
      "val loss in 1 epoch: 0.83438\n",
      "train loss in 1 epoch in 2600 batch: 0.80853\n",
      "val loss in 1 epoch: 0.84309\n",
      "train loss in 1 epoch in 2800 batch: 0.64625\n",
      "val loss in 1 epoch: 0.83070\n",
      "train loss in 1 epoch in 3000 batch: 0.66420\n",
      "val loss in 1 epoch: 0.83023\n",
      "train loss in 1 epoch in 3200 batch: 1.54703\n",
      "val loss in 1 epoch: 0.82673\n",
      "train loss in 1 epoch in 3400 batch: 0.70176\n",
      "val loss in 1 epoch: 0.82793\n",
      "train loss in 1 epoch in 3600 batch: 1.17590\n",
      "val loss in 1 epoch: 0.83316\n",
      "train loss in 1 epoch in 3800 batch: 0.81880\n",
      "val loss in 1 epoch: 0.82679\n",
      "train loss in 1 epoch in 4000 batch: 1.53397\n",
      "val loss in 1 epoch: 0.82712\n",
      "train loss in 1 epoch in 4200 batch: 0.28540\n",
      "val loss in 1 epoch: 0.82469\n",
      "train loss in 1 epoch in 4400 batch: 1.53343\n",
      "val loss in 1 epoch: 0.82482\n",
      "train loss in 1 epoch in 4600 batch: 0.61848\n",
      "val loss in 1 epoch: 0.82492\n",
      "train loss in 1 epoch in 4800 batch: 1.76189\n",
      "val loss in 1 epoch: 0.82764\n",
      "train loss in 1 epoch in 5000 batch: 1.40926\n",
      "val loss in 1 epoch: 0.82254\n",
      "train loss in 1 epoch in 5200 batch: 1.46670\n",
      "val loss in 1 epoch: 0.82074\n",
      "train loss in 1 epoch in 5400 batch: 1.04882\n",
      "val loss in 1 epoch: 0.82470\n",
      "train loss in 1 epoch in 5600 batch: 0.24577\n",
      "val loss in 1 epoch: 0.82015\n",
      "train loss in 1 epoch in 5800 batch: 0.89345\n",
      "val loss in 1 epoch: 0.82383\n",
      "train loss in 1 epoch in 6000 batch: 0.52218\n",
      "val loss in 1 epoch: 0.82252\n",
      "train loss in 1 epoch in 6200 batch: 0.87198\n",
      "val loss in 1 epoch: 0.81928\n",
      "train loss in 1 epoch in 6400 batch: 0.62062\n",
      "val loss in 1 epoch: 0.82253\n",
      "train loss in 1 epoch in 6600 batch: 0.06994\n",
      "val loss in 1 epoch: 0.81432\n",
      "train loss in 1 epoch in 6800 batch: 1.29373\n",
      "val loss in 1 epoch: 0.81564\n",
      "train loss in 1 epoch in 7000 batch: 0.60238\n",
      "val loss in 1 epoch: 0.81806\n",
      "train loss in 1 epoch in 7200 batch: 1.07488\n",
      "val loss in 1 epoch: 0.83414\n",
      "train loss in 1 epoch in 7400 batch: 0.72981\n",
      "val loss in 1 epoch: 0.81536\n",
      "train loss in 2 epoch in 200 batch: 0.27829\n",
      "val loss in 2 epoch: 0.81971\n",
      "train loss in 2 epoch in 400 batch: 1.24227\n",
      "val loss in 2 epoch: 0.81601\n",
      "train loss in 2 epoch in 600 batch: 0.93866\n",
      "val loss in 2 epoch: 0.81729\n",
      "train loss in 2 epoch in 800 batch: 1.02744\n",
      "val loss in 2 epoch: 0.81119\n",
      "train loss in 2 epoch in 1000 batch: 0.84006\n",
      "val loss in 2 epoch: 0.81000\n",
      "train loss in 2 epoch in 1200 batch: 0.74935\n",
      "val loss in 2 epoch: 0.81461\n",
      "train loss in 2 epoch in 1400 batch: 0.50451\n",
      "val loss in 2 epoch: 0.81121\n",
      "train loss in 2 epoch in 1600 batch: 0.29665\n",
      "val loss in 2 epoch: 0.81453\n",
      "train loss in 2 epoch in 1800 batch: 0.51098\n",
      "val loss in 2 epoch: 0.81434\n",
      "train loss in 2 epoch in 2000 batch: 1.37880\n",
      "val loss in 2 epoch: 0.81548\n",
      "train loss in 2 epoch in 2200 batch: 0.33191\n",
      "val loss in 2 epoch: 0.81263\n",
      "train loss in 2 epoch in 2400 batch: 0.98062\n",
      "val loss in 2 epoch: 0.82570\n",
      "train loss in 2 epoch in 2600 batch: 0.79112\n",
      "val loss in 2 epoch: 0.80859\n",
      "train loss in 2 epoch in 2800 batch: 0.53864\n",
      "val loss in 2 epoch: 0.80928\n",
      "train loss in 2 epoch in 3000 batch: 0.95682\n",
      "val loss in 2 epoch: 0.81079\n",
      "train loss in 2 epoch in 3200 batch: 0.79365\n",
      "val loss in 2 epoch: 0.81163\n",
      "train loss in 2 epoch in 3400 batch: 0.51483\n",
      "val loss in 2 epoch: 0.81292\n",
      "train loss in 2 epoch in 3600 batch: 0.38270\n",
      "val loss in 2 epoch: 0.80828\n",
      "train loss in 2 epoch in 3800 batch: 1.33390\n",
      "val loss in 2 epoch: 0.80756\n",
      "train loss in 2 epoch in 4000 batch: 1.54215\n",
      "val loss in 2 epoch: 0.80649\n",
      "train loss in 2 epoch in 4200 batch: 1.00838\n",
      "val loss in 2 epoch: 0.80556\n",
      "train loss in 2 epoch in 4400 batch: 0.82915\n",
      "val loss in 2 epoch: 0.81363\n",
      "train loss in 2 epoch in 4600 batch: 0.92276\n",
      "val loss in 2 epoch: 0.80977\n",
      "train loss in 2 epoch in 4800 batch: 1.07884\n",
      "val loss in 2 epoch: 0.80427\n",
      "train loss in 2 epoch in 5000 batch: 1.27918\n",
      "val loss in 2 epoch: 0.80621\n",
      "train loss in 2 epoch in 5200 batch: 0.38346\n",
      "val loss in 2 epoch: 0.80650\n",
      "train loss in 2 epoch in 5400 batch: 1.75918\n",
      "val loss in 2 epoch: 0.80452\n",
      "train loss in 2 epoch in 5600 batch: 0.95532\n",
      "val loss in 2 epoch: 0.80252\n",
      "train loss in 2 epoch in 5800 batch: 0.17131\n",
      "val loss in 2 epoch: 0.80654\n",
      "train loss in 2 epoch in 6000 batch: 0.83913\n",
      "val loss in 2 epoch: 0.80139\n",
      "train loss in 2 epoch in 6200 batch: 0.22845\n",
      "val loss in 2 epoch: 0.81175\n",
      "train loss in 2 epoch in 6400 batch: 1.08435\n",
      "val loss in 2 epoch: 0.80479\n",
      "train loss in 2 epoch in 6600 batch: 0.99044\n",
      "val loss in 2 epoch: 0.80750\n",
      "train loss in 2 epoch in 6800 batch: 0.43457\n",
      "val loss in 2 epoch: 0.80280\n",
      "train loss in 2 epoch in 7000 batch: 0.33949\n",
      "val loss in 2 epoch: 0.80065\n",
      "train loss in 2 epoch in 7200 batch: 0.35732\n",
      "val loss in 2 epoch: 0.80064\n",
      "train loss in 2 epoch in 7400 batch: 1.26350\n",
      "val loss in 2 epoch: 0.80024\n",
      "train loss in 3 epoch in 200 batch: 1.32180\n",
      "val loss in 3 epoch: 0.79804\n",
      "train loss in 3 epoch in 400 batch: 1.70057\n",
      "val loss in 3 epoch: 0.79866\n",
      "train loss in 3 epoch in 600 batch: 0.80382\n",
      "val loss in 3 epoch: 0.79666\n",
      "train loss in 3 epoch in 800 batch: 0.21740\n",
      "val loss in 3 epoch: 0.79887\n",
      "train loss in 3 epoch in 1000 batch: 0.40488\n",
      "val loss in 3 epoch: 0.79827\n",
      "train loss in 3 epoch in 1200 batch: 1.21064\n",
      "val loss in 3 epoch: 0.80145\n",
      "train loss in 3 epoch in 1400 batch: 1.06767\n",
      "val loss in 3 epoch: 0.80274\n",
      "train loss in 3 epoch in 1600 batch: 0.48904\n",
      "val loss in 3 epoch: 0.80671\n",
      "train loss in 3 epoch in 1800 batch: 0.60611\n",
      "val loss in 3 epoch: 0.81874\n",
      "train loss in 3 epoch in 2000 batch: 0.69335\n",
      "val loss in 3 epoch: 0.79727\n",
      "train loss in 3 epoch in 2200 batch: 0.55185\n",
      "val loss in 3 epoch: 0.79965\n",
      "train loss in 3 epoch in 2400 batch: 0.94909\n",
      "val loss in 3 epoch: 0.79531\n",
      "train loss in 3 epoch in 2600 batch: 1.48620\n",
      "val loss in 3 epoch: 0.79697\n",
      "train loss in 3 epoch in 2800 batch: 1.00277\n",
      "val loss in 3 epoch: 0.80081\n",
      "train loss in 3 epoch in 3000 batch: 0.87694\n",
      "val loss in 3 epoch: 0.79499\n",
      "train loss in 3 epoch in 3200 batch: 1.13953\n",
      "val loss in 3 epoch: 0.80062\n",
      "train loss in 3 epoch in 3400 batch: 0.71710\n",
      "val loss in 3 epoch: 0.79479\n",
      "train loss in 3 epoch in 3600 batch: 0.51458\n",
      "val loss in 3 epoch: 0.79946\n",
      "train loss in 3 epoch in 3800 batch: 1.72633\n",
      "val loss in 3 epoch: 0.79344\n",
      "train loss in 3 epoch in 4000 batch: 0.86761\n",
      "val loss in 3 epoch: 0.79959\n",
      "train loss in 3 epoch in 4200 batch: 0.55135\n",
      "val loss in 3 epoch: 0.79674\n",
      "train loss in 3 epoch in 4400 batch: 0.70482\n",
      "val loss in 3 epoch: 0.79455\n",
      "train loss in 3 epoch in 4600 batch: 1.07401\n",
      "val loss in 3 epoch: 0.79586\n",
      "train loss in 3 epoch in 4800 batch: 0.62858\n",
      "val loss in 3 epoch: 0.79560\n",
      "train loss in 3 epoch in 5000 batch: 0.64652\n",
      "val loss in 3 epoch: 0.79934\n",
      "train loss in 3 epoch in 5200 batch: 0.57832\n",
      "val loss in 3 epoch: 0.79306\n",
      "train loss in 3 epoch in 5400 batch: 0.87559\n",
      "val loss in 3 epoch: 0.79343\n",
      "train loss in 3 epoch in 5600 batch: 0.75590\n",
      "val loss in 3 epoch: 0.79321\n",
      "train loss in 3 epoch in 5800 batch: 0.17910\n",
      "val loss in 3 epoch: 0.79300\n",
      "train loss in 3 epoch in 6000 batch: 2.16604\n",
      "val loss in 3 epoch: 0.79545\n",
      "train loss in 3 epoch in 6200 batch: 1.10283\n",
      "val loss in 3 epoch: 0.78855\n",
      "train loss in 3 epoch in 6400 batch: 0.41574\n",
      "val loss in 3 epoch: 0.79607\n",
      "train loss in 3 epoch in 6600 batch: 1.03588\n",
      "val loss in 3 epoch: 0.79987\n",
      "train loss in 3 epoch in 6800 batch: 0.53203\n",
      "val loss in 3 epoch: 0.79594\n",
      "train loss in 3 epoch in 7000 batch: 1.92294\n",
      "val loss in 3 epoch: 0.79120\n",
      "train loss in 3 epoch in 7200 batch: 0.82804\n",
      "val loss in 3 epoch: 0.78978\n",
      "train loss in 3 epoch in 7400 batch: 0.03768\n",
      "val loss in 3 epoch: 0.79254\n",
      "train loss in 4 epoch in 200 batch: 0.46615\n",
      "val loss in 4 epoch: 0.79735\n",
      "train loss in 4 epoch in 400 batch: 0.13962\n",
      "val loss in 4 epoch: 0.79918\n",
      "train loss in 4 epoch in 600 batch: 1.46162\n",
      "val loss in 4 epoch: 0.79136\n",
      "train loss in 4 epoch in 800 batch: 0.65626\n",
      "val loss in 4 epoch: 0.79058\n",
      "train loss in 4 epoch in 1000 batch: 1.14361\n",
      "val loss in 4 epoch: 0.79640\n",
      "train loss in 4 epoch in 1200 batch: 0.62495\n",
      "val loss in 4 epoch: 0.79189\n",
      "train loss in 4 epoch in 1400 batch: 1.27826\n",
      "val loss in 4 epoch: 0.79560\n",
      "train loss in 4 epoch in 1600 batch: 1.25065\n",
      "val loss in 4 epoch: 0.79451\n",
      "train loss in 4 epoch in 1800 batch: 0.46729\n",
      "val loss in 4 epoch: 0.79133\n",
      "train loss in 4 epoch in 2000 batch: 0.66808\n",
      "val loss in 4 epoch: 0.79095\n",
      "train loss in 4 epoch in 2200 batch: 1.09060\n",
      "val loss in 4 epoch: 0.79101\n",
      "train loss in 4 epoch in 2400 batch: 0.81492\n",
      "val loss in 4 epoch: 0.79046\n",
      "train loss in 4 epoch in 2600 batch: 1.20912\n",
      "val loss in 4 epoch: 0.79265\n",
      "train loss in 4 epoch in 2800 batch: 1.06287\n",
      "val loss in 4 epoch: 0.78868\n",
      "train loss in 4 epoch in 3000 batch: 0.38516\n",
      "val loss in 4 epoch: 0.79207\n",
      "train loss in 4 epoch in 3200 batch: 0.71883\n",
      "val loss in 4 epoch: 0.78988\n",
      "train loss in 4 epoch in 3400 batch: 0.29825\n",
      "val loss in 4 epoch: 0.79384\n",
      "train loss in 4 epoch in 3600 batch: 1.23611\n",
      "val loss in 4 epoch: 0.79111\n",
      "train loss in 4 epoch in 3800 batch: 1.02907\n",
      "val loss in 4 epoch: 0.79224\n",
      "train loss in 4 epoch in 4000 batch: 0.00792\n",
      "val loss in 4 epoch: 0.79022\n",
      "train loss in 4 epoch in 4200 batch: 1.76412\n",
      "val loss in 4 epoch: 0.78739\n",
      "train loss in 4 epoch in 4400 batch: 1.19173\n",
      "val loss in 4 epoch: 0.78974\n",
      "train loss in 4 epoch in 4600 batch: 0.95773\n",
      "val loss in 4 epoch: 0.79485\n",
      "train loss in 4 epoch in 4800 batch: 1.07364\n",
      "val loss in 4 epoch: 0.78961\n",
      "train loss in 4 epoch in 5000 batch: 0.32650\n",
      "val loss in 4 epoch: 0.78834\n",
      "train loss in 4 epoch in 5200 batch: 0.15651\n",
      "val loss in 4 epoch: 0.78855\n",
      "train loss in 4 epoch in 5400 batch: 0.83503\n",
      "val loss in 4 epoch: 0.78996\n",
      "train loss in 4 epoch in 5600 batch: 0.22068\n",
      "val loss in 4 epoch: 0.79105\n",
      "train loss in 4 epoch in 5800 batch: 0.56530\n",
      "val loss in 4 epoch: 0.78998\n",
      "train loss in 4 epoch in 6000 batch: 0.61289\n",
      "val loss in 4 epoch: 0.79781\n",
      "train loss in 4 epoch in 6200 batch: 0.60580\n",
      "val loss in 4 epoch: 0.79778\n",
      "train loss in 4 epoch in 6400 batch: 1.19592\n",
      "val loss in 4 epoch: 0.79754\n",
      "train loss in 4 epoch in 6600 batch: 1.10746\n",
      "val loss in 4 epoch: 0.79013\n",
      "train loss in 4 epoch in 6800 batch: 0.37310\n",
      "val loss in 4 epoch: 0.79415\n",
      "train loss in 4 epoch in 7000 batch: 0.47249\n",
      "val loss in 4 epoch: 0.78843\n",
      "train loss in 4 epoch in 7200 batch: 1.53837\n",
      "val loss in 4 epoch: 0.79125\n",
      "train loss in 4 epoch in 7400 batch: 0.90454\n",
      "val loss in 4 epoch: 0.78334\n"
     ]
    }
   ],
   "source": [
    "# v12\n",
    "INPUT_F = 72\n",
    "H = 256\n",
    "DROP_P = 0.1\n",
    "\n",
    "model = NNWithCusomFeatures2(INPUT_F, DROP_P, H)\n",
    "\n",
    "def policy_gradient(logits_batch, rewards_batch, actions_batch):\n",
    "    loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    logits_batch = torch.reshape(logits_batch, (-1, 100))\n",
    "    policy = loss(logits_batch, actions_batch)\n",
    "    out = torch.mean(policy * rewards_batch)\n",
    "    return out\n",
    "\n",
    "learn(train, val, model, freq=200, batch_size=16,lr=1e-4, epochs=4, criterion=policy_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "687.0 667.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "714.0 712.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "651.0 634.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "621.0 603.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "681.0 637.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "637.0 614.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "611.0 613.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "629.0 577.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "629.0 606.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "647.0 624.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7.3509443816825105e-06,\n",
       " 22.0,\n",
       " 15.517731793016658,\n",
       " 0.9,\n",
       " 'tmp/b_0.21800793341522473.py',\n",
       " 'tmp/b_0.36094695632683116.py',\n",
       " array([687., 714., 651., 621., 681., 637., 611., 629., 629., 647.]),\n",
       " array([667., 712., 634., 603., 637., 614., 613., 577., 606., 624.]))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v12\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,True\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=False\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "631.0 658.0 tmp/b_0.8036731827992181.py tmp/b_0.6096653281782922.py\n",
      "651.0 616.0 tmp/b_0.8036731827992181.py tmp/b_0.6096653281782922.py\n",
      "613.0 623.0 tmp/b_0.8036731827992181.py tmp/b_0.6096653281782922.py\n",
      "701.0 644.0 tmp/b_0.8036731827992181.py tmp/b_0.6096653281782922.py\n",
      "682.0 652.0 tmp/b_0.8036731827992181.py tmp/b_0.6096653281782922.py\n",
      "643.0 670.0 tmp/b_0.8036731827992181.py tmp/b_0.6096653281782922.py\n",
      "656.0 574.0 tmp/b_0.8036731827992181.py tmp/b_0.6096653281782922.py\n",
      "670.0 630.0 tmp/b_0.8036731827992181.py tmp/b_0.6096653281782922.py\n",
      "626.0 666.0 tmp/b_0.8036731827992181.py tmp/b_0.6096653281782922.py\n",
      "700.0 678.0 tmp/b_0.8036731827992181.py tmp/b_0.6096653281782922.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.18173072385616584,\n",
       " 16.2,\n",
       " 38.360917611548345,\n",
       " 0.6,\n",
       " 'tmp/b_0.8036731827992181.py',\n",
       " 'tmp/b_0.6096653281782922.py',\n",
       " array([631., 651., 613., 701., 682., 643., 656., 670., 626., 700.]),\n",
       " array([658., 616., 623., 644., 652., 670., 574., 630., 666., 678.]))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v12 vs neural with mean\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,True\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.,  1.,  2.,  3.],\n",
       "         [ 4.,  5.,  6.,  7.],\n",
       "         [ 8.,  9., 10., 11.]]),\n",
       " tensor([0., 5., 9.]))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(np.arange(12)).reshape(3,4), torch.Tensor(np.arange(12)).reshape(3,4)[[0,1,2],[0,1,1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 200 batch: -1.62668\n",
      "val loss in 1 epoch: -0.91307\n",
      "train loss in 1 epoch in 400 batch: -6.53250\n",
      "val loss in 1 epoch: -2.57788\n",
      "train loss in 1 epoch in 600 batch: -5.15921\n",
      "val loss in 1 epoch: -4.05404\n",
      "train loss in 1 epoch in 800 batch: -4.44009\n",
      "val loss in 1 epoch: -5.12954\n",
      "train loss in 1 epoch in 1000 batch: -7.12367\n",
      "val loss in 1 epoch: -6.16595\n",
      "train loss in 1 epoch in 1200 batch: -4.96797\n",
      "val loss in 1 epoch: -7.14651\n",
      "train loss in 1 epoch in 1400 batch: -13.49207\n",
      "val loss in 1 epoch: -8.11245\n",
      "train loss in 1 epoch in 1600 batch: -7.55415\n",
      "val loss in 1 epoch: -9.07644\n",
      "train loss in 1 epoch in 1800 batch: -10.27917\n",
      "val loss in 1 epoch: -9.91737\n",
      "train loss in 1 epoch in 2000 batch: -22.79364\n",
      "val loss in 1 epoch: -10.84522\n",
      "train loss in 1 epoch in 2200 batch: -3.48836\n",
      "val loss in 1 epoch: -11.78287\n",
      "train loss in 1 epoch in 2400 batch: -14.74745\n",
      "val loss in 1 epoch: -12.66552\n",
      "train loss in 1 epoch in 2600 batch: -7.11691\n",
      "val loss in 1 epoch: -13.51396\n",
      "train loss in 1 epoch in 2800 batch: -16.29850\n",
      "val loss in 1 epoch: -14.41182\n",
      "train loss in 1 epoch in 3000 batch: -31.68697\n",
      "val loss in 1 epoch: -15.28776\n",
      "train loss in 1 epoch in 3200 batch: -7.67572\n",
      "val loss in 1 epoch: -16.16250\n",
      "train loss in 1 epoch in 3400 batch: -11.42318\n",
      "val loss in 1 epoch: -17.00066\n",
      "train loss in 1 epoch in 3600 batch: -12.68818\n",
      "val loss in 1 epoch: -17.81806\n",
      "train loss in 1 epoch in 3800 batch: -10.31767\n",
      "val loss in 1 epoch: -18.68371\n",
      "train loss in 1 epoch in 4000 batch: -10.76764\n",
      "val loss in 1 epoch: -19.54197\n",
      "train loss in 1 epoch in 4200 batch: -33.65575\n",
      "val loss in 1 epoch: -20.37877\n",
      "train loss in 1 epoch in 4400 batch: -14.84341\n",
      "val loss in 1 epoch: -21.19129\n",
      "train loss in 1 epoch in 4600 batch: -26.70997\n",
      "val loss in 1 epoch: -21.95176\n",
      "train loss in 1 epoch in 4800 batch: -3.74116\n",
      "val loss in 1 epoch: -22.80237\n",
      "train loss in 1 epoch in 5000 batch: -10.30423\n",
      "val loss in 1 epoch: -23.74339\n",
      "train loss in 1 epoch in 5200 batch: -12.29099\n",
      "val loss in 1 epoch: -24.60523\n",
      "train loss in 1 epoch in 5400 batch: -20.47081\n",
      "val loss in 1 epoch: -25.37999\n",
      "train loss in 1 epoch in 5600 batch: -41.62640\n",
      "val loss in 1 epoch: -26.33972\n",
      "train loss in 1 epoch in 5800 batch: -32.81760\n",
      "val loss in 1 epoch: -27.19401\n",
      "train loss in 1 epoch in 6000 batch: -49.35514\n",
      "val loss in 1 epoch: -27.93053\n",
      "train loss in 1 epoch in 6200 batch: -26.33596\n",
      "val loss in 1 epoch: -28.88539\n",
      "train loss in 1 epoch in 6400 batch: -21.73076\n",
      "val loss in 1 epoch: -29.74869\n",
      "train loss in 1 epoch in 6600 batch: -57.24688\n",
      "val loss in 1 epoch: -30.43298\n",
      "train loss in 1 epoch in 6800 batch: -52.92451\n",
      "val loss in 1 epoch: -31.35033\n",
      "train loss in 1 epoch in 7000 batch: -32.95591\n",
      "val loss in 1 epoch: -32.28513\n",
      "train loss in 1 epoch in 7200 batch: -44.43321\n",
      "val loss in 1 epoch: -33.13984\n",
      "train loss in 1 epoch in 7400 batch: -35.13468\n",
      "val loss in 1 epoch: -33.98473\n",
      "train loss in 2 epoch in 200 batch: -48.92607\n",
      "val loss in 2 epoch: -35.15710\n",
      "train loss in 2 epoch in 400 batch: -36.14332\n",
      "val loss in 2 epoch: -36.00486\n",
      "train loss in 2 epoch in 600 batch: -56.40798\n",
      "val loss in 2 epoch: -36.91511\n",
      "train loss in 2 epoch in 800 batch: -42.57056\n",
      "val loss in 2 epoch: -37.70372\n",
      "train loss in 2 epoch in 1000 batch: -48.08264\n",
      "val loss in 2 epoch: -38.52735\n",
      "train loss in 2 epoch in 1200 batch: -52.84301\n",
      "val loss in 2 epoch: -39.32609\n",
      "train loss in 2 epoch in 1400 batch: -58.22562\n",
      "val loss in 2 epoch: -40.20822\n",
      "train loss in 2 epoch in 1600 batch: -55.97213\n",
      "val loss in 2 epoch: -41.05030\n",
      "train loss in 2 epoch in 1800 batch: -57.68771\n",
      "val loss in 2 epoch: -41.86082\n",
      "train loss in 2 epoch in 2000 batch: -28.93356\n",
      "val loss in 2 epoch: -42.66961\n",
      "train loss in 2 epoch in 2200 batch: -90.48587\n",
      "val loss in 2 epoch: -43.32503\n",
      "train loss in 2 epoch in 2400 batch: -56.98162\n",
      "val loss in 2 epoch: -44.23492\n",
      "train loss in 2 epoch in 2600 batch: -75.75990\n",
      "val loss in 2 epoch: -45.15954\n",
      "train loss in 2 epoch in 2800 batch: -78.27119\n",
      "val loss in 2 epoch: -45.97718\n",
      "train loss in 2 epoch in 3000 batch: -24.22292\n",
      "val loss in 2 epoch: -46.82413\n",
      "train loss in 2 epoch in 3200 batch: -33.46371\n",
      "val loss in 2 epoch: -47.68477\n",
      "train loss in 2 epoch in 3400 batch: -32.89407\n",
      "val loss in 2 epoch: -48.48010\n",
      "train loss in 2 epoch in 3600 batch: -69.93153\n",
      "val loss in 2 epoch: -49.14201\n",
      "train loss in 2 epoch in 3800 batch: -59.86174\n",
      "val loss in 2 epoch: -50.14062\n",
      "train loss in 2 epoch in 4000 batch: -17.41350\n",
      "val loss in 2 epoch: -50.73187\n",
      "train loss in 2 epoch in 4200 batch: -43.44345\n",
      "val loss in 2 epoch: -51.76050\n",
      "train loss in 2 epoch in 4400 batch: -44.76748\n",
      "val loss in 2 epoch: -52.59081\n",
      "train loss in 2 epoch in 4600 batch: -36.19372\n",
      "val loss in 2 epoch: -53.43819\n",
      "train loss in 2 epoch in 4800 batch: -45.88231\n",
      "val loss in 2 epoch: -54.25477\n",
      "train loss in 2 epoch in 5000 batch: -56.16136\n",
      "val loss in 2 epoch: -55.02443\n",
      "train loss in 2 epoch in 5200 batch: -112.78261\n",
      "val loss in 2 epoch: -55.89358\n",
      "train loss in 2 epoch in 5400 batch: -38.40346\n",
      "val loss in 2 epoch: -56.67838\n",
      "train loss in 2 epoch in 5600 batch: -29.45930\n",
      "val loss in 2 epoch: -57.40404\n",
      "train loss in 2 epoch in 5800 batch: -89.28367\n",
      "val loss in 2 epoch: -58.36880\n",
      "train loss in 2 epoch in 6000 batch: -91.00150\n",
      "val loss in 2 epoch: -59.21341\n",
      "train loss in 2 epoch in 6200 batch: -91.63668\n",
      "val loss in 2 epoch: -60.08942\n",
      "train loss in 2 epoch in 6400 batch: -46.08531\n",
      "val loss in 2 epoch: -60.88958\n",
      "train loss in 2 epoch in 6600 batch: -83.77719\n",
      "val loss in 2 epoch: -61.49802\n",
      "train loss in 2 epoch in 6800 batch: -95.60591\n",
      "val loss in 2 epoch: -62.44282\n",
      "train loss in 2 epoch in 7000 batch: -107.72905\n",
      "val loss in 2 epoch: -63.36083\n",
      "train loss in 2 epoch in 7200 batch: -87.31415\n",
      "val loss in 2 epoch: -63.92459\n",
      "train loss in 2 epoch in 7400 batch: 22.23984\n",
      "val loss in 2 epoch: -64.69996\n",
      "train loss in 3 epoch in 200 batch: -27.43362\n",
      "val loss in 3 epoch: -66.40648\n",
      "train loss in 3 epoch in 400 batch: -43.86753\n",
      "val loss in 3 epoch: -67.32829\n",
      "train loss in 3 epoch in 600 batch: -76.27380\n",
      "val loss in 3 epoch: -67.94343\n",
      "train loss in 3 epoch in 800 batch: -128.53278\n",
      "val loss in 3 epoch: -68.88927\n",
      "train loss in 3 epoch in 1000 batch: -59.44627\n",
      "val loss in 3 epoch: -69.67209\n",
      "train loss in 3 epoch in 1200 batch: -86.72391\n",
      "val loss in 3 epoch: -70.47476\n",
      "train loss in 3 epoch in 1400 batch: -15.49856\n",
      "val loss in 3 epoch: -71.04775\n",
      "train loss in 3 epoch in 1600 batch: -108.79015\n",
      "val loss in 3 epoch: -72.11911\n",
      "train loss in 3 epoch in 1800 batch: -89.24868\n",
      "val loss in 3 epoch: -72.99306\n",
      "train loss in 3 epoch in 2000 batch: -62.39189\n",
      "val loss in 3 epoch: -73.78649\n",
      "train loss in 3 epoch in 2200 batch: -125.53136\n",
      "val loss in 3 epoch: -74.62959\n",
      "train loss in 3 epoch in 2400 batch: -113.00246\n",
      "val loss in 3 epoch: -75.39542\n",
      "train loss in 3 epoch in 2600 batch: -90.03540\n",
      "val loss in 3 epoch: -76.22681\n",
      "train loss in 3 epoch in 2800 batch: -78.95911\n",
      "val loss in 3 epoch: -76.85830\n",
      "train loss in 3 epoch in 3000 batch: -118.85086\n",
      "val loss in 3 epoch: -77.87996\n",
      "train loss in 3 epoch in 3200 batch: -87.94016\n",
      "val loss in 3 epoch: -78.58400\n",
      "train loss in 3 epoch in 3400 batch: -121.31821\n",
      "val loss in 3 epoch: -79.58887\n",
      "train loss in 3 epoch in 3600 batch: -109.46731\n",
      "val loss in 3 epoch: -80.16911\n",
      "train loss in 3 epoch in 3800 batch: -0.11796\n",
      "val loss in 3 epoch: -80.99784\n",
      "train loss in 3 epoch in 4000 batch: -92.50861\n",
      "val loss in 3 epoch: -81.89231\n",
      "train loss in 3 epoch in 4200 batch: -70.24555\n",
      "val loss in 3 epoch: -82.53947\n",
      "train loss in 3 epoch in 4400 batch: -106.67622\n",
      "val loss in 3 epoch: -83.59621\n",
      "train loss in 3 epoch in 4600 batch: -96.85333\n",
      "val loss in 3 epoch: -84.54183\n",
      "train loss in 3 epoch in 4800 batch: -100.29291\n",
      "val loss in 3 epoch: -85.03918\n",
      "train loss in 3 epoch in 5000 batch: -58.73314\n",
      "val loss in 3 epoch: -86.15523\n",
      "train loss in 3 epoch in 5200 batch: -146.02985\n",
      "val loss in 3 epoch: -86.94093\n",
      "train loss in 3 epoch in 5400 batch: -89.50716\n",
      "val loss in 3 epoch: -87.82146\n",
      "train loss in 3 epoch in 5600 batch: -31.36629\n",
      "val loss in 3 epoch: -88.76143\n",
      "train loss in 3 epoch in 5800 batch: -160.06059\n",
      "val loss in 3 epoch: -89.64958\n",
      "train loss in 3 epoch in 6000 batch: -34.01963\n",
      "val loss in 3 epoch: -90.55453\n",
      "train loss in 3 epoch in 6200 batch: -63.42426\n",
      "val loss in 3 epoch: -91.09252\n",
      "train loss in 3 epoch in 6400 batch: -69.35089\n",
      "val loss in 3 epoch: -92.13212\n",
      "train loss in 3 epoch in 6600 batch: -48.78053\n",
      "val loss in 3 epoch: -92.63034\n",
      "train loss in 3 epoch in 6800 batch: -102.15420\n",
      "val loss in 3 epoch: -93.88626\n",
      "train loss in 3 epoch in 7000 batch: -33.73584\n",
      "val loss in 3 epoch: -94.77945\n",
      "train loss in 3 epoch in 7200 batch: -32.44237\n",
      "val loss in 3 epoch: -95.56495\n",
      "train loss in 3 epoch in 7400 batch: -196.30124\n",
      "val loss in 3 epoch: -96.44894\n",
      "train loss in 4 epoch in 200 batch: -147.89250\n",
      "val loss in 4 epoch: -97.58594\n",
      "train loss in 4 epoch in 400 batch: -166.54272\n",
      "val loss in 4 epoch: -98.50156\n",
      "train loss in 4 epoch in 600 batch: -28.93037\n",
      "val loss in 4 epoch: -99.24405\n",
      "train loss in 4 epoch in 800 batch: -138.40347\n",
      "val loss in 4 epoch: -99.98544\n",
      "train loss in 4 epoch in 1000 batch: -41.10895\n",
      "val loss in 4 epoch: -100.60589\n",
      "train loss in 4 epoch in 1200 batch: -35.34073\n",
      "val loss in 4 epoch: -101.66281\n",
      "train loss in 4 epoch in 1400 batch: -89.35466\n",
      "val loss in 4 epoch: -102.48312\n",
      "train loss in 4 epoch in 1600 batch: -57.41962\n",
      "val loss in 4 epoch: -103.05316\n",
      "train loss in 4 epoch in 1800 batch: -87.10833\n",
      "val loss in 4 epoch: -103.98984\n",
      "train loss in 4 epoch in 2000 batch: -69.58330\n",
      "val loss in 4 epoch: -104.93395\n",
      "train loss in 4 epoch in 2200 batch: -147.82146\n",
      "val loss in 4 epoch: -105.81278\n",
      "train loss in 4 epoch in 2400 batch: -144.26984\n",
      "val loss in 4 epoch: -106.35442\n",
      "train loss in 4 epoch in 2600 batch: -72.41127\n",
      "val loss in 4 epoch: -107.22501\n",
      "train loss in 4 epoch in 2800 batch: -20.70479\n",
      "val loss in 4 epoch: -108.32578\n",
      "train loss in 4 epoch in 3000 batch: -203.79228\n",
      "val loss in 4 epoch: -109.19625\n",
      "train loss in 4 epoch in 3200 batch: -150.66754\n",
      "val loss in 4 epoch: -109.94408\n",
      "train loss in 4 epoch in 3400 batch: -206.71950\n",
      "val loss in 4 epoch: -110.71936\n",
      "train loss in 4 epoch in 3600 batch: -38.92852\n",
      "val loss in 4 epoch: -111.68169\n",
      "train loss in 4 epoch in 3800 batch: -39.59703\n",
      "val loss in 4 epoch: -112.53560\n",
      "train loss in 4 epoch in 4000 batch: -249.83163\n",
      "val loss in 4 epoch: -113.19936\n",
      "train loss in 4 epoch in 4200 batch: 30.75320\n",
      "val loss in 4 epoch: -114.20766\n",
      "train loss in 4 epoch in 4400 batch: -58.41453\n",
      "val loss in 4 epoch: -114.71473\n",
      "train loss in 4 epoch in 4600 batch: -110.70320\n",
      "val loss in 4 epoch: -115.55088\n",
      "train loss in 4 epoch in 4800 batch: -145.69044\n",
      "val loss in 4 epoch: -116.63905\n",
      "train loss in 4 epoch in 5000 batch: -239.70940\n",
      "val loss in 4 epoch: -117.30102\n",
      "train loss in 4 epoch in 5200 batch: -180.04883\n",
      "val loss in 4 epoch: -118.17056\n",
      "train loss in 4 epoch in 5400 batch: -146.50256\n",
      "val loss in 4 epoch: -118.84054\n",
      "train loss in 4 epoch in 5600 batch: -283.20801\n",
      "val loss in 4 epoch: -119.97848\n",
      "train loss in 4 epoch in 5800 batch: -114.09288\n",
      "val loss in 4 epoch: -120.67046\n",
      "train loss in 4 epoch in 6000 batch: -206.02260\n",
      "val loss in 4 epoch: -121.49149\n",
      "train loss in 4 epoch in 6200 batch: -124.48602\n",
      "val loss in 4 epoch: -122.25634\n",
      "train loss in 4 epoch in 6400 batch: -105.54929\n",
      "val loss in 4 epoch: -123.35668\n",
      "train loss in 4 epoch in 6600 batch: -0.26698\n",
      "val loss in 4 epoch: -124.16942\n",
      "train loss in 4 epoch in 6800 batch: -48.45526\n",
      "val loss in 4 epoch: -124.95611\n",
      "train loss in 4 epoch in 7000 batch: -127.16392\n",
      "val loss in 4 epoch: -125.87675\n",
      "train loss in 4 epoch in 7200 batch: -75.83733\n",
      "val loss in 4 epoch: -126.75718\n",
      "train loss in 4 epoch in 7400 batch: -134.97375\n",
      "val loss in 4 epoch: -127.42634\n"
     ]
    }
   ],
   "source": [
    "# v13\n",
    "INPUT_F = 72\n",
    "H = 256\n",
    "DROP_P = 0.1\n",
    "\n",
    "baseline_f = 9  # gittins feature\n",
    "\n",
    "model = NNWithCusomFeatures2(INPUT_F, DROP_P, H)\n",
    "\n",
    "def policy_gradient_with_baseline(logits_batch, rewards_batch, actions_batch, X_batch):\n",
    "    loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    logits_batch = torch.reshape(logits_batch, (-1, 100))\n",
    "    \n",
    "#     baselines = X_batch[:, baseline_f].detach()\n",
    "#     baselines = torch.reshape(baselines, (-1, 100))\n",
    "#     baselines_needed = baselines[np.arange(baselines.shape[0]), actions_batch.detach().numpy()]\n",
    "\n",
    "    policy = loss(logits_batch, actions_batch)\n",
    "    out = torch.mean(policy * (rewards_batch - 0.5))\n",
    "    return out\n",
    "\n",
    "learn(train, val, model, freq=200,\n",
    "      batch_size=16,lr=1e-4, epochs=4, criterion=policy_gradient_with_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v13')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509.0 597.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "532.0 696.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "547.0 636.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "526.0 646.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "501.0 640.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n",
      "504.0 638.0 tmp/b_0.21800793341522473.py tmp/b_0.36094695632683116.py\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-04b1f0f7dcc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mneural_with_nagiss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbandits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_only_nagiss,use_mean=True,True\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mneural_default\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbandits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneural\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_mean=False\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m utils.bandits.compare(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbandits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneural_with_nagiss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbandits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneural_default\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/my/bandits/utils/bandits.py\u001b[0m in \u001b[0;36mcompare\u001b[0;34m(t1, t2, T, first_is_sep_nn, verbose, set_weights_in_sep_nn)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset_weights_in_sep_nn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0minit_agent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mres1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mres2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reward'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/kaggle_environments/core.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, agents)\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunTimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/kaggle_environments/core.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(none_action)\u001b[0m\n\u001b[1;32m    639\u001b[0m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m                 \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mact_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0;31m# results is a list of tuples where the first element is an agent action and the second is the agent log\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/kaggle_environments/core.py\u001b[0m in \u001b[0;36mact_agent\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnone_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"observation\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/kaggle_environments/agent.py\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/kaggle_environments/agent.py\u001b[0m in \u001b[0;36mcallable_agent\u001b[0;34m(observation, configuration)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mexec\u001b[0;34m(observation, configuration)\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mget_initial_probs_estimations\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mg\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mget_expected_mean_std\u001b[0;34m(weights, rewards)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/integrate/quadpack.py\u001b[0m in \u001b[0;36mquad\u001b[0;34m(func, a, b, args, full_output, epsabs, epsrel, limit, points, weight, wvar, wopts, maxp1, limlst)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         retval = _quad(func, a, b, args, full_output, epsabs, epsrel, limit,\n\u001b[0m\u001b[1;32m    352\u001b[0m                        points)\n\u001b[1;32m    353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/scipy/integrate/quadpack.py\u001b[0m in \u001b[0;36m_quad\u001b[0;34m(func, a, b, args, full_output, epsabs, epsrel, limit, points)\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpoints\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minfbounds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_quadpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qagse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsabs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsrel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_quadpack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qagie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minfbounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfull_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsabs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsrel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mpdf\u001b[0;34m(p, weights, rewards, normalization)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# v13\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,True\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=False\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"models/nagiss_v13\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0269,  0.0722,  0.0958,  ...,  0.0060, -0.0049, -0.0259],\n",
       "        [-0.0678, -0.1197, -0.1033,  ...,  0.0255, -0.0558, -0.0673],\n",
       "        [ 0.0392,  0.1247,  0.0980,  ...,  0.0802, -0.0334,  0.0140],\n",
       "        ...,\n",
       "        [ 0.0385,  0.0472,  0.1031,  ...,  0.0349,  0.0768,  0.0432],\n",
       "        [ 0.0998,  0.0202,  0.1445,  ..., -0.0355, -0.0189,  0.0373],\n",
       "        [ 0.1110,  0.0506,  0.0584,  ...,  0.0982,  0.0425,  0.0193]])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()['model_ff.2.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 200 batch: 2.92199\n",
      "val loss in 1 epoch: 3.35883\n",
      "train loss in 1 epoch in 400 batch: 3.02275\n",
      "val loss in 1 epoch: 3.25613\n",
      "train loss in 1 epoch in 600 batch: 3.82501\n",
      "val loss in 1 epoch: 3.20623\n",
      "train loss in 1 epoch in 800 batch: 3.61449\n",
      "val loss in 1 epoch: 3.17061\n",
      "train loss in 1 epoch in 1000 batch: 3.38468\n",
      "val loss in 1 epoch: 3.13840\n",
      "train loss in 1 epoch in 1200 batch: 3.35493\n",
      "val loss in 1 epoch: 3.11387\n",
      "train loss in 1 epoch in 1400 batch: 3.62872\n",
      "val loss in 1 epoch: 3.11241\n",
      "train loss in 1 epoch in 1600 batch: 2.70347\n",
      "val loss in 1 epoch: 3.13029\n",
      "train loss in 1 epoch in 1800 batch: 2.82388\n",
      "val loss in 1 epoch: 3.10540\n",
      "train loss in 1 epoch in 2000 batch: 2.82319\n",
      "val loss in 1 epoch: 3.10048\n",
      "train loss in 1 epoch in 2200 batch: 3.38223\n",
      "val loss in 1 epoch: 3.09638\n",
      "train loss in 1 epoch in 2400 batch: 3.38437\n",
      "val loss in 1 epoch: 3.07828\n",
      "train loss in 1 epoch in 2600 batch: 2.98415\n",
      "val loss in 1 epoch: 3.10255\n",
      "train loss in 1 epoch in 2800 batch: 2.59249\n",
      "val loss in 1 epoch: 3.07689\n",
      "train loss in 1 epoch in 3000 batch: 3.87751\n",
      "val loss in 1 epoch: 3.09039\n",
      "train loss in 1 epoch in 3200 batch: 3.22012\n",
      "val loss in 1 epoch: 3.07253\n",
      "train loss in 1 epoch in 3400 batch: 3.08010\n",
      "val loss in 1 epoch: 3.06589\n",
      "train loss in 1 epoch in 3600 batch: 4.19604\n",
      "val loss in 1 epoch: 3.06027\n",
      "train loss in 1 epoch in 3800 batch: 2.43300\n",
      "val loss in 1 epoch: 3.06796\n",
      "train loss in 1 epoch in 4000 batch: 3.62742\n",
      "val loss in 1 epoch: 3.06652\n",
      "train loss in 1 epoch in 4200 batch: 2.77414\n",
      "val loss in 1 epoch: 3.05176\n",
      "train loss in 1 epoch in 4400 batch: 3.79232\n",
      "val loss in 1 epoch: 3.04949\n",
      "train loss in 1 epoch in 4600 batch: 3.07224\n",
      "val loss in 1 epoch: 3.05629\n",
      "train loss in 1 epoch in 4800 batch: 3.62605\n",
      "val loss in 1 epoch: 3.05412\n",
      "train loss in 1 epoch in 5000 batch: 3.48783\n",
      "val loss in 1 epoch: 3.03695\n",
      "train loss in 1 epoch in 5200 batch: 2.62303\n",
      "val loss in 1 epoch: 3.03539\n",
      "train loss in 1 epoch in 5400 batch: 3.49602\n",
      "val loss in 1 epoch: 3.04716\n",
      "train loss in 1 epoch in 5600 batch: 3.67932\n",
      "val loss in 1 epoch: 3.03561\n",
      "train loss in 1 epoch in 5800 batch: 3.09004\n",
      "val loss in 1 epoch: 3.03714\n",
      "train loss in 1 epoch in 6000 batch: 2.69882\n",
      "val loss in 1 epoch: 3.02469\n",
      "train loss in 1 epoch in 6200 batch: 3.33552\n",
      "val loss in 1 epoch: 3.03150\n",
      "train loss in 1 epoch in 6400 batch: 2.85779\n",
      "val loss in 1 epoch: 3.03592\n",
      "train loss in 1 epoch in 6600 batch: 3.60527\n",
      "val loss in 1 epoch: 3.02327\n",
      "train loss in 1 epoch in 6800 batch: 3.80080\n",
      "val loss in 1 epoch: 3.02559\n",
      "train loss in 1 epoch in 7000 batch: 2.99944\n",
      "val loss in 1 epoch: 3.04130\n",
      "train loss in 1 epoch in 7200 batch: 2.78039\n",
      "val loss in 1 epoch: 3.04068\n",
      "train loss in 1 epoch in 7400 batch: 3.14263\n",
      "val loss in 1 epoch: 3.02126\n",
      "train loss in 2 epoch in 200 batch: 3.24798\n",
      "val loss in 2 epoch: 3.01244\n",
      "train loss in 2 epoch in 400 batch: 3.33655\n",
      "val loss in 2 epoch: 3.01784\n",
      "train loss in 2 epoch in 600 batch: 3.41118\n",
      "val loss in 2 epoch: 3.02085\n",
      "train loss in 2 epoch in 800 batch: 2.96712\n",
      "val loss in 2 epoch: 3.01624\n",
      "train loss in 2 epoch in 1000 batch: 3.48471\n",
      "val loss in 2 epoch: 3.00956\n",
      "train loss in 2 epoch in 1200 batch: 2.99247\n",
      "val loss in 2 epoch: 3.01522\n",
      "train loss in 2 epoch in 1400 batch: 2.83214\n",
      "val loss in 2 epoch: 3.01200\n",
      "train loss in 2 epoch in 1600 batch: 2.67271\n",
      "val loss in 2 epoch: 3.00656\n",
      "train loss in 2 epoch in 1800 batch: 2.34406\n",
      "val loss in 2 epoch: 3.01817\n",
      "train loss in 2 epoch in 2000 batch: 3.33473\n",
      "val loss in 2 epoch: 3.01041\n",
      "train loss in 2 epoch in 2200 batch: 3.68428\n",
      "val loss in 2 epoch: 3.01644\n",
      "train loss in 2 epoch in 2400 batch: 3.21996\n",
      "val loss in 2 epoch: 3.00568\n",
      "train loss in 2 epoch in 2600 batch: 3.26688\n",
      "val loss in 2 epoch: 3.00231\n",
      "train loss in 2 epoch in 2800 batch: 3.25438\n",
      "val loss in 2 epoch: 3.00004\n",
      "train loss in 2 epoch in 3000 batch: 3.51668\n",
      "val loss in 2 epoch: 3.00880\n",
      "train loss in 2 epoch in 3200 batch: 3.11785\n",
      "val loss in 2 epoch: 3.00914\n",
      "train loss in 2 epoch in 3400 batch: 1.89195\n",
      "val loss in 2 epoch: 2.99625\n",
      "train loss in 2 epoch in 3600 batch: 3.58876\n",
      "val loss in 2 epoch: 2.98680\n",
      "train loss in 2 epoch in 3800 batch: 4.07734\n",
      "val loss in 2 epoch: 2.98764\n",
      "train loss in 2 epoch in 4000 batch: 3.47786\n",
      "val loss in 2 epoch: 2.99477\n",
      "train loss in 2 epoch in 4200 batch: 2.98653\n",
      "val loss in 2 epoch: 2.98363\n",
      "train loss in 2 epoch in 4400 batch: 3.31505\n",
      "val loss in 2 epoch: 2.99496\n",
      "train loss in 2 epoch in 4600 batch: 2.44877\n",
      "val loss in 2 epoch: 2.99007\n",
      "train loss in 2 epoch in 4800 batch: 3.74272\n",
      "val loss in 2 epoch: 2.97765\n",
      "train loss in 2 epoch in 5000 batch: 3.53734\n",
      "val loss in 2 epoch: 2.98394\n",
      "train loss in 2 epoch in 5200 batch: 3.51290\n",
      "val loss in 2 epoch: 2.99818\n",
      "train loss in 2 epoch in 5400 batch: 3.93134\n",
      "val loss in 2 epoch: 2.98711\n",
      "train loss in 2 epoch in 5600 batch: 3.06526\n",
      "val loss in 2 epoch: 2.98977\n",
      "train loss in 2 epoch in 5800 batch: 2.94545\n",
      "val loss in 2 epoch: 2.98683\n",
      "train loss in 2 epoch in 6000 batch: 3.29950\n",
      "val loss in 2 epoch: 2.98006\n",
      "train loss in 2 epoch in 6200 batch: 1.67280\n",
      "val loss in 2 epoch: 2.99519\n",
      "train loss in 2 epoch in 6400 batch: 2.37386\n",
      "val loss in 2 epoch: 2.99588\n",
      "train loss in 2 epoch in 6600 batch: 3.14638\n",
      "val loss in 2 epoch: 2.97670\n",
      "train loss in 2 epoch in 6800 batch: 3.38796\n",
      "val loss in 2 epoch: 2.98091\n",
      "train loss in 2 epoch in 7000 batch: 2.55160\n",
      "val loss in 2 epoch: 2.99293\n",
      "train loss in 2 epoch in 7200 batch: 3.07593\n",
      "val loss in 2 epoch: 2.98386\n",
      "train loss in 2 epoch in 7400 batch: 3.31785\n",
      "val loss in 2 epoch: 2.97780\n",
      "train loss in 3 epoch in 200 batch: 2.80688\n",
      "val loss in 3 epoch: 2.99103\n",
      "train loss in 3 epoch in 400 batch: 3.49211\n",
      "val loss in 3 epoch: 2.97496\n",
      "train loss in 3 epoch in 600 batch: 2.54492\n",
      "val loss in 3 epoch: 2.96814\n",
      "train loss in 3 epoch in 800 batch: 2.71109\n",
      "val loss in 3 epoch: 2.97261\n",
      "train loss in 3 epoch in 1000 batch: 2.69264\n",
      "val loss in 3 epoch: 2.97354\n",
      "train loss in 3 epoch in 1200 batch: 3.58548\n",
      "val loss in 3 epoch: 2.97250\n",
      "train loss in 3 epoch in 1400 batch: 2.51612\n",
      "val loss in 3 epoch: 2.96697\n",
      "train loss in 3 epoch in 1600 batch: 2.40613\n",
      "val loss in 3 epoch: 2.96918\n",
      "train loss in 3 epoch in 1800 batch: 2.62344\n",
      "val loss in 3 epoch: 2.97484\n",
      "train loss in 3 epoch in 2000 batch: 2.08574\n",
      "val loss in 3 epoch: 2.97806\n",
      "train loss in 3 epoch in 2200 batch: 3.05754\n",
      "val loss in 3 epoch: 2.96897\n",
      "train loss in 3 epoch in 2400 batch: 2.94272\n",
      "val loss in 3 epoch: 2.96281\n",
      "train loss in 3 epoch in 2600 batch: 3.44657\n",
      "val loss in 3 epoch: 2.95949\n",
      "train loss in 3 epoch in 2800 batch: 4.23959\n",
      "val loss in 3 epoch: 2.96293\n",
      "train loss in 3 epoch in 3000 batch: 2.87307\n",
      "val loss in 3 epoch: 2.96545\n",
      "train loss in 3 epoch in 3200 batch: 3.85475\n",
      "val loss in 3 epoch: 2.97063\n",
      "train loss in 3 epoch in 3400 batch: 2.43207\n",
      "val loss in 3 epoch: 2.95833\n",
      "train loss in 3 epoch in 3600 batch: 3.23174\n",
      "val loss in 3 epoch: 2.97255\n",
      "train loss in 3 epoch in 3800 batch: 3.40083\n",
      "val loss in 3 epoch: 2.96320\n",
      "train loss in 3 epoch in 4000 batch: 3.55597\n",
      "val loss in 3 epoch: 2.98306\n",
      "train loss in 3 epoch in 4200 batch: 2.25121\n",
      "val loss in 3 epoch: 2.99738\n",
      "train loss in 3 epoch in 4400 batch: 2.99034\n",
      "val loss in 3 epoch: 2.96205\n",
      "train loss in 3 epoch in 4600 batch: 2.84077\n",
      "val loss in 3 epoch: 2.97833\n",
      "train loss in 3 epoch in 4800 batch: 3.79854\n",
      "val loss in 3 epoch: 2.96468\n",
      "train loss in 3 epoch in 5000 batch: 2.60745\n",
      "val loss in 3 epoch: 2.96782\n",
      "train loss in 3 epoch in 5200 batch: 2.75140\n",
      "val loss in 3 epoch: 2.95814\n",
      "train loss in 3 epoch in 5400 batch: 3.14650\n",
      "val loss in 3 epoch: 2.96345\n",
      "train loss in 3 epoch in 5600 batch: 1.99467\n",
      "val loss in 3 epoch: 2.95827\n",
      "train loss in 3 epoch in 5800 batch: 3.01578\n",
      "val loss in 3 epoch: 2.95249\n",
      "train loss in 3 epoch in 6000 batch: 4.04297\n",
      "val loss in 3 epoch: 2.97539\n",
      "train loss in 3 epoch in 6200 batch: 2.73434\n",
      "val loss in 3 epoch: 2.94819\n",
      "train loss in 3 epoch in 6400 batch: 2.49196\n",
      "val loss in 3 epoch: 2.98326\n",
      "train loss in 3 epoch in 6600 batch: 3.02777\n",
      "val loss in 3 epoch: 2.97341\n",
      "train loss in 3 epoch in 6800 batch: 3.41959\n",
      "val loss in 3 epoch: 2.96845\n",
      "train loss in 3 epoch in 7000 batch: 3.92204\n",
      "val loss in 3 epoch: 2.94725\n",
      "train loss in 3 epoch in 7200 batch: 2.88963\n",
      "val loss in 3 epoch: 2.95404\n",
      "train loss in 3 epoch in 7400 batch: 2.90338\n",
      "val loss in 3 epoch: 2.94610\n",
      "train loss in 4 epoch in 200 batch: 2.97381\n",
      "val loss in 4 epoch: 2.95603\n",
      "train loss in 4 epoch in 400 batch: 2.49693\n",
      "val loss in 4 epoch: 2.95911\n",
      "train loss in 4 epoch in 600 batch: 3.67830\n",
      "val loss in 4 epoch: 2.95509\n",
      "train loss in 4 epoch in 800 batch: 3.42267\n",
      "val loss in 4 epoch: 2.93842\n",
      "train loss in 4 epoch in 1000 batch: 3.16476\n",
      "val loss in 4 epoch: 2.96409\n",
      "train loss in 4 epoch in 1200 batch: 2.04341\n",
      "val loss in 4 epoch: 2.97050\n",
      "train loss in 4 epoch in 1400 batch: 3.37196\n",
      "val loss in 4 epoch: 2.95630\n",
      "train loss in 4 epoch in 1600 batch: 3.03574\n",
      "val loss in 4 epoch: 2.95759\n",
      "train loss in 4 epoch in 1800 batch: 2.64129\n",
      "val loss in 4 epoch: 2.94607\n",
      "train loss in 4 epoch in 2000 batch: 2.47967\n",
      "val loss in 4 epoch: 2.95130\n",
      "train loss in 4 epoch in 2200 batch: 3.00083\n",
      "val loss in 4 epoch: 2.94112\n",
      "train loss in 4 epoch in 2400 batch: 3.41507\n",
      "val loss in 4 epoch: 2.95275\n",
      "train loss in 4 epoch in 2600 batch: 3.25711\n",
      "val loss in 4 epoch: 2.94990\n",
      "train loss in 4 epoch in 2800 batch: 2.44596\n",
      "val loss in 4 epoch: 2.94696\n",
      "train loss in 4 epoch in 3000 batch: 3.14059\n",
      "val loss in 4 epoch: 2.94440\n",
      "train loss in 4 epoch in 3200 batch: 3.57812\n",
      "val loss in 4 epoch: 2.95061\n",
      "train loss in 4 epoch in 3400 batch: 2.72624\n",
      "val loss in 4 epoch: 2.95762\n",
      "train loss in 4 epoch in 3600 batch: 2.66246\n",
      "val loss in 4 epoch: 2.94397\n",
      "train loss in 4 epoch in 3800 batch: 3.14203\n",
      "val loss in 4 epoch: 2.95198\n",
      "train loss in 4 epoch in 4000 batch: 3.20103\n",
      "val loss in 4 epoch: 2.95603\n",
      "train loss in 4 epoch in 4200 batch: 3.03092\n",
      "val loss in 4 epoch: 2.94619\n",
      "train loss in 4 epoch in 4400 batch: 3.08221\n",
      "val loss in 4 epoch: 2.95609\n",
      "train loss in 4 epoch in 4600 batch: 2.65119\n",
      "val loss in 4 epoch: 2.95336\n",
      "train loss in 4 epoch in 4800 batch: 3.18991\n",
      "val loss in 4 epoch: 2.94100\n",
      "train loss in 4 epoch in 5000 batch: 3.10476\n",
      "val loss in 4 epoch: 2.94905\n",
      "train loss in 4 epoch in 5200 batch: 1.99553\n",
      "val loss in 4 epoch: 2.94328\n",
      "train loss in 4 epoch in 5400 batch: 3.03614\n",
      "val loss in 4 epoch: 2.95390\n",
      "train loss in 4 epoch in 5600 batch: 3.65861\n",
      "val loss in 4 epoch: 2.94634\n",
      "train loss in 4 epoch in 5800 batch: 2.53866\n",
      "val loss in 4 epoch: 2.96250\n",
      "train loss in 4 epoch in 6000 batch: 2.95577\n",
      "val loss in 4 epoch: 2.95161\n",
      "train loss in 4 epoch in 6200 batch: 2.40995\n",
      "val loss in 4 epoch: 2.94905\n",
      "train loss in 4 epoch in 6400 batch: 3.25316\n",
      "val loss in 4 epoch: 2.96247\n",
      "train loss in 4 epoch in 6600 batch: 2.57859\n",
      "val loss in 4 epoch: 2.94492\n",
      "train loss in 4 epoch in 6800 batch: 1.76785\n",
      "val loss in 4 epoch: 2.94213\n",
      "train loss in 4 epoch in 7000 batch: 1.78920\n",
      "val loss in 4 epoch: 2.94665\n",
      "train loss in 4 epoch in 7200 batch: 3.03924\n",
      "val loss in 4 epoch: 2.94144\n",
      "train loss in 4 epoch in 7400 batch: 3.32986\n",
      "val loss in 4 epoch: 2.93633\n"
     ]
    }
   ],
   "source": [
    "# v14\n",
    "INPUT_F = 72\n",
    "H = 256\n",
    "DROP_P = 0.1\n",
    "\n",
    "model = NNWithCusomFeatures2(INPUT_F, DROP_P, H)\n",
    "\n",
    "def policy_copy(logits_batch, rewards_batch, actions_batch, _):\n",
    "    loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    logits_batch = torch.reshape(logits_batch, (-1, 100))\n",
    "    policy = loss(logits_batch, actions_batch)\n",
    "    out = torch.mean(policy)\n",
    "    return out\n",
    "\n",
    "learn(train, val, model, freq=200, batch_size=16,lr=1e-4, epochs=4, criterion=policy_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v14')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "702.0 652.0 tmp/b_0.9104238836280365.py tmp/b_0.5616156202153793.py\n",
      "566.0 586.0 tmp/b_0.9104238836280365.py tmp/b_0.5616156202153793.py\n",
      "596.0 582.0 tmp/b_0.9104238836280365.py tmp/b_0.5616156202153793.py\n",
      "692.0 627.0 tmp/b_0.9104238836280365.py tmp/b_0.5616156202153793.py\n",
      "626.0 584.0 tmp/b_0.9104238836280365.py tmp/b_0.5616156202153793.py\n",
      "663.0 648.0 tmp/b_0.9104238836280365.py tmp/b_0.5616156202153793.py\n",
      "690.0 592.0 tmp/b_0.9104238836280365.py tmp/b_0.5616156202153793.py\n",
      "687.0 702.0 tmp/b_0.9104238836280365.py tmp/b_0.5616156202153793.py\n",
      "678.0 649.0 tmp/b_0.9104238836280365.py tmp/b_0.5616156202153793.py\n",
      "576.0 524.0 tmp/b_0.9104238836280365.py tmp/b_0.5616156202153793.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0023762587560710747,\n",
       " 33.0,\n",
       " 34.34239362653687,\n",
       " 0.8,\n",
       " 'tmp/b_0.9104238836280365.py',\n",
       " 'tmp/b_0.5616156202153793.py',\n",
       " array([702., 566., 596., 692., 626., 663., 690., 687., 678., 576.]),\n",
       " array([652., 586., 582., 627., 584., 648., 592., 702., 649., 524.]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v14\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,True\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=False\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566.0 588.0 tmp/b_0.26055979727588763.py tmp/b_0.955884903168955.py\n",
      "677.0 644.0 tmp/b_0.26055979727588763.py tmp/b_0.955884903168955.py\n",
      "658.0 628.0 tmp/b_0.26055979727588763.py tmp/b_0.955884903168955.py\n",
      "611.0 599.0 tmp/b_0.26055979727588763.py tmp/b_0.955884903168955.py\n",
      "643.0 572.0 tmp/b_0.26055979727588763.py tmp/b_0.955884903168955.py\n",
      "612.0 609.0 tmp/b_0.26055979727588763.py tmp/b_0.955884903168955.py\n",
      "613.0 641.0 tmp/b_0.26055979727588763.py tmp/b_0.955884903168955.py\n",
      "641.0 665.0 tmp/b_0.26055979727588763.py tmp/b_0.955884903168955.py\n",
      "634.0 657.0 tmp/b_0.26055979727588763.py tmp/b_0.955884903168955.py\n",
      "669.0 627.0 tmp/b_0.26055979727588763.py tmp/b_0.955884903168955.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3575011703035106,\n",
       " 9.4,\n",
       " 32.30541750233233,\n",
       " 0.6,\n",
       " 'tmp/b_0.26055979727588763.py',\n",
       " 'tmp/b_0.955884903168955.py',\n",
       " array([566., 677., 658., 611., 643., 612., 613., 641., 634., 669.]),\n",
       " array([588., 644., 628., 599., 572., 609., 641., 665., 657., 627.]))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v14 vs neural with mean\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,True\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nagiss in top 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sessions/20201227/18775092 ['sessions/20201227/18775092/8667645.json', 'sessions/20201227/18775092/9895519.json', 'sessions/20201227/18775092/7994638.json', 'sessions/20201227/18775092/7991093.json', 'sessions/20201227/18775092/7988820.json', 'sessions/20201227/18775092/9818142.json', 'sessions/20201227/18775092/8718088.json', 'sessions/20201227/18775092/8386716.json', 'sessions/20201227/18775092/9782190.json', 'sessions/20201227/18775092/8038792.json', 'sessions/20201227/18775092/9748485.json', 'sessions/20201227/18775092/8652496.json', 'sessions/20201227/18775092/9800050.json', 'sessions/20201227/18775092/8233786.json', 'sessions/20201227/18775092/8073493.json', 'sessions/20201227/18775092/9042670.json', 'sessions/20201227/18775092/8012231.json', 'sessions/20201227/18775092/9609526.json', 'sessions/20201227/18775092/8752347.json', 'sessions/20201227/18775092/9345093.json', 'sessions/20201227/18775092/7995268.json', 'sessions/20201227/18775092/8046220.json', 'sessions/20201227/18775092/8321309.json', 'sessions/20201227/18775092/9913372.json', 'sessions/20201227/18775092/8102725.json', 'sessions/20201227/18775092/8636292.json', 'sessions/20201227/18775092/9575823.json', 'sessions/20201227/18775092/9644517.json', 'sessions/20201227/18775092/9731722.json', 'sessions/20201227/18775092/9765485.json', 'sessions/20201227/18775092/8146617.json', 'sessions/20201227/18775092/7986639.json', 'sessions/20201227/18775092/9712961.json', 'sessions/20201227/18775092/8029737.json', 'sessions/20201227/18775092/8089900.json', 'sessions/20201227/18775092/8354985.json', 'sessions/20201227/18775092/8782648.json', 'sessions/20201227/18775092/7995644.json', 'sessions/20201227/18775092/8894599.json', 'sessions/20201227/18775092/7998867.json', 'sessions/20201227/18775092/8257767.json', 'sessions/20201227/18775092/7992085.json', 'sessions/20201227/18775092/7987842.json', 'sessions/20201227/18775092/8172893.json', 'sessions/20201227/18775092/8201003.json', 'sessions/20201227/18775092/8009354.json', 'sessions/20201227/18775092/9412149.json', 'sessions/20201227/18775092/9245733.json', 'sessions/20201227/18775092/7988519.json', 'sessions/20201227/18775092/9262047.json', 'sessions/20201227/18775092/8573496.json', 'sessions/20201227/18775092/9539854.json', 'sessions/20201227/18775092/9328065.json', 'sessions/20201227/18775092/7987183.json', 'sessions/20201227/18775092/7992694.json', 'sessions/20201227/18775092/7991811.json', 'sessions/20201227/18775092/9378293.json', 'sessions/20201227/18775092/8464420.json', 'sessions/20201227/18775092/10005266.json', 'sessions/20201227/18775092/8186631.json', 'sessions/20201227/18775092/9488388.json', 'sessions/20201227/18775092/8054877.json', 'sessions/20201227/18775092/8249308.json', 'sessions/20201227/18775092/8273034.json', 'sessions/20201227/18775092/8846664.json', 'sessions/20201227/18775092/8735972.json', 'sessions/20201227/18775092/7987567.json', 'sessions/20201227/18775092/9178517.json', 'sessions/20201227/18775092/9461887.json', 'sessions/20201227/18775092/8371128.json', 'sessions/20201227/18775092/9558133.json', 'sessions/20201227/18775092/8060412.json', 'sessions/20201227/18775092/8541194.json', 'sessions/20201227/18775092/8481316.json', 'sessions/20201227/18775092/8683270.json', 'sessions/20201227/18775092/7994974.json', 'sessions/20201227/18775092/9279184.json', 'sessions/20201227/18775092/9967359.json', 'sessions/20201227/18775092/9296137.json', 'sessions/20201227/18775092/8132606.json', 'sessions/20201227/18775092/9853682.json', 'sessions/20201227/18775092/9107255.json', 'sessions/20201227/18775092/9361121.json', 'sessions/20201227/18775092/7989777.json', 'sessions/20201227/18775092/8621563.json', 'sessions/20201227/18775092/8606240.json', 'sessions/20201227/18775092/7992382.json', 'sessions/20201227/18775092/8160880.json', 'sessions/20201227/18775092/8005498.json', 'sessions/20201227/18775092/8337440.json', 'sessions/20201227/18775092/9471625.json', 'sessions/20201227/18775092/8008069.json', 'sessions/20201227/18775092/9593000.json', 'sessions/20201227/18775092/8814634.json', 'sessions/20201227/18775092/8878997.json', 'sessions/20201227/18775092/8304463.json', 'sessions/20201227/18775092/9026104.json', 'sessions/20201227/18775092/7994017.json', 'sessions/20201227/18775092/7995939.json', 'sessions/20201227/18775092/8510303.json', 'sessions/20201227/18775092/8798801.json', 'sessions/20201227/18775092/9446079.json', 'sessions/20201227/18775092/8433263.json', 'sessions/20201227/18775092/7996231.json', 'sessions/20201227/18775092/9212131.json', 'sessions/20201227/18775092/8940653.json', 'sessions/20201227/18775092/7996548.json', 'sessions/20201227/18775092/9888861.json', 'sessions/20201227/18775092/9144179.json', 'sessions/20201227/18775092/9075680.json', 'sessions/20201227/18775092/8449014.json', 'sessions/20201227/18775092/7988184.json', 'sessions/20201227/18775092/9395157.json', 'sessions/20201227/18775092/8215761.json', 'sessions/20201227/18775092/9091679.json', 'sessions/20201227/18775092/7993014.json', 'sessions/20201227/18775092/9195912.json', 'sessions/20201227/18775092/9124533.json', 'sessions/20201227/18775092/9523498.json', 'sessions/20201227/18775092/9058966.json', 'sessions/20201227/18775092/7989458.json', 'sessions/20201227/18775092/7990100.json', 'sessions/20201227/18775092/9679559.json', 'sessions/20201227/18775092/8288389.json', 'sessions/20201227/18775092/9429565.json', 'sessions/20201227/18775092/10023445.json', 'sessions/20201227/18775092/9161181.json', 'sessions/20201227/18775092/8925060.json', 'sessions/20201227/18775092/7991414.json', 'sessions/20201227/18775092/7994342.json', 'sessions/20201227/18775092/8957934.json', 'sessions/20201227/18775092/8862735.json', 'sessions/20201227/18775092/9228990.json', 'sessions/20201227/18775092/8417594.json', 'sessions/20201227/18775092/8990599.json', 'sessions/20201227/18775092/8701004.json', 'sessions/20201227/18775092/9505089.json', 'sessions/20201227/18775092/7991731.json', 'sessions/20201227/18775092/8974063.json', 'sessions/20201227/18775092/8402256.json', 'sessions/20201227/18775092/8002300.json', 'sessions/20201227/18775092/9949124.json', 'sessions/20201227/18775092/8495892.json', 'sessions/20201227/18775092/8557211.json', 'sessions/20201227/18775092/8073192.json', 'sessions/20201227/18775092/8589678.json', 'sessions/20201227/18775092/9835873.json', 'sessions/20201227/18775092/8909786.json', 'sessions/20201227/18775092/10041188.json', 'sessions/20201227/18775092/8831209.json', 'sessions/20201227/18775092/8118585.json', 'sessions/20201227/18775092/8016051.json', 'sessions/20201227/18775092/9871428.json', 'sessions/20201227/18775092/9312054.json', 'sessions/20201227/18775092/7993371.json', 'sessions/20201227/18775092/7989156.json', 'sessions/20201227/18775092/9662861.json', 'sessions/20201227/18775092/9626174.json', 'sessions/20201227/18775092/7990750.json', 'sessions/20201227/18775092/8021586.json', 'sessions/20201227/18775092/7993701.json', 'sessions/20201227/18775092/9985195.json', 'sessions/20201227/18775092/9694145.json', 'sessions/20201227/18775092/9930876.json', 'sessions/20201227/18775092/9009495.json', 'sessions/20201227/18775092/7990443.json', 'sessions/20201227/18775092/8079696.json', 'sessions/20201227/18775092/10059473.json', 'sessions/20201227/18775092/7987514.json', 'sessions/20201227/18775092/8767271.json', 'sessions/20201227/18775092/8525948.json']\n",
      "error while parse session sessions/20201227/18775092/8354985.json : \n",
      "error while parse session sessions/20201227/18775092/7991811.json : \n",
      "error while parse session sessions/20201227/18775092/8054877.json : \n",
      "error while parse session sessions/20201227/18775092/8005498.json : \n",
      "error while parse session sessions/20201227/18775092/7995939.json : \n",
      "error while parse session sessions/20201227/18775092/7991414.json : \n",
      "error while parse session sessions/20201227/18775092/8002300.json : \n",
      "error while parse session sessions/20201227/18775092/8831209.json : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 131/131 [06:45<00:00,  3.10s/it]\n",
      "100%|██████████| 32/32 [01:16<00:00,  2.40s/it]\n"
     ]
    }
   ],
   "source": [
    "nagiss_top_10 = snowden.collect_dataset_from_dir(\n",
    "    neural_agent_class,\n",
    "    'sessions/20201227/18775092',\n",
    "    val_ratio=2/10,\n",
    "    n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_dataset(nagiss_top_10[0], nagiss_top_10[1], 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 200 batch: 1.36220\n",
      "val loss in 1 epoch: 0.99618\n",
      "train loss in 1 epoch in 400 batch: 0.31884\n",
      "val loss in 1 epoch: 0.91305\n",
      "train loss in 1 epoch in 600 batch: 0.66208\n",
      "val loss in 1 epoch: 0.90011\n",
      "train loss in 1 epoch in 800 batch: 0.61705\n",
      "val loss in 1 epoch: 0.89267\n",
      "train loss in 1 epoch in 1000 batch: 0.06988\n",
      "val loss in 1 epoch: 0.88315\n",
      "train loss in 1 epoch in 1200 batch: 0.65496\n",
      "val loss in 1 epoch: 0.87599\n",
      "train loss in 1 epoch in 1400 batch: 1.08106\n",
      "val loss in 1 epoch: 0.86881\n",
      "train loss in 1 epoch in 1600 batch: 0.11978\n",
      "val loss in 1 epoch: 0.86409\n",
      "train loss in 1 epoch in 1800 batch: 0.29621\n",
      "val loss in 1 epoch: 0.85758\n",
      "train loss in 1 epoch in 2000 batch: 0.49103\n",
      "val loss in 1 epoch: 0.85109\n",
      "train loss in 1 epoch in 2200 batch: 0.99746\n",
      "val loss in 1 epoch: 0.84623\n",
      "train loss in 1 epoch in 2400 batch: 0.33409\n",
      "val loss in 1 epoch: 0.84369\n",
      "train loss in 1 epoch in 2600 batch: 1.36439\n",
      "val loss in 1 epoch: 0.84170\n",
      "train loss in 1 epoch in 2800 batch: 1.64272\n",
      "val loss in 1 epoch: 0.83837\n",
      "train loss in 1 epoch in 3000 batch: 0.31446\n",
      "val loss in 1 epoch: 0.83574\n",
      "train loss in 1 epoch in 3200 batch: 0.29427\n",
      "val loss in 1 epoch: 0.83852\n",
      "train loss in 1 epoch in 3400 batch: 0.63197\n",
      "val loss in 1 epoch: 0.83438\n",
      "train loss in 1 epoch in 3600 batch: 1.26079\n",
      "val loss in 1 epoch: 0.83217\n",
      "train loss in 1 epoch in 3800 batch: 0.92535\n",
      "val loss in 1 epoch: 0.83211\n",
      "train loss in 1 epoch in 4000 batch: 0.86649\n",
      "val loss in 1 epoch: 0.83055\n",
      "train loss in 1 epoch in 4200 batch: 0.53427\n",
      "val loss in 1 epoch: 0.83046\n",
      "train loss in 1 epoch in 4400 batch: 1.33703\n",
      "val loss in 1 epoch: 0.82918\n",
      "train loss in 1 epoch in 4600 batch: 2.08869\n",
      "val loss in 1 epoch: 0.82787\n",
      "train loss in 1 epoch in 4800 batch: 0.33121\n",
      "val loss in 1 epoch: 0.82877\n",
      "train loss in 1 epoch in 5000 batch: 0.76567\n",
      "val loss in 1 epoch: 0.82909\n",
      "train loss in 1 epoch in 5200 batch: 1.00386\n",
      "val loss in 1 epoch: 0.82671\n",
      "train loss in 1 epoch in 5400 batch: 0.88476\n",
      "val loss in 1 epoch: 0.82672\n",
      "train loss in 1 epoch in 5600 batch: 1.77256\n",
      "val loss in 1 epoch: 0.82971\n",
      "train loss in 1 epoch in 5800 batch: 0.63567\n",
      "val loss in 1 epoch: 0.82739\n",
      "train loss in 1 epoch in 6000 batch: 0.42874\n",
      "val loss in 1 epoch: 0.82718\n",
      "train loss in 1 epoch in 6200 batch: 0.85381\n",
      "val loss in 1 epoch: 0.82496\n",
      "train loss in 1 epoch in 6400 batch: 1.15805\n",
      "val loss in 1 epoch: 0.82460\n",
      "train loss in 1 epoch in 6600 batch: 1.15674\n",
      "val loss in 1 epoch: 0.82440\n",
      "train loss in 1 epoch in 6800 batch: 1.52846\n",
      "val loss in 1 epoch: 0.82301\n",
      "train loss in 1 epoch in 7000 batch: 0.65112\n",
      "val loss in 1 epoch: 0.82529\n",
      "train loss in 1 epoch in 7200 batch: 0.23068\n",
      "val loss in 1 epoch: 0.82301\n",
      "train loss in 1 epoch in 7400 batch: 0.93621\n",
      "val loss in 1 epoch: 0.82240\n",
      "train loss in 1 epoch in 7600 batch: 0.73618\n",
      "val loss in 1 epoch: 0.82324\n",
      "train loss in 1 epoch in 7800 batch: 0.81372\n",
      "val loss in 1 epoch: 0.82307\n",
      "train loss in 1 epoch in 8000 batch: 0.69256\n",
      "val loss in 1 epoch: 0.82379\n",
      "train loss in 1 epoch in 8200 batch: 0.55559\n",
      "val loss in 1 epoch: 0.82366\n",
      "train loss in 1 epoch in 8400 batch: 0.68631\n",
      "val loss in 1 epoch: 0.82259\n",
      "train loss in 1 epoch in 8600 batch: 1.01419\n",
      "val loss in 1 epoch: 0.82112\n",
      "train loss in 1 epoch in 8800 batch: 0.59293\n",
      "val loss in 1 epoch: 0.82421\n",
      "train loss in 1 epoch in 9000 batch: 1.64681\n",
      "val loss in 1 epoch: 0.82317\n",
      "train loss in 1 epoch in 9200 batch: 0.76329\n",
      "val loss in 1 epoch: 0.82149\n",
      "train loss in 1 epoch in 9400 batch: 0.95866\n",
      "val loss in 1 epoch: 0.81950\n",
      "train loss in 1 epoch in 9600 batch: 1.16686\n",
      "val loss in 1 epoch: 0.82020\n",
      "train loss in 1 epoch in 9800 batch: 0.52218\n",
      "val loss in 1 epoch: 0.81939\n",
      "train loss in 1 epoch in 10000 batch: 0.44679\n",
      "val loss in 1 epoch: 0.81838\n",
      "train loss in 1 epoch in 10200 batch: 1.09480\n",
      "val loss in 1 epoch: 0.82009\n",
      "train loss in 1 epoch in 10400 batch: 0.73365\n",
      "val loss in 1 epoch: 0.81926\n",
      "train loss in 1 epoch in 10600 batch: 0.52897\n",
      "val loss in 1 epoch: 0.81818\n",
      "train loss in 1 epoch in 10800 batch: 0.74241\n",
      "val loss in 1 epoch: 0.81898\n",
      "train loss in 1 epoch in 11000 batch: 1.72888\n",
      "val loss in 1 epoch: 0.81972\n",
      "train loss in 1 epoch in 11200 batch: 0.35364\n",
      "val loss in 1 epoch: 0.81819\n",
      "train loss in 1 epoch in 11400 batch: 0.49089\n",
      "val loss in 1 epoch: 0.81755\n",
      "train loss in 1 epoch in 11600 batch: 0.86183\n",
      "val loss in 1 epoch: 0.81646\n",
      "train loss in 1 epoch in 11800 batch: 1.58794\n",
      "val loss in 1 epoch: 0.81929\n",
      "train loss in 1 epoch in 12000 batch: 0.43689\n",
      "val loss in 1 epoch: 0.81603\n",
      "train loss in 1 epoch in 12200 batch: 0.41245\n",
      "val loss in 1 epoch: 0.81590\n",
      "train loss in 1 epoch in 12400 batch: 0.95575\n",
      "val loss in 1 epoch: 0.81620\n",
      "train loss in 1 epoch in 12600 batch: 0.78353\n",
      "val loss in 1 epoch: 0.81629\n",
      "train loss in 1 epoch in 12800 batch: 0.16405\n",
      "val loss in 1 epoch: 0.81807\n",
      "train loss in 1 epoch in 13000 batch: 0.76307\n",
      "val loss in 1 epoch: 0.81706\n",
      "train loss in 1 epoch in 13200 batch: 1.05009\n",
      "val loss in 1 epoch: 0.81621\n",
      "train loss in 1 epoch in 13400 batch: 0.74470\n",
      "val loss in 1 epoch: 0.81611\n",
      "train loss in 1 epoch in 13600 batch: 0.00000\n",
      "val loss in 1 epoch: 0.81730\n",
      "train loss in 1 epoch in 13800 batch: 1.08717\n",
      "val loss in 1 epoch: 0.81559\n",
      "train loss in 1 epoch in 14000 batch: 0.90724\n",
      "val loss in 1 epoch: 0.81527\n",
      "train loss in 1 epoch in 14200 batch: 0.78375\n",
      "val loss in 1 epoch: 0.81529\n",
      "train loss in 1 epoch in 14400 batch: 0.50894\n",
      "val loss in 1 epoch: 0.81442\n",
      "train loss in 1 epoch in 14600 batch: 1.18293\n",
      "val loss in 1 epoch: 0.81430\n",
      "train loss in 1 epoch in 14800 batch: 0.65195\n",
      "val loss in 1 epoch: 0.81301\n",
      "train loss in 1 epoch in 15000 batch: 0.81498\n",
      "val loss in 1 epoch: 0.81544\n",
      "train loss in 1 epoch in 15200 batch: 1.11277\n",
      "val loss in 1 epoch: 0.81367\n",
      "train loss in 1 epoch in 15400 batch: 1.39601\n",
      "val loss in 1 epoch: 0.81380\n",
      "train loss in 1 epoch in 15600 batch: 0.93844\n",
      "val loss in 1 epoch: 0.81420\n",
      "train loss in 1 epoch in 15800 batch: 0.90605\n",
      "val loss in 1 epoch: 0.81303\n",
      "train loss in 1 epoch in 16000 batch: 0.82854\n",
      "val loss in 1 epoch: 0.81375\n",
      "train loss in 1 epoch in 16200 batch: 0.40446\n",
      "val loss in 1 epoch: 0.81149\n",
      "train loss in 2 epoch in 200 batch: 0.76156\n",
      "val loss in 2 epoch: 0.81501\n",
      "train loss in 2 epoch in 400 batch: 0.53520\n",
      "val loss in 2 epoch: 0.81230\n",
      "train loss in 2 epoch in 600 batch: 0.35730\n",
      "val loss in 2 epoch: 0.81494\n",
      "train loss in 2 epoch in 800 batch: 0.57491\n",
      "val loss in 2 epoch: 0.81153\n",
      "train loss in 2 epoch in 1000 batch: 0.86646\n",
      "val loss in 2 epoch: 0.81040\n",
      "train loss in 2 epoch in 1200 batch: 0.05768\n",
      "val loss in 2 epoch: 0.81154\n",
      "train loss in 2 epoch in 1400 batch: 1.08133\n",
      "val loss in 2 epoch: 0.80953\n",
      "train loss in 2 epoch in 1600 batch: 1.98639\n",
      "val loss in 2 epoch: 0.80962\n",
      "train loss in 2 epoch in 1800 batch: 1.09793\n",
      "val loss in 2 epoch: 0.80963\n",
      "train loss in 2 epoch in 2000 batch: 0.23374\n",
      "val loss in 2 epoch: 0.80928\n",
      "train loss in 2 epoch in 2200 batch: 0.25822\n",
      "val loss in 2 epoch: 0.81001\n",
      "train loss in 2 epoch in 2400 batch: 1.00290\n",
      "val loss in 2 epoch: 0.81405\n",
      "train loss in 2 epoch in 2600 batch: 0.46344\n",
      "val loss in 2 epoch: 0.80941\n",
      "train loss in 2 epoch in 2800 batch: 0.93977\n",
      "val loss in 2 epoch: 0.81041\n",
      "train loss in 2 epoch in 3000 batch: 0.66250\n",
      "val loss in 2 epoch: 0.81142\n",
      "train loss in 2 epoch in 3200 batch: 1.00107\n",
      "val loss in 2 epoch: 0.81045\n",
      "train loss in 2 epoch in 3400 batch: 0.83040\n",
      "val loss in 2 epoch: 0.81183\n",
      "train loss in 2 epoch in 3600 batch: 0.54959\n",
      "val loss in 2 epoch: 0.80881\n",
      "train loss in 2 epoch in 3800 batch: 0.51611\n",
      "val loss in 2 epoch: 0.80802\n",
      "train loss in 2 epoch in 4000 batch: 0.58899\n",
      "val loss in 2 epoch: 0.81018\n",
      "train loss in 2 epoch in 4200 batch: 1.01685\n",
      "val loss in 2 epoch: 0.80858\n",
      "train loss in 2 epoch in 4400 batch: 1.18389\n",
      "val loss in 2 epoch: 0.80823\n",
      "train loss in 2 epoch in 4600 batch: 1.54326\n",
      "val loss in 2 epoch: 0.80803\n",
      "train loss in 2 epoch in 4800 batch: 0.27365\n",
      "val loss in 2 epoch: 0.80857\n",
      "train loss in 2 epoch in 5000 batch: 0.93868\n",
      "val loss in 2 epoch: 0.81002\n",
      "train loss in 2 epoch in 5200 batch: 1.14202\n",
      "val loss in 2 epoch: 0.80681\n",
      "train loss in 2 epoch in 5400 batch: 0.85706\n",
      "val loss in 2 epoch: 0.80834\n",
      "train loss in 2 epoch in 5600 batch: 1.13934\n",
      "val loss in 2 epoch: 0.80831\n",
      "train loss in 2 epoch in 5800 batch: 1.59316\n",
      "val loss in 2 epoch: 0.80552\n",
      "train loss in 2 epoch in 6000 batch: 0.65795\n",
      "val loss in 2 epoch: 0.80596\n",
      "train loss in 2 epoch in 6200 batch: 0.92639\n",
      "val loss in 2 epoch: 0.80534\n",
      "train loss in 2 epoch in 6400 batch: 0.58212\n",
      "val loss in 2 epoch: 0.80654\n",
      "train loss in 2 epoch in 6600 batch: 0.95995\n",
      "val loss in 2 epoch: 0.80432\n",
      "train loss in 2 epoch in 6800 batch: 1.18637\n",
      "val loss in 2 epoch: 0.80433\n",
      "train loss in 2 epoch in 7000 batch: 1.09695\n",
      "val loss in 2 epoch: 0.80975\n",
      "train loss in 2 epoch in 7200 batch: 1.01743\n",
      "val loss in 2 epoch: 0.80646\n",
      "train loss in 2 epoch in 7400 batch: 0.76450\n",
      "val loss in 2 epoch: 0.80438\n",
      "train loss in 2 epoch in 7600 batch: 2.05856\n",
      "val loss in 2 epoch: 0.80429\n",
      "train loss in 2 epoch in 7800 batch: 0.89168\n",
      "val loss in 2 epoch: 0.80382\n",
      "train loss in 2 epoch in 8000 batch: 0.69788\n",
      "val loss in 2 epoch: 0.80475\n",
      "train loss in 2 epoch in 8200 batch: 1.00615\n",
      "val loss in 2 epoch: 0.80694\n",
      "train loss in 2 epoch in 8400 batch: 0.08600\n",
      "val loss in 2 epoch: 0.80632\n",
      "train loss in 2 epoch in 8600 batch: 1.36695\n",
      "val loss in 2 epoch: 0.80401\n",
      "train loss in 2 epoch in 8800 batch: 0.57059\n",
      "val loss in 2 epoch: 0.80509\n",
      "train loss in 2 epoch in 9000 batch: 0.61426\n",
      "val loss in 2 epoch: 0.80447\n",
      "train loss in 2 epoch in 9200 batch: 0.84435\n",
      "val loss in 2 epoch: 0.80478\n",
      "train loss in 2 epoch in 9400 batch: 0.42208\n",
      "val loss in 2 epoch: 0.80199\n",
      "train loss in 2 epoch in 9600 batch: 0.38611\n",
      "val loss in 2 epoch: 0.80184\n",
      "train loss in 2 epoch in 9800 batch: 0.31907\n",
      "val loss in 2 epoch: 0.80315\n",
      "train loss in 2 epoch in 10000 batch: 0.15234\n",
      "val loss in 2 epoch: 0.80192\n",
      "train loss in 2 epoch in 10200 batch: 0.70195\n",
      "val loss in 2 epoch: 0.80497\n",
      "train loss in 2 epoch in 10400 batch: 0.91669\n",
      "val loss in 2 epoch: 0.80165\n",
      "train loss in 2 epoch in 10600 batch: 0.80331\n",
      "val loss in 2 epoch: 0.80105\n",
      "train loss in 2 epoch in 10800 batch: 1.38649\n",
      "val loss in 2 epoch: 0.80156\n",
      "train loss in 2 epoch in 11000 batch: 1.61447\n",
      "val loss in 2 epoch: 0.80234\n",
      "train loss in 2 epoch in 11200 batch: 1.04293\n",
      "val loss in 2 epoch: 0.80413\n",
      "train loss in 2 epoch in 11400 batch: 1.09234\n",
      "val loss in 2 epoch: 0.80227\n",
      "train loss in 2 epoch in 11600 batch: 0.70875\n",
      "val loss in 2 epoch: 0.80140\n",
      "train loss in 2 epoch in 11800 batch: 0.84518\n",
      "val loss in 2 epoch: 0.80360\n",
      "train loss in 2 epoch in 12000 batch: 0.54388\n",
      "val loss in 2 epoch: 0.80309\n",
      "train loss in 2 epoch in 12200 batch: 0.28295\n",
      "val loss in 2 epoch: 0.80227\n",
      "train loss in 2 epoch in 12400 batch: 0.63373\n",
      "val loss in 2 epoch: 0.80140\n",
      "train loss in 2 epoch in 12600 batch: 1.66599\n",
      "val loss in 2 epoch: 0.80019\n",
      "train loss in 2 epoch in 12800 batch: 0.65517\n",
      "val loss in 2 epoch: 0.80164\n",
      "train loss in 2 epoch in 13000 batch: 1.16497\n",
      "val loss in 2 epoch: 0.80057\n",
      "train loss in 2 epoch in 13200 batch: 0.33045\n",
      "val loss in 2 epoch: 0.80033\n",
      "train loss in 2 epoch in 13400 batch: 0.97019\n",
      "val loss in 2 epoch: 0.79991\n",
      "train loss in 2 epoch in 13600 batch: 0.49605\n",
      "val loss in 2 epoch: 0.79924\n",
      "train loss in 2 epoch in 13800 batch: 0.43104\n",
      "val loss in 2 epoch: 0.80113\n",
      "train loss in 2 epoch in 14000 batch: 0.68282\n",
      "val loss in 2 epoch: 0.80014\n",
      "train loss in 2 epoch in 14200 batch: 0.56529\n",
      "val loss in 2 epoch: 0.79952\n",
      "train loss in 2 epoch in 14400 batch: 0.37141\n",
      "val loss in 2 epoch: 0.80213\n",
      "train loss in 2 epoch in 14600 batch: 0.53008\n",
      "val loss in 2 epoch: 0.79842\n",
      "train loss in 2 epoch in 14800 batch: 0.95493\n",
      "val loss in 2 epoch: 0.79920\n",
      "train loss in 2 epoch in 15000 batch: 1.17047\n",
      "val loss in 2 epoch: 0.80400\n",
      "train loss in 2 epoch in 15200 batch: 0.75877\n",
      "val loss in 2 epoch: 0.80143\n",
      "train loss in 2 epoch in 15400 batch: 0.22278\n",
      "val loss in 2 epoch: 0.79993\n",
      "train loss in 2 epoch in 15600 batch: 0.94275\n",
      "val loss in 2 epoch: 0.79933\n",
      "train loss in 2 epoch in 15800 batch: 0.57196\n",
      "val loss in 2 epoch: 0.80019\n",
      "train loss in 2 epoch in 16000 batch: 0.02777\n",
      "val loss in 2 epoch: 0.79941\n",
      "train loss in 2 epoch in 16200 batch: 0.31414\n",
      "val loss in 2 epoch: 0.79983\n",
      "train loss in 3 epoch in 200 batch: 1.17847\n",
      "val loss in 3 epoch: 0.79959\n",
      "train loss in 3 epoch in 400 batch: 0.91977\n",
      "val loss in 3 epoch: 0.79736\n",
      "train loss in 3 epoch in 600 batch: 0.26918\n",
      "val loss in 3 epoch: 0.79935\n",
      "train loss in 3 epoch in 800 batch: 0.76760\n",
      "val loss in 3 epoch: 0.79701\n",
      "train loss in 3 epoch in 1000 batch: 1.01441\n",
      "val loss in 3 epoch: 0.79893\n",
      "train loss in 3 epoch in 1200 batch: 1.09919\n",
      "val loss in 3 epoch: 0.79903\n",
      "train loss in 3 epoch in 1400 batch: 0.34876\n",
      "val loss in 3 epoch: 0.79904\n",
      "train loss in 3 epoch in 1600 batch: 2.38822\n",
      "val loss in 3 epoch: 0.79908\n",
      "train loss in 3 epoch in 1800 batch: 0.28965\n",
      "val loss in 3 epoch: 0.80096\n",
      "train loss in 3 epoch in 2000 batch: 0.34199\n",
      "val loss in 3 epoch: 0.79974\n",
      "train loss in 3 epoch in 2200 batch: 0.47426\n",
      "val loss in 3 epoch: 0.79677\n",
      "train loss in 3 epoch in 2400 batch: 0.12578\n",
      "val loss in 3 epoch: 0.79754\n",
      "train loss in 3 epoch in 2600 batch: 1.05666\n",
      "val loss in 3 epoch: 0.80173\n",
      "train loss in 3 epoch in 2800 batch: 0.97087\n",
      "val loss in 3 epoch: 0.79909\n",
      "train loss in 3 epoch in 3000 batch: 0.31378\n",
      "val loss in 3 epoch: 0.79885\n",
      "train loss in 3 epoch in 3200 batch: 0.64724\n",
      "val loss in 3 epoch: 0.79920\n",
      "train loss in 3 epoch in 3400 batch: 0.59141\n",
      "val loss in 3 epoch: 0.79558\n",
      "train loss in 3 epoch in 3600 batch: 0.30603\n",
      "val loss in 3 epoch: 0.79871\n",
      "train loss in 3 epoch in 3800 batch: 0.41374\n",
      "val loss in 3 epoch: 0.79546\n",
      "train loss in 3 epoch in 4000 batch: 0.70361\n",
      "val loss in 3 epoch: 0.79847\n",
      "train loss in 3 epoch in 4200 batch: 0.47149\n",
      "val loss in 3 epoch: 0.80005\n",
      "train loss in 3 epoch in 4400 batch: 1.02394\n",
      "val loss in 3 epoch: 0.79508\n",
      "train loss in 3 epoch in 4600 batch: 0.72873\n",
      "val loss in 3 epoch: 0.79792\n",
      "train loss in 3 epoch in 4800 batch: 0.78011\n",
      "val loss in 3 epoch: 0.79908\n",
      "train loss in 3 epoch in 5000 batch: 0.97466\n",
      "val loss in 3 epoch: 0.79825\n",
      "train loss in 3 epoch in 5200 batch: 1.50745\n",
      "val loss in 3 epoch: 0.79517\n",
      "train loss in 3 epoch in 5400 batch: 0.86986\n",
      "val loss in 3 epoch: 0.79818\n",
      "train loss in 3 epoch in 5600 batch: 0.72631\n",
      "val loss in 3 epoch: 0.79556\n",
      "train loss in 3 epoch in 5800 batch: 0.36652\n",
      "val loss in 3 epoch: 0.79287\n",
      "train loss in 3 epoch in 6000 batch: 0.65552\n",
      "val loss in 3 epoch: 0.79582\n",
      "train loss in 3 epoch in 6200 batch: 0.46296\n",
      "val loss in 3 epoch: 0.79415\n",
      "train loss in 3 epoch in 6400 batch: 1.17090\n",
      "val loss in 3 epoch: 0.79335\n",
      "train loss in 3 epoch in 6600 batch: 0.97669\n",
      "val loss in 3 epoch: 0.79737\n",
      "train loss in 3 epoch in 6800 batch: 0.04829\n",
      "val loss in 3 epoch: 0.79413\n",
      "train loss in 3 epoch in 7000 batch: 0.86322\n",
      "val loss in 3 epoch: 0.79503\n",
      "train loss in 3 epoch in 7200 batch: 1.27838\n",
      "val loss in 3 epoch: 0.79363\n",
      "train loss in 3 epoch in 7400 batch: 0.02021\n",
      "val loss in 3 epoch: 0.79452\n",
      "train loss in 3 epoch in 7600 batch: 0.62909\n",
      "val loss in 3 epoch: 0.79209\n",
      "train loss in 3 epoch in 7800 batch: 1.12786\n",
      "val loss in 3 epoch: 0.79465\n",
      "train loss in 3 epoch in 8000 batch: 0.16609\n",
      "val loss in 3 epoch: 0.79709\n",
      "train loss in 3 epoch in 8200 batch: 0.80926\n",
      "val loss in 3 epoch: 0.79529\n",
      "train loss in 3 epoch in 8400 batch: 1.56605\n",
      "val loss in 3 epoch: 0.79244\n",
      "train loss in 3 epoch in 8600 batch: 0.71464\n",
      "val loss in 3 epoch: 0.79255\n",
      "train loss in 3 epoch in 8800 batch: 0.48262\n",
      "val loss in 3 epoch: 0.79468\n",
      "train loss in 3 epoch in 9000 batch: 1.23481\n",
      "val loss in 3 epoch: 0.79454\n",
      "train loss in 3 epoch in 9200 batch: 0.06832\n",
      "val loss in 3 epoch: 0.79531\n",
      "train loss in 3 epoch in 9400 batch: 1.03401\n",
      "val loss in 3 epoch: 0.79228\n",
      "train loss in 3 epoch in 9600 batch: 0.03071\n",
      "val loss in 3 epoch: 0.79896\n",
      "train loss in 3 epoch in 9800 batch: 0.36078\n",
      "val loss in 3 epoch: 0.79246\n",
      "train loss in 3 epoch in 10000 batch: 0.29597\n",
      "val loss in 3 epoch: 0.79268\n",
      "train loss in 3 epoch in 10200 batch: 0.60870\n",
      "val loss in 3 epoch: 0.79264\n",
      "train loss in 3 epoch in 10400 batch: 0.08708\n",
      "val loss in 3 epoch: 0.79551\n",
      "train loss in 3 epoch in 10600 batch: 0.03458\n",
      "val loss in 3 epoch: 0.79752\n",
      "train loss in 3 epoch in 10800 batch: 0.76120\n",
      "val loss in 3 epoch: 0.79279\n",
      "train loss in 3 epoch in 11000 batch: 0.22516\n",
      "val loss in 3 epoch: 0.79328\n",
      "train loss in 3 epoch in 11200 batch: 0.44708\n",
      "val loss in 3 epoch: 0.79194\n",
      "train loss in 3 epoch in 11400 batch: 0.59527\n",
      "val loss in 3 epoch: 0.79305\n",
      "train loss in 3 epoch in 11600 batch: 0.97203\n",
      "val loss in 3 epoch: 0.79206\n",
      "train loss in 3 epoch in 11800 batch: 0.49102\n",
      "val loss in 3 epoch: 0.79398\n",
      "train loss in 3 epoch in 12000 batch: 0.98046\n",
      "val loss in 3 epoch: 0.79177\n",
      "train loss in 3 epoch in 12200 batch: 0.47403\n",
      "val loss in 3 epoch: 0.79367\n",
      "train loss in 3 epoch in 12400 batch: 1.47027\n",
      "val loss in 3 epoch: 0.79780\n",
      "train loss in 3 epoch in 12600 batch: 1.23243\n",
      "val loss in 3 epoch: 0.79326\n",
      "train loss in 3 epoch in 12800 batch: 1.16789\n",
      "val loss in 3 epoch: 0.79514\n",
      "train loss in 3 epoch in 13000 batch: 0.26616\n",
      "val loss in 3 epoch: 0.79286\n",
      "train loss in 3 epoch in 13200 batch: 1.47727\n",
      "val loss in 3 epoch: 0.79120\n",
      "train loss in 3 epoch in 13400 batch: 0.86569\n",
      "val loss in 3 epoch: 0.79179\n",
      "train loss in 3 epoch in 13600 batch: 0.30119\n",
      "val loss in 3 epoch: 0.79386\n",
      "train loss in 3 epoch in 13800 batch: 0.62920\n",
      "val loss in 3 epoch: 0.79110\n",
      "train loss in 3 epoch in 14000 batch: 1.16406\n",
      "val loss in 3 epoch: 0.79352\n",
      "train loss in 3 epoch in 14200 batch: 0.31553\n",
      "val loss in 3 epoch: 0.79186\n",
      "train loss in 3 epoch in 14400 batch: 1.04489\n",
      "val loss in 3 epoch: 0.79185\n",
      "train loss in 3 epoch in 14600 batch: 0.97938\n",
      "val loss in 3 epoch: 0.79344\n",
      "train loss in 3 epoch in 14800 batch: 0.91250\n",
      "val loss in 3 epoch: 0.79518\n",
      "train loss in 3 epoch in 15000 batch: 0.97071\n",
      "val loss in 3 epoch: 0.79277\n",
      "train loss in 3 epoch in 15200 batch: 0.76956\n",
      "val loss in 3 epoch: 0.79274\n",
      "train loss in 3 epoch in 15400 batch: 0.93458\n",
      "val loss in 3 epoch: 0.79400\n",
      "train loss in 3 epoch in 15600 batch: 0.63245\n",
      "val loss in 3 epoch: 0.79305\n",
      "train loss in 3 epoch in 15800 batch: 0.92509\n",
      "val loss in 3 epoch: 0.79271\n",
      "train loss in 3 epoch in 16000 batch: 1.63339\n",
      "val loss in 3 epoch: 0.79233\n",
      "train loss in 3 epoch in 16200 batch: 0.50076\n",
      "val loss in 3 epoch: 0.79042\n",
      "train loss in 4 epoch in 200 batch: 1.23740\n",
      "val loss in 4 epoch: 0.79617\n",
      "train loss in 4 epoch in 400 batch: 0.23797\n",
      "val loss in 4 epoch: 0.79197\n",
      "train loss in 4 epoch in 600 batch: 0.87811\n",
      "val loss in 4 epoch: 0.79028\n",
      "train loss in 4 epoch in 800 batch: 0.33584\n",
      "val loss in 4 epoch: 0.79209\n",
      "train loss in 4 epoch in 1000 batch: 0.55235\n",
      "val loss in 4 epoch: 0.79235\n",
      "train loss in 4 epoch in 1200 batch: 0.19217\n",
      "val loss in 4 epoch: 0.79100\n",
      "train loss in 4 epoch in 1400 batch: 0.63839\n",
      "val loss in 4 epoch: 0.78996\n",
      "train loss in 4 epoch in 1600 batch: 0.04841\n",
      "val loss in 4 epoch: 0.79189\n",
      "train loss in 4 epoch in 1800 batch: 1.15907\n",
      "val loss in 4 epoch: 0.78947\n",
      "train loss in 4 epoch in 2000 batch: 1.20162\n",
      "val loss in 4 epoch: 0.78994\n",
      "train loss in 4 epoch in 2200 batch: 0.05260\n",
      "val loss in 4 epoch: 0.79275\n",
      "train loss in 4 epoch in 2400 batch: 0.95228\n",
      "val loss in 4 epoch: 0.79411\n",
      "train loss in 4 epoch in 2600 batch: 0.85173\n",
      "val loss in 4 epoch: 0.79118\n",
      "train loss in 4 epoch in 2800 batch: 0.98816\n",
      "val loss in 4 epoch: 0.78935\n",
      "train loss in 4 epoch in 3000 batch: 0.86162\n",
      "val loss in 4 epoch: 0.79362\n",
      "train loss in 4 epoch in 3200 batch: 0.66500\n",
      "val loss in 4 epoch: 0.78946\n",
      "train loss in 4 epoch in 3400 batch: 1.07019\n",
      "val loss in 4 epoch: 0.79030\n",
      "train loss in 4 epoch in 3600 batch: 1.25195\n",
      "val loss in 4 epoch: 0.79227\n",
      "train loss in 4 epoch in 3800 batch: 0.60626\n",
      "val loss in 4 epoch: 0.78995\n",
      "train loss in 4 epoch in 4000 batch: 0.49028\n",
      "val loss in 4 epoch: 0.78851\n",
      "train loss in 4 epoch in 4200 batch: 0.40207\n",
      "val loss in 4 epoch: 0.78984\n",
      "train loss in 4 epoch in 4400 batch: 0.01764\n",
      "val loss in 4 epoch: 0.79130\n",
      "train loss in 4 epoch in 4600 batch: 0.36376\n",
      "val loss in 4 epoch: 0.78919\n",
      "train loss in 4 epoch in 4800 batch: 0.66439\n",
      "val loss in 4 epoch: 0.78849\n",
      "train loss in 4 epoch in 5000 batch: 0.71222\n",
      "val loss in 4 epoch: 0.79087\n",
      "train loss in 4 epoch in 5200 batch: 0.78793\n",
      "val loss in 4 epoch: 0.78900\n",
      "train loss in 4 epoch in 5400 batch: 0.76790\n",
      "val loss in 4 epoch: 0.78885\n",
      "train loss in 4 epoch in 5600 batch: 0.70912\n",
      "val loss in 4 epoch: 0.78890\n",
      "train loss in 4 epoch in 5800 batch: 0.38661\n",
      "val loss in 4 epoch: 0.78787\n",
      "train loss in 4 epoch in 6000 batch: 0.81501\n",
      "val loss in 4 epoch: 0.78878\n",
      "train loss in 4 epoch in 6200 batch: 0.49932\n",
      "val loss in 4 epoch: 0.79294\n",
      "train loss in 4 epoch in 6400 batch: 0.89111\n",
      "val loss in 4 epoch: 0.78999\n",
      "train loss in 4 epoch in 6600 batch: 0.67350\n",
      "val loss in 4 epoch: 0.78846\n",
      "train loss in 4 epoch in 6800 batch: 0.15381\n",
      "val loss in 4 epoch: 0.78881\n",
      "train loss in 4 epoch in 7000 batch: 0.67841\n",
      "val loss in 4 epoch: 0.78967\n",
      "train loss in 4 epoch in 7200 batch: 0.95975\n",
      "val loss in 4 epoch: 0.78997\n",
      "train loss in 4 epoch in 7400 batch: 0.51068\n",
      "val loss in 4 epoch: 0.79208\n",
      "train loss in 4 epoch in 7600 batch: 0.23658\n",
      "val loss in 4 epoch: 0.79124\n",
      "train loss in 4 epoch in 7800 batch: 0.88354\n",
      "val loss in 4 epoch: 0.78804\n",
      "train loss in 4 epoch in 8000 batch: 0.49100\n",
      "val loss in 4 epoch: 0.79053\n",
      "train loss in 4 epoch in 8200 batch: 1.28378\n",
      "val loss in 4 epoch: 0.78712\n",
      "train loss in 4 epoch in 8400 batch: 0.81878\n",
      "val loss in 4 epoch: 0.78729\n",
      "train loss in 4 epoch in 8600 batch: 1.38586\n",
      "val loss in 4 epoch: 0.78649\n",
      "train loss in 4 epoch in 8800 batch: 0.76111\n",
      "val loss in 4 epoch: 0.79051\n",
      "train loss in 4 epoch in 9000 batch: 0.39656\n",
      "val loss in 4 epoch: 0.79016\n",
      "train loss in 4 epoch in 9200 batch: 1.18437\n",
      "val loss in 4 epoch: 0.78697\n",
      "train loss in 4 epoch in 9400 batch: 1.16089\n",
      "val loss in 4 epoch: 0.79020\n",
      "train loss in 4 epoch in 9600 batch: 0.36570\n",
      "val loss in 4 epoch: 0.78727\n",
      "train loss in 4 epoch in 9800 batch: 0.32830\n",
      "val loss in 4 epoch: 0.79292\n",
      "train loss in 4 epoch in 10000 batch: 0.64308\n",
      "val loss in 4 epoch: 0.78915\n",
      "train loss in 4 epoch in 10200 batch: 0.37957\n",
      "val loss in 4 epoch: 0.78784\n",
      "train loss in 4 epoch in 10400 batch: 0.11919\n",
      "val loss in 4 epoch: 0.78896\n",
      "train loss in 4 epoch in 10600 batch: 0.29404\n",
      "val loss in 4 epoch: 0.79186\n",
      "train loss in 4 epoch in 10800 batch: 0.26729\n",
      "val loss in 4 epoch: 0.79099\n",
      "train loss in 4 epoch in 11000 batch: 0.60622\n",
      "val loss in 4 epoch: 0.78932\n",
      "train loss in 4 epoch in 11200 batch: 0.57747\n",
      "val loss in 4 epoch: 0.79108\n",
      "train loss in 4 epoch in 11400 batch: 1.16786\n",
      "val loss in 4 epoch: 0.79212\n",
      "train loss in 4 epoch in 11600 batch: 0.82218\n",
      "val loss in 4 epoch: 0.78757\n",
      "train loss in 4 epoch in 11800 batch: 0.90479\n",
      "val loss in 4 epoch: 0.78915\n",
      "train loss in 4 epoch in 12000 batch: 0.24898\n",
      "val loss in 4 epoch: 0.78982\n",
      "train loss in 4 epoch in 12200 batch: 0.56960\n",
      "val loss in 4 epoch: 0.78615\n",
      "train loss in 4 epoch in 12400 batch: 0.20078\n",
      "val loss in 4 epoch: 0.79579\n",
      "train loss in 4 epoch in 12600 batch: 0.92590\n",
      "val loss in 4 epoch: 0.78641\n",
      "train loss in 4 epoch in 12800 batch: 0.82832\n",
      "val loss in 4 epoch: 0.78633\n",
      "train loss in 4 epoch in 13000 batch: 1.48854\n",
      "val loss in 4 epoch: 0.78928\n",
      "train loss in 4 epoch in 13200 batch: 1.60796\n",
      "val loss in 4 epoch: 0.79016\n",
      "train loss in 4 epoch in 13400 batch: 1.39958\n",
      "val loss in 4 epoch: 0.78876\n",
      "train loss in 4 epoch in 13600 batch: 1.04981\n",
      "val loss in 4 epoch: 0.78872\n",
      "train loss in 4 epoch in 13800 batch: 0.69757\n",
      "val loss in 4 epoch: 0.78664\n",
      "train loss in 4 epoch in 14000 batch: 0.86527\n",
      "val loss in 4 epoch: 0.78625\n",
      "train loss in 4 epoch in 14200 batch: 1.32332\n",
      "val loss in 4 epoch: 0.78700\n",
      "train loss in 4 epoch in 14400 batch: 1.10267\n",
      "val loss in 4 epoch: 0.78647\n",
      "train loss in 4 epoch in 14600 batch: 0.81914\n",
      "val loss in 4 epoch: 0.78826\n",
      "train loss in 4 epoch in 14800 batch: 0.25799\n",
      "val loss in 4 epoch: 0.78828\n",
      "train loss in 4 epoch in 15000 batch: 0.37719\n",
      "val loss in 4 epoch: 0.78738\n",
      "train loss in 4 epoch in 15200 batch: 1.06264\n",
      "val loss in 4 epoch: 0.78744\n",
      "train loss in 4 epoch in 15400 batch: 0.67873\n",
      "val loss in 4 epoch: 0.78767\n",
      "train loss in 4 epoch in 15600 batch: 0.35209\n",
      "val loss in 4 epoch: 0.78842\n",
      "train loss in 4 epoch in 15800 batch: 0.46913\n",
      "val loss in 4 epoch: 0.78828\n",
      "train loss in 4 epoch in 16000 batch: 0.99455\n",
      "val loss in 4 epoch: 0.78561\n",
      "train loss in 4 epoch in 16200 batch: 0.69626\n",
      "val loss in 4 epoch: 0.79268\n"
     ]
    }
   ],
   "source": [
    "# v15\n",
    "INPUT_F = 36\n",
    "H = 256\n",
    "DROP_P = 0.1\n",
    "\n",
    "model = NNWithCusomFeatures2(INPUT_F, DROP_P, H)\n",
    "\n",
    "def policy_gradient(logits_batch, rewards_batch, actions_batch, _):\n",
    "    loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    logits_batch = torch.reshape(logits_batch, (-1, 100))\n",
    "    policy = loss(logits_batch, actions_batch)\n",
    "    out = torch.mean(policy * rewards_batch)\n",
    "    return out\n",
    "\n",
    "learn(nagiss_top_10[0], nagiss_top_10[1], model,\n",
    "      freq=200, batch_size=16,lr=1e-4, epochs=4, criterion=policy_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622.0 591.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "664.0 606.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "610.0 571.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "632.0 631.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "652.0 651.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "660.0 562.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "607.0 644.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "679.0 660.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "666.0 645.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "653.0 625.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.016639938795987075,\n",
       " 25.9,\n",
       " 34.20365477547684,\n",
       " 0.9,\n",
       " 'tmp/b_0.8650894478131601.py',\n",
       " 'tmp/b_0.8547409200351491.py',\n",
       " array([622., 664., 610., 632., 652., 660., 607., 679., 666., 653.]),\n",
       " array([591., 606., 571., 631., 651., 562., 644., 660., 645., 625.]))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v15\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,False\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=False\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "630.0 594.0 tmp/b_0.41885852893729714.py tmp/b_0.11874647569664065.py\n",
      "625.0 642.0 tmp/b_0.41885852893729714.py tmp/b_0.11874647569664065.py\n",
      "699.0 667.0 tmp/b_0.41885852893729714.py tmp/b_0.11874647569664065.py\n",
      "643.0 643.0 tmp/b_0.41885852893729714.py tmp/b_0.11874647569664065.py\n",
      "674.0 610.0 tmp/b_0.41885852893729714.py tmp/b_0.11874647569664065.py\n",
      "723.0 737.0 tmp/b_0.41885852893729714.py tmp/b_0.11874647569664065.py\n",
      "586.0 654.0 tmp/b_0.41885852893729714.py tmp/b_0.11874647569664065.py\n",
      "633.0 657.0 tmp/b_0.41885852893729714.py tmp/b_0.11874647569664065.py\n",
      "651.0 595.0 tmp/b_0.41885852893729714.py tmp/b_0.11874647569664065.py\n",
      "644.0 655.0 tmp/b_0.41885852893729714.py tmp/b_0.11874647569664065.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6599838197678104,\n",
       " 5.4,\n",
       " 38.81546083714581,\n",
       " 0.5,\n",
       " 'tmp/b_0.41885852893729714.py',\n",
       " 'tmp/b_0.11874647569664065.py',\n",
       " array([630., 625., 699., 643., 674., 723., 586., 633., 651., 644.]),\n",
       " array([594., 642., 667., 643., 610., 737., 654., 657., 595., 655.]))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v15 vs neural with mean\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,False\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\" Applies attention mechanism on the `context` using the `query`.\n",
    "\n",
    "    **Thank you** to IBM for their initial implementation of :class:`Attention`. Here is\n",
    "    their `License\n",
    "    <https://github.com/IBM/pytorch-seq2seq/blob/master/LICENSE>`__.\n",
    "\n",
    "    Args:\n",
    "        dimensions (int): Dimensionality of the query and context.\n",
    "        attention_type (str, optional): How to compute the attention score:\n",
    "\n",
    "            * dot: :math:`score(H_j,q) = H_j^T q`\n",
    "            * general: :math:`score(H_j, q) = H_j^T W_a q`\n",
    "\n",
    "    Example:\n",
    "\n",
    "         >>> attention = Attention(256)\n",
    "         >>> query = torch.randn(5, 1, 256)\n",
    "         >>> context = torch.randn(5, 5, 256)\n",
    "         >>> output, weights = attention(query, context)\n",
    "         >>> output.size()\n",
    "         torch.Size([5, 1, 256])\n",
    "         >>> weights.size()\n",
    "         torch.Size([5, 1, 5])\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dimensions, attention_type='general'):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        if attention_type not in ['dot', 'general']:\n",
    "            raise ValueError('Invalid attention type selected.')\n",
    "\n",
    "        self.attention_type = attention_type\n",
    "        if self.attention_type == 'general':\n",
    "            self.linear_in = nn.Linear(dimensions, dimensions, bias=False)\n",
    "\n",
    "        self.linear_out = nn.Linear(dimensions * 2, dimensions, bias=False)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, query, context):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            query (:class:`torch.FloatTensor` [batch size, output length, dimensions]): Sequence of\n",
    "                queries to query the context.\n",
    "            context (:class:`torch.FloatTensor` [batch size, query length, dimensions]): Data\n",
    "                overwhich to apply the attention mechanism.\n",
    "\n",
    "        Returns:\n",
    "            :class:`tuple` with `output` and `weights`:\n",
    "            * **output** (:class:`torch.LongTensor` [batch size, output length, dimensions]):\n",
    "              Tensor containing the attended features.\n",
    "            * **weights** (:class:`torch.FloatTensor` [batch size, output length, query length]):\n",
    "              Tensor containing attention weights.\n",
    "        \"\"\"\n",
    "        batch_size, output_len, dimensions = query.size()\n",
    "        query_len = context.size(1)\n",
    "\n",
    "        if self.attention_type == \"general\":\n",
    "            query = query.reshape(batch_size * output_len, dimensions)\n",
    "            query = self.linear_in(query)\n",
    "            query = query.reshape(batch_size, output_len, dimensions)\n",
    "\n",
    "        # TODO: Include mask on PADDING_INDEX?\n",
    "\n",
    "        # (batch_size, output_len, dimensions) * (batch_size, query_len, dimensions) ->\n",
    "        # (batch_size, output_len, query_len)\n",
    "        attention_scores = torch.bmm(query, context.transpose(1, 2).contiguous())\n",
    "\n",
    "        # Compute weights across every context sequence\n",
    "        attention_scores = attention_scores.view(batch_size * output_len, query_len)\n",
    "        attention_weights = self.softmax(attention_scores)\n",
    "        attention_weights = attention_weights.view(batch_size, output_len, query_len)\n",
    "\n",
    "        # (batch_size, output_len, query_len) * (batch_size, query_len, dimensions) ->\n",
    "        # (batch_size, output_len, dimensions)\n",
    "        mix = torch.bmm(attention_weights, context)\n",
    "\n",
    "        # concat -> (batch_size * output_len, 2*dimensions)\n",
    "        combined = torch.cat((mix, query), dim=2)\n",
    "        combined = combined.view(batch_size * output_len, 2 * dimensions)\n",
    "\n",
    "        # Apply linear_out on every 2nd dimension of concat\n",
    "        # output -> (batch_size, output_len, dimensions)\n",
    "        output = self.linear_out(combined).view(batch_size, output_len, dimensions)\n",
    "        output = self.tanh(output)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNWithAttention(nn.Module):\n",
    "    def __init__(self, INPUT_F, DROP_P, H):\n",
    "        super().__init__()\n",
    "        self.H = H\n",
    "        self.O = 100\n",
    "        INPUT_F_C = INPUT_F + 2 * INPUT_F\n",
    "        self.model_ff_1 =  nn.Sequential(\n",
    "            nn.BatchNorm1d(INPUT_F_C),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(INPUT_F_C, H),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(H, H),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Dropout(DROP_P),\n",
    "#             nn.Linear(H, 1)\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.attn = Attention(self.H, attention_type='general')\n",
    "        \n",
    "        self.model_ff_2 =  nn.Sequential(\n",
    "            nn.Linear(H, H),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(DROP_P),\n",
    "            nn.Linear(H, 1),\n",
    "#             nn.Sigmoid(),\n",
    "#             nn.Dropout(DROP_P),\n",
    "#             nn.Linear(H, 1)\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def make_features(self, x):\n",
    "        x = x[:, :36]   # v9\n",
    "        lg = torch.log(1 + torch.abs(x))\n",
    "        sn = torch.sin(x)\n",
    "        input_x = torch.cat([x, lg, sn], axis=1)\n",
    "        return input_x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.make_features(x)\n",
    "        query = self.model_ff_1(x)\n",
    "        hidden = torch.reshape(query, (-1, self.O, self.H))\n",
    "        B = hidden.shape[0]\n",
    "        b_ones = torch.ones((self.O, self.O))\n",
    "#         mask = torch.block_diag([b_ones for _ in np.arange(B)])\n",
    "        mask = 1\n",
    "#         print(query.shape, hidden.shape)\n",
    "#         assert False\n",
    "        weighted_query, weights = self.attn(hidden, hidden)\n",
    "        weighted_query = torch.reshape(weighted_query, (-1, self.H))\n",
    "        out = self.model_ff_2(weighted_query)\n",
    "#         print(query.shape, hidden.shape, weighted_query.shape, weights.shape, out.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 200 batch: 1.11466\n",
      "val loss in 1 epoch: 0.91816\n",
      "train loss in 1 epoch in 400 batch: 0.37721\n",
      "val loss in 1 epoch: 0.90005\n",
      "train loss in 1 epoch in 600 batch: 0.63423\n",
      "val loss in 1 epoch: 0.86460\n",
      "train loss in 1 epoch in 800 batch: 0.53577\n",
      "val loss in 1 epoch: 0.85394\n",
      "train loss in 1 epoch in 1000 batch: 0.06103\n",
      "val loss in 1 epoch: 0.87833\n",
      "train loss in 1 epoch in 1200 batch: 0.60838\n",
      "val loss in 1 epoch: 0.87533\n",
      "train loss in 1 epoch in 1400 batch: 1.06349\n",
      "val loss in 1 epoch: 0.84282\n",
      "train loss in 1 epoch in 1600 batch: 0.15884\n",
      "val loss in 1 epoch: 0.84613\n",
      "train loss in 1 epoch in 1800 batch: 0.26516\n",
      "val loss in 1 epoch: 0.83164\n",
      "train loss in 1 epoch in 2000 batch: 0.40338\n",
      "val loss in 1 epoch: 0.83390\n",
      "train loss in 1 epoch in 2200 batch: 1.01126\n",
      "val loss in 1 epoch: 0.83487\n",
      "train loss in 1 epoch in 2400 batch: 0.24559\n",
      "val loss in 1 epoch: 0.82700\n",
      "train loss in 1 epoch in 2600 batch: 1.43094\n",
      "val loss in 1 epoch: 0.83151\n",
      "train loss in 1 epoch in 2800 batch: 1.65811\n",
      "val loss in 1 epoch: 0.83457\n",
      "train loss in 1 epoch in 3000 batch: 0.27593\n",
      "val loss in 1 epoch: 0.82337\n",
      "train loss in 1 epoch in 3200 batch: 0.28317\n",
      "val loss in 1 epoch: 0.82588\n",
      "train loss in 1 epoch in 3400 batch: 0.66342\n",
      "val loss in 1 epoch: 0.82105\n",
      "train loss in 1 epoch in 3600 batch: 1.29261\n",
      "val loss in 1 epoch: 0.81658\n",
      "train loss in 1 epoch in 3800 batch: 0.84523\n",
      "val loss in 1 epoch: 0.81471\n",
      "train loss in 1 epoch in 4000 batch: 0.85893\n",
      "val loss in 1 epoch: 0.81338\n",
      "train loss in 1 epoch in 4200 batch: 0.54805\n",
      "val loss in 1 epoch: 0.81901\n",
      "train loss in 1 epoch in 4400 batch: 1.42272\n",
      "val loss in 1 epoch: 0.81731\n",
      "train loss in 1 epoch in 4600 batch: 2.00144\n",
      "val loss in 1 epoch: 0.81487\n",
      "train loss in 1 epoch in 4800 batch: 0.37707\n",
      "val loss in 1 epoch: 0.80716\n",
      "train loss in 1 epoch in 5000 batch: 0.66627\n",
      "val loss in 1 epoch: 0.80885\n",
      "train loss in 1 epoch in 5200 batch: 0.94819\n",
      "val loss in 1 epoch: 0.80932\n",
      "train loss in 1 epoch in 5400 batch: 0.78555\n",
      "val loss in 1 epoch: 0.80526\n",
      "train loss in 1 epoch in 5600 batch: 1.75861\n",
      "val loss in 1 epoch: 0.80372\n",
      "train loss in 1 epoch in 5800 batch: 0.63797\n",
      "val loss in 1 epoch: 0.80351\n",
      "train loss in 1 epoch in 6000 batch: 0.38469\n",
      "val loss in 1 epoch: 0.80872\n",
      "train loss in 1 epoch in 6200 batch: 0.79932\n",
      "val loss in 1 epoch: 0.80337\n",
      "train loss in 1 epoch in 6400 batch: 1.14328\n",
      "val loss in 1 epoch: 0.80070\n",
      "train loss in 1 epoch in 6600 batch: 1.19509\n",
      "val loss in 1 epoch: 0.80347\n",
      "train loss in 1 epoch in 6800 batch: 1.57013\n",
      "val loss in 1 epoch: 0.80608\n",
      "train loss in 1 epoch in 7000 batch: 0.62910\n",
      "val loss in 1 epoch: 0.79489\n",
      "train loss in 1 epoch in 7200 batch: 0.20349\n",
      "val loss in 1 epoch: 0.80584\n",
      "train loss in 1 epoch in 7400 batch: 0.83411\n",
      "val loss in 1 epoch: 0.79696\n",
      "train loss in 1 epoch in 7600 batch: 0.79822\n",
      "val loss in 1 epoch: 0.80811\n",
      "train loss in 1 epoch in 7800 batch: 0.66809\n",
      "val loss in 1 epoch: 0.79908\n",
      "train loss in 1 epoch in 8000 batch: 0.64226\n",
      "val loss in 1 epoch: 0.79943\n",
      "train loss in 1 epoch in 8200 batch: 0.57306\n",
      "val loss in 1 epoch: 0.80469\n",
      "train loss in 1 epoch in 8400 batch: 0.73700\n",
      "val loss in 1 epoch: 0.80844\n",
      "train loss in 1 epoch in 8600 batch: 1.03900\n",
      "val loss in 1 epoch: 0.79678\n",
      "train loss in 1 epoch in 8800 batch: 0.61396\n",
      "val loss in 1 epoch: 0.80524\n",
      "train loss in 1 epoch in 9000 batch: 1.62239\n",
      "val loss in 1 epoch: 0.80386\n",
      "train loss in 1 epoch in 9200 batch: 0.75914\n",
      "val loss in 1 epoch: 0.79735\n",
      "train loss in 1 epoch in 9400 batch: 0.98137\n",
      "val loss in 1 epoch: 0.80516\n",
      "train loss in 1 epoch in 9600 batch: 0.96356\n",
      "val loss in 1 epoch: 0.79305\n",
      "train loss in 1 epoch in 9800 batch: 0.46512\n",
      "val loss in 1 epoch: 0.79661\n",
      "train loss in 1 epoch in 10000 batch: 0.38772\n",
      "val loss in 1 epoch: 0.79711\n",
      "train loss in 1 epoch in 10200 batch: 0.99438\n",
      "val loss in 1 epoch: 0.79809\n",
      "train loss in 1 epoch in 10400 batch: 0.69437\n",
      "val loss in 1 epoch: 0.79637\n",
      "train loss in 1 epoch in 10600 batch: 0.52181\n",
      "val loss in 1 epoch: 0.82205\n",
      "train loss in 1 epoch in 10800 batch: 0.69279\n",
      "val loss in 1 epoch: 0.78982\n",
      "train loss in 1 epoch in 11000 batch: 1.77517\n",
      "val loss in 1 epoch: 0.81935\n",
      "train loss in 1 epoch in 11200 batch: 0.37855\n",
      "val loss in 1 epoch: 0.78834\n",
      "train loss in 1 epoch in 11400 batch: 0.42451\n",
      "val loss in 1 epoch: 0.78506\n",
      "train loss in 1 epoch in 11600 batch: 0.89362\n",
      "val loss in 1 epoch: 0.79952\n",
      "train loss in 1 epoch in 11800 batch: 1.55833\n",
      "val loss in 1 epoch: 0.78193\n",
      "train loss in 1 epoch in 12000 batch: 0.49627\n",
      "val loss in 1 epoch: 0.78550\n",
      "train loss in 1 epoch in 12200 batch: 0.36861\n",
      "val loss in 1 epoch: 0.78710\n",
      "train loss in 1 epoch in 12400 batch: 0.89240\n",
      "val loss in 1 epoch: 0.79277\n",
      "train loss in 1 epoch in 12600 batch: 0.78055\n",
      "val loss in 1 epoch: 0.78904\n",
      "train loss in 1 epoch in 12800 batch: 0.08401\n",
      "val loss in 1 epoch: 0.79432\n",
      "train loss in 1 epoch in 13000 batch: 0.79899\n",
      "val loss in 1 epoch: 0.78332\n",
      "train loss in 1 epoch in 13200 batch: 1.01713\n",
      "val loss in 1 epoch: 0.79527\n",
      "train loss in 1 epoch in 13400 batch: 0.67837\n",
      "val loss in 1 epoch: 0.78520\n",
      "train loss in 1 epoch in 13600 batch: 0.00000\n",
      "val loss in 1 epoch: 0.78946\n",
      "train loss in 1 epoch in 13800 batch: 1.08617\n",
      "val loss in 1 epoch: 0.78932\n",
      "train loss in 1 epoch in 14000 batch: 0.81833\n",
      "val loss in 1 epoch: 0.78756\n",
      "train loss in 1 epoch in 14200 batch: 0.72100\n",
      "val loss in 1 epoch: 0.78299\n",
      "train loss in 1 epoch in 14400 batch: 0.49694\n",
      "val loss in 1 epoch: 0.77825\n",
      "train loss in 1 epoch in 14600 batch: 1.16354\n",
      "val loss in 1 epoch: 0.79099\n",
      "train loss in 1 epoch in 14800 batch: 0.56763\n",
      "val loss in 1 epoch: 0.78004\n",
      "train loss in 1 epoch in 15000 batch: 0.80957\n",
      "val loss in 1 epoch: 0.78031\n",
      "train loss in 1 epoch in 15200 batch: 1.08096\n",
      "val loss in 1 epoch: 0.78947\n",
      "train loss in 1 epoch in 15400 batch: 1.31158\n",
      "val loss in 1 epoch: 0.78270\n",
      "train loss in 1 epoch in 15600 batch: 0.91127\n",
      "val loss in 1 epoch: 0.78505\n",
      "train loss in 1 epoch in 15800 batch: 1.13486\n",
      "val loss in 1 epoch: 0.78061\n",
      "train loss in 1 epoch in 16000 batch: 0.79059\n",
      "val loss in 1 epoch: 0.77958\n",
      "train loss in 1 epoch in 16200 batch: 0.35676\n",
      "val loss in 1 epoch: 0.77719\n",
      "train loss in 2 epoch in 200 batch: 0.73624\n",
      "val loss in 2 epoch: 0.77605\n",
      "train loss in 2 epoch in 400 batch: 0.47379\n",
      "val loss in 2 epoch: 0.77674\n",
      "train loss in 2 epoch in 600 batch: 0.31282\n",
      "val loss in 2 epoch: 0.77702\n",
      "train loss in 2 epoch in 800 batch: 0.65037\n",
      "val loss in 2 epoch: 0.77275\n",
      "train loss in 2 epoch in 1000 batch: 0.86505\n",
      "val loss in 2 epoch: 0.77922\n",
      "train loss in 2 epoch in 1200 batch: 0.05162\n",
      "val loss in 2 epoch: 0.77678\n",
      "train loss in 2 epoch in 1400 batch: 1.06411\n",
      "val loss in 2 epoch: 0.78093\n",
      "train loss in 2 epoch in 1600 batch: 1.66702\n",
      "val loss in 2 epoch: 0.77450\n",
      "train loss in 2 epoch in 1800 batch: 1.08642\n",
      "val loss in 2 epoch: 0.77865\n",
      "train loss in 2 epoch in 2000 batch: 0.43662\n",
      "val loss in 2 epoch: 0.77511\n",
      "train loss in 2 epoch in 2200 batch: 0.23352\n",
      "val loss in 2 epoch: 0.77168\n",
      "train loss in 2 epoch in 2400 batch: 1.17048\n",
      "val loss in 2 epoch: 0.77591\n",
      "train loss in 2 epoch in 2600 batch: 0.42869\n",
      "val loss in 2 epoch: 0.77340\n",
      "train loss in 2 epoch in 2800 batch: 0.86820\n",
      "val loss in 2 epoch: 0.77026\n",
      "train loss in 2 epoch in 3000 batch: 0.90239\n",
      "val loss in 2 epoch: 0.77803\n",
      "train loss in 2 epoch in 3200 batch: 0.98341\n",
      "val loss in 2 epoch: 0.77882\n",
      "train loss in 2 epoch in 3400 batch: 0.55119\n",
      "val loss in 2 epoch: 0.77205\n",
      "train loss in 2 epoch in 3600 batch: 0.50242\n",
      "val loss in 2 epoch: 0.77020\n",
      "train loss in 2 epoch in 3800 batch: 0.61095\n",
      "val loss in 2 epoch: 0.76748\n",
      "train loss in 2 epoch in 4000 batch: 0.72577\n",
      "val loss in 2 epoch: 0.76969\n",
      "train loss in 2 epoch in 4200 batch: 1.02801\n",
      "val loss in 2 epoch: 0.76754\n",
      "train loss in 2 epoch in 4400 batch: 1.30888\n",
      "val loss in 2 epoch: 0.77190\n",
      "train loss in 2 epoch in 4600 batch: 1.51690\n",
      "val loss in 2 epoch: 0.76732\n",
      "train loss in 2 epoch in 4800 batch: 0.21876\n",
      "val loss in 2 epoch: 0.77761\n",
      "train loss in 2 epoch in 5000 batch: 0.79448\n",
      "val loss in 2 epoch: 0.77231\n",
      "train loss in 2 epoch in 5200 batch: 1.26296\n",
      "val loss in 2 epoch: 0.77272\n",
      "train loss in 2 epoch in 5400 batch: 0.81702\n",
      "val loss in 2 epoch: 0.77587\n",
      "train loss in 2 epoch in 5600 batch: 1.00441\n",
      "val loss in 2 epoch: 0.77113\n",
      "train loss in 2 epoch in 5800 batch: 1.44165\n",
      "val loss in 2 epoch: 0.76452\n",
      "train loss in 2 epoch in 6000 batch: 0.72288\n",
      "val loss in 2 epoch: 0.76587\n",
      "train loss in 2 epoch in 6200 batch: 0.90638\n",
      "val loss in 2 epoch: 0.79248\n",
      "train loss in 2 epoch in 6400 batch: 0.53991\n",
      "val loss in 2 epoch: 0.76785\n",
      "train loss in 2 epoch in 6600 batch: 1.01057\n",
      "val loss in 2 epoch: 0.76957\n",
      "train loss in 2 epoch in 6800 batch: 1.01360\n",
      "val loss in 2 epoch: 0.76182\n",
      "train loss in 2 epoch in 7000 batch: 0.93811\n",
      "val loss in 2 epoch: 0.77344\n",
      "train loss in 2 epoch in 7200 batch: 0.87884\n",
      "val loss in 2 epoch: 0.75928\n",
      "train loss in 2 epoch in 7400 batch: 0.81905\n",
      "val loss in 2 epoch: 0.75725\n",
      "train loss in 2 epoch in 7600 batch: 1.78257\n",
      "val loss in 2 epoch: 0.77759\n",
      "train loss in 2 epoch in 7800 batch: 0.87594\n",
      "val loss in 2 epoch: 0.76519\n",
      "train loss in 2 epoch in 8000 batch: 0.70503\n",
      "val loss in 2 epoch: 0.76539\n",
      "train loss in 2 epoch in 8200 batch: 0.87785\n",
      "val loss in 2 epoch: 0.78884\n",
      "train loss in 2 epoch in 8400 batch: 0.09554\n",
      "val loss in 2 epoch: 0.76422\n",
      "train loss in 2 epoch in 8600 batch: 1.07728\n",
      "val loss in 2 epoch: 0.76009\n",
      "train loss in 2 epoch in 8800 batch: 0.56751\n",
      "val loss in 2 epoch: 0.76673\n",
      "train loss in 2 epoch in 9000 batch: 0.59252\n",
      "val loss in 2 epoch: 0.76149\n",
      "train loss in 2 epoch in 9200 batch: 0.78339\n",
      "val loss in 2 epoch: 0.75660\n",
      "train loss in 2 epoch in 9400 batch: 0.21533\n",
      "val loss in 2 epoch: 0.75608\n",
      "train loss in 2 epoch in 9600 batch: 0.40439\n",
      "val loss in 2 epoch: 0.76003\n",
      "train loss in 2 epoch in 9800 batch: 0.21213\n",
      "val loss in 2 epoch: 0.77133\n",
      "train loss in 2 epoch in 10000 batch: 0.10952\n",
      "val loss in 2 epoch: 0.75929\n",
      "train loss in 2 epoch in 10200 batch: 0.69882\n",
      "val loss in 2 epoch: 0.77465\n",
      "train loss in 2 epoch in 10400 batch: 0.89220\n",
      "val loss in 2 epoch: 0.75961\n",
      "train loss in 2 epoch in 10600 batch: 0.75947\n",
      "val loss in 2 epoch: 0.75212\n",
      "train loss in 2 epoch in 10800 batch: 1.47574\n",
      "val loss in 2 epoch: 0.76907\n",
      "train loss in 2 epoch in 11000 batch: 1.57564\n",
      "val loss in 2 epoch: 0.75354\n",
      "train loss in 2 epoch in 11200 batch: 0.82780\n",
      "val loss in 2 epoch: 0.76139\n",
      "train loss in 2 epoch in 11400 batch: 1.06739\n",
      "val loss in 2 epoch: 0.76094\n",
      "train loss in 2 epoch in 11600 batch: 0.64636\n",
      "val loss in 2 epoch: 0.75880\n",
      "train loss in 2 epoch in 11800 batch: 0.81731\n",
      "val loss in 2 epoch: 0.75330\n",
      "train loss in 2 epoch in 12000 batch: 0.50818\n",
      "val loss in 2 epoch: 0.75677\n",
      "train loss in 2 epoch in 12200 batch: 0.27661\n",
      "val loss in 2 epoch: 0.75208\n",
      "train loss in 2 epoch in 12400 batch: 0.56184\n",
      "val loss in 2 epoch: 0.76153\n",
      "train loss in 2 epoch in 12600 batch: 1.60858\n",
      "val loss in 2 epoch: 0.75833\n",
      "train loss in 2 epoch in 12800 batch: 0.61676\n",
      "val loss in 2 epoch: 0.76614\n",
      "train loss in 2 epoch in 13000 batch: 1.23761\n",
      "val loss in 2 epoch: 0.75205\n",
      "train loss in 2 epoch in 13200 batch: 0.40417\n",
      "val loss in 2 epoch: 0.75295\n",
      "train loss in 2 epoch in 13400 batch: 0.84152\n",
      "val loss in 2 epoch: 0.75074\n",
      "train loss in 2 epoch in 13600 batch: 0.58402\n",
      "val loss in 2 epoch: 0.75795\n",
      "train loss in 2 epoch in 13800 batch: 0.34054\n",
      "val loss in 2 epoch: 0.75461\n",
      "train loss in 2 epoch in 14000 batch: 0.66083\n",
      "val loss in 2 epoch: 0.75469\n",
      "train loss in 2 epoch in 14200 batch: 0.65541\n",
      "val loss in 2 epoch: 0.75387\n",
      "train loss in 2 epoch in 14400 batch: 0.31154\n",
      "val loss in 2 epoch: 0.75627\n",
      "train loss in 2 epoch in 14600 batch: 0.63876\n",
      "val loss in 2 epoch: 0.76013\n",
      "train loss in 2 epoch in 14800 batch: 0.86238\n",
      "val loss in 2 epoch: 0.75053\n",
      "train loss in 2 epoch in 15000 batch: 1.00922\n",
      "val loss in 2 epoch: 0.76070\n",
      "train loss in 2 epoch in 15200 batch: 0.79700\n",
      "val loss in 2 epoch: 0.75450\n",
      "train loss in 2 epoch in 15400 batch: 0.26591\n",
      "val loss in 2 epoch: 0.75024\n",
      "train loss in 2 epoch in 15600 batch: 0.92849\n",
      "val loss in 2 epoch: 0.75041\n",
      "train loss in 2 epoch in 15800 batch: 0.50199\n",
      "val loss in 2 epoch: 0.75714\n",
      "train loss in 2 epoch in 16000 batch: 0.08762\n",
      "val loss in 2 epoch: 0.75917\n",
      "train loss in 2 epoch in 16200 batch: 0.28203\n",
      "val loss in 2 epoch: 0.74664\n",
      "train loss in 3 epoch in 200 batch: 1.15559\n",
      "val loss in 3 epoch: 0.76130\n",
      "train loss in 3 epoch in 400 batch: 0.90052\n",
      "val loss in 3 epoch: 0.75788\n",
      "train loss in 3 epoch in 600 batch: 0.26009\n",
      "val loss in 3 epoch: 0.75031\n",
      "train loss in 3 epoch in 800 batch: 0.70121\n",
      "val loss in 3 epoch: 0.74901\n",
      "train loss in 3 epoch in 1000 batch: 1.04025\n",
      "val loss in 3 epoch: 0.74967\n",
      "train loss in 3 epoch in 1200 batch: 1.07077\n",
      "val loss in 3 epoch: 0.75748\n",
      "train loss in 3 epoch in 1400 batch: 0.29692\n",
      "val loss in 3 epoch: 0.75875\n",
      "train loss in 3 epoch in 1600 batch: 1.97564\n",
      "val loss in 3 epoch: 0.74943\n",
      "train loss in 3 epoch in 1800 batch: 0.20378\n",
      "val loss in 3 epoch: 0.75131\n",
      "train loss in 3 epoch in 2000 batch: 0.31253\n",
      "val loss in 3 epoch: 0.75052\n",
      "train loss in 3 epoch in 2200 batch: 0.45673\n",
      "val loss in 3 epoch: 0.75068\n",
      "train loss in 3 epoch in 2400 batch: 0.09876\n",
      "val loss in 3 epoch: 0.74752\n",
      "train loss in 3 epoch in 2600 batch: 1.01066\n",
      "val loss in 3 epoch: 0.75947\n",
      "train loss in 3 epoch in 2800 batch: 1.01726\n",
      "val loss in 3 epoch: 0.75388\n",
      "train loss in 3 epoch in 3000 batch: 0.28908\n",
      "val loss in 3 epoch: 0.74584\n",
      "train loss in 3 epoch in 3200 batch: 0.67246\n",
      "val loss in 3 epoch: 0.75643\n",
      "train loss in 3 epoch in 3400 batch: 0.51515\n",
      "val loss in 3 epoch: 0.74770\n",
      "train loss in 3 epoch in 3600 batch: 0.27387\n",
      "val loss in 3 epoch: 0.75364\n",
      "train loss in 3 epoch in 3800 batch: 0.35993\n",
      "val loss in 3 epoch: 0.75899\n",
      "train loss in 3 epoch in 4000 batch: 0.67027\n",
      "val loss in 3 epoch: 0.75441\n",
      "train loss in 3 epoch in 4200 batch: 0.50648\n",
      "val loss in 3 epoch: 0.75784\n",
      "train loss in 3 epoch in 4400 batch: 0.98495\n",
      "val loss in 3 epoch: 0.74780\n",
      "train loss in 3 epoch in 4600 batch: 0.77972\n",
      "val loss in 3 epoch: 0.75994\n",
      "train loss in 3 epoch in 4800 batch: 0.65856\n",
      "val loss in 3 epoch: 0.75284\n",
      "train loss in 3 epoch in 5000 batch: 0.81545\n",
      "val loss in 3 epoch: 0.75637\n",
      "train loss in 3 epoch in 5200 batch: 1.28438\n",
      "val loss in 3 epoch: 0.74657\n",
      "train loss in 3 epoch in 5400 batch: 0.86858\n",
      "val loss in 3 epoch: 0.74979\n",
      "train loss in 3 epoch in 5600 batch: 0.71037\n",
      "val loss in 3 epoch: 0.75454\n",
      "train loss in 3 epoch in 5800 batch: 0.39574\n",
      "val loss in 3 epoch: 0.74284\n",
      "train loss in 3 epoch in 6000 batch: 0.62920\n",
      "val loss in 3 epoch: 0.75062\n",
      "train loss in 3 epoch in 6200 batch: 0.54229\n",
      "val loss in 3 epoch: 0.74751\n",
      "train loss in 3 epoch in 6400 batch: 0.88790\n",
      "val loss in 3 epoch: 0.74394\n",
      "train loss in 3 epoch in 6600 batch: 1.02845\n",
      "val loss in 3 epoch: 0.74945\n",
      "train loss in 3 epoch in 6800 batch: 0.07259\n",
      "val loss in 3 epoch: 0.75190\n",
      "train loss in 3 epoch in 7000 batch: 0.83510\n",
      "val loss in 3 epoch: 0.74759\n",
      "train loss in 3 epoch in 7200 batch: 0.91997\n",
      "val loss in 3 epoch: 0.74274\n",
      "train loss in 3 epoch in 7400 batch: 0.05105\n",
      "val loss in 3 epoch: 0.74025\n",
      "train loss in 3 epoch in 7600 batch: 0.51624\n",
      "val loss in 3 epoch: 0.75458\n",
      "train loss in 3 epoch in 7800 batch: 0.98366\n",
      "val loss in 3 epoch: 0.74327\n",
      "train loss in 3 epoch in 8000 batch: 0.13640\n",
      "val loss in 3 epoch: 0.75298\n",
      "train loss in 3 epoch in 8200 batch: 0.61366\n",
      "val loss in 3 epoch: 0.74545\n",
      "train loss in 3 epoch in 8400 batch: 1.61458\n",
      "val loss in 3 epoch: 0.74356\n",
      "train loss in 3 epoch in 8600 batch: 0.71656\n",
      "val loss in 3 epoch: 0.74465\n",
      "train loss in 3 epoch in 8800 batch: 0.30226\n",
      "val loss in 3 epoch: 0.74420\n",
      "train loss in 3 epoch in 9000 batch: 1.14038\n",
      "val loss in 3 epoch: 0.74808\n",
      "train loss in 3 epoch in 9200 batch: 0.07585\n",
      "val loss in 3 epoch: 0.74132\n",
      "train loss in 3 epoch in 9400 batch: 1.04132\n",
      "val loss in 3 epoch: 0.74491\n",
      "train loss in 3 epoch in 9600 batch: 0.02896\n",
      "val loss in 3 epoch: 0.75132\n",
      "train loss in 3 epoch in 9800 batch: 0.31388\n",
      "val loss in 3 epoch: 0.75529\n",
      "train loss in 3 epoch in 10000 batch: 0.31375\n",
      "val loss in 3 epoch: 0.75347\n",
      "train loss in 3 epoch in 10200 batch: 0.60356\n",
      "val loss in 3 epoch: 0.74708\n",
      "train loss in 3 epoch in 10400 batch: 0.02440\n",
      "val loss in 3 epoch: 0.74369\n",
      "train loss in 3 epoch in 10600 batch: 0.05364\n",
      "val loss in 3 epoch: 0.75536\n",
      "train loss in 3 epoch in 10800 batch: 0.69793\n",
      "val loss in 3 epoch: 0.74012\n",
      "train loss in 3 epoch in 11000 batch: 0.20962\n",
      "val loss in 3 epoch: 0.74620\n",
      "train loss in 3 epoch in 11200 batch: 0.36911\n",
      "val loss in 3 epoch: 0.73730\n",
      "train loss in 3 epoch in 11400 batch: 0.50676\n",
      "val loss in 3 epoch: 0.74649\n",
      "train loss in 3 epoch in 11600 batch: 0.97024\n",
      "val loss in 3 epoch: 0.74292\n",
      "train loss in 3 epoch in 11800 batch: 0.60259\n",
      "val loss in 3 epoch: 0.74370\n",
      "train loss in 3 epoch in 12000 batch: 0.96764\n",
      "val loss in 3 epoch: 0.74025\n",
      "train loss in 3 epoch in 12200 batch: 0.44635\n",
      "val loss in 3 epoch: 0.73957\n",
      "train loss in 3 epoch in 12400 batch: 1.40636\n",
      "val loss in 3 epoch: 0.75078\n",
      "train loss in 3 epoch in 12600 batch: 1.39257\n",
      "val loss in 3 epoch: 0.74427\n",
      "train loss in 3 epoch in 12800 batch: 1.03849\n",
      "val loss in 3 epoch: 0.74238\n",
      "train loss in 3 epoch in 13000 batch: 0.29799\n",
      "val loss in 3 epoch: 0.74365\n",
      "train loss in 3 epoch in 13200 batch: 1.39547\n",
      "val loss in 3 epoch: 0.74544\n",
      "train loss in 3 epoch in 13400 batch: 0.81034\n",
      "val loss in 3 epoch: 0.73787\n",
      "train loss in 3 epoch in 13600 batch: 0.29965\n",
      "val loss in 3 epoch: 0.74779\n",
      "train loss in 3 epoch in 13800 batch: 0.60516\n",
      "val loss in 3 epoch: 0.74081\n",
      "train loss in 3 epoch in 14000 batch: 0.87979\n",
      "val loss in 3 epoch: 0.74377\n",
      "train loss in 3 epoch in 14200 batch: 0.32340\n",
      "val loss in 3 epoch: 0.74203\n",
      "train loss in 3 epoch in 14400 batch: 0.89713\n",
      "val loss in 3 epoch: 0.74410\n",
      "train loss in 3 epoch in 14600 batch: 0.71631\n",
      "val loss in 3 epoch: 0.73521\n",
      "train loss in 3 epoch in 14800 batch: 0.98230\n",
      "val loss in 3 epoch: 0.74379\n",
      "train loss in 3 epoch in 15000 batch: 0.87845\n",
      "val loss in 3 epoch: 0.74376\n",
      "train loss in 3 epoch in 15200 batch: 0.74442\n",
      "val loss in 3 epoch: 0.74672\n",
      "train loss in 3 epoch in 15400 batch: 0.75734\n",
      "val loss in 3 epoch: 0.74516\n",
      "train loss in 3 epoch in 15600 batch: 0.56636\n",
      "val loss in 3 epoch: 0.74180\n",
      "train loss in 3 epoch in 15800 batch: 0.86262\n",
      "val loss in 3 epoch: 0.73934\n",
      "train loss in 3 epoch in 16000 batch: 1.38078\n",
      "val loss in 3 epoch: 0.74508\n",
      "train loss in 3 epoch in 16200 batch: 0.52306\n",
      "val loss in 3 epoch: 0.73620\n",
      "train loss in 4 epoch in 200 batch: 0.96301\n",
      "val loss in 4 epoch: 0.75421\n",
      "train loss in 4 epoch in 400 batch: 0.17857\n",
      "val loss in 4 epoch: 0.74387\n",
      "train loss in 4 epoch in 600 batch: 1.00523\n",
      "val loss in 4 epoch: 0.74145\n",
      "train loss in 4 epoch in 800 batch: 0.30702\n",
      "val loss in 4 epoch: 0.73983\n",
      "train loss in 4 epoch in 1000 batch: 0.43927\n",
      "val loss in 4 epoch: 0.74189\n",
      "train loss in 4 epoch in 1200 batch: 0.22347\n",
      "val loss in 4 epoch: 0.74034\n",
      "train loss in 4 epoch in 1400 batch: 0.69853\n",
      "val loss in 4 epoch: 0.74202\n",
      "train loss in 4 epoch in 1600 batch: 0.03741\n",
      "val loss in 4 epoch: 0.73983\n",
      "train loss in 4 epoch in 1800 batch: 0.94501\n",
      "val loss in 4 epoch: 0.74190\n",
      "train loss in 4 epoch in 2000 batch: 0.99115\n",
      "val loss in 4 epoch: 0.74129\n",
      "train loss in 4 epoch in 2200 batch: 0.03515\n",
      "val loss in 4 epoch: 0.74187\n",
      "train loss in 4 epoch in 2400 batch: 0.81675\n",
      "val loss in 4 epoch: 0.73837\n",
      "train loss in 4 epoch in 2600 batch: 0.72579\n",
      "val loss in 4 epoch: 0.74438\n",
      "train loss in 4 epoch in 2800 batch: 0.94052\n",
      "val loss in 4 epoch: 0.73861\n",
      "train loss in 4 epoch in 3000 batch: 0.83099\n",
      "val loss in 4 epoch: 0.73572\n",
      "train loss in 4 epoch in 3200 batch: 0.52517\n",
      "val loss in 4 epoch: 0.73790\n",
      "train loss in 4 epoch in 3400 batch: 0.87244\n",
      "val loss in 4 epoch: 0.73747\n",
      "train loss in 4 epoch in 3600 batch: 1.22718\n",
      "val loss in 4 epoch: 0.74144\n",
      "train loss in 4 epoch in 3800 batch: 0.51966\n",
      "val loss in 4 epoch: 0.74759\n",
      "train loss in 4 epoch in 4000 batch: 0.67872\n",
      "val loss in 4 epoch: 0.73763\n",
      "train loss in 4 epoch in 4200 batch: 0.31350\n",
      "val loss in 4 epoch: 0.73211\n",
      "train loss in 4 epoch in 4400 batch: 0.00607\n",
      "val loss in 4 epoch: 0.73974\n",
      "train loss in 4 epoch in 4600 batch: 0.38716\n",
      "val loss in 4 epoch: 0.74437\n",
      "train loss in 4 epoch in 4800 batch: 0.60516\n",
      "val loss in 4 epoch: 0.73624\n",
      "train loss in 4 epoch in 5000 batch: 0.72381\n",
      "val loss in 4 epoch: 0.73307\n",
      "train loss in 4 epoch in 5200 batch: 0.64075\n",
      "val loss in 4 epoch: 0.73273\n",
      "train loss in 4 epoch in 5400 batch: 0.68878\n",
      "val loss in 4 epoch: 0.73309\n",
      "train loss in 4 epoch in 5600 batch: 0.67929\n",
      "val loss in 4 epoch: 0.73209\n",
      "train loss in 4 epoch in 5800 batch: 0.37683\n",
      "val loss in 4 epoch: 0.72986\n",
      "train loss in 4 epoch in 6000 batch: 0.72566\n",
      "val loss in 4 epoch: 0.73164\n",
      "train loss in 4 epoch in 6200 batch: 0.39323\n",
      "val loss in 4 epoch: 0.73970\n",
      "train loss in 4 epoch in 6400 batch: 0.80366\n",
      "val loss in 4 epoch: 0.74515\n",
      "train loss in 4 epoch in 6600 batch: 0.58966\n",
      "val loss in 4 epoch: 0.74075\n",
      "train loss in 4 epoch in 6800 batch: 0.09309\n",
      "val loss in 4 epoch: 0.73894\n",
      "train loss in 4 epoch in 7000 batch: 0.72800\n",
      "val loss in 4 epoch: 0.74112\n",
      "train loss in 4 epoch in 7200 batch: 0.97297\n",
      "val loss in 4 epoch: 0.73683\n",
      "train loss in 4 epoch in 7400 batch: 0.47509\n",
      "val loss in 4 epoch: 0.73758\n",
      "train loss in 4 epoch in 7600 batch: 0.21242\n",
      "val loss in 4 epoch: 0.73358\n",
      "train loss in 4 epoch in 7800 batch: 0.72338\n",
      "val loss in 4 epoch: 0.73678\n",
      "train loss in 4 epoch in 8000 batch: 0.41418\n",
      "val loss in 4 epoch: 0.74951\n",
      "train loss in 4 epoch in 8200 batch: 1.11141\n",
      "val loss in 4 epoch: 0.73289\n",
      "train loss in 4 epoch in 8400 batch: 0.82622\n",
      "val loss in 4 epoch: 0.73932\n",
      "train loss in 4 epoch in 8600 batch: 1.29172\n",
      "val loss in 4 epoch: 0.73794\n",
      "train loss in 4 epoch in 8800 batch: 0.75570\n",
      "val loss in 4 epoch: 0.73986\n",
      "train loss in 4 epoch in 9000 batch: 0.38123\n",
      "val loss in 4 epoch: 0.74873\n",
      "train loss in 4 epoch in 9200 batch: 1.04416\n",
      "val loss in 4 epoch: 0.72887\n",
      "train loss in 4 epoch in 9400 batch: 1.11719\n",
      "val loss in 4 epoch: 0.73106\n",
      "train loss in 4 epoch in 9600 batch: 0.23674\n",
      "val loss in 4 epoch: 0.74485\n",
      "train loss in 4 epoch in 9800 batch: 0.28855\n",
      "val loss in 4 epoch: 0.73345\n",
      "train loss in 4 epoch in 10000 batch: 0.65292\n",
      "val loss in 4 epoch: 0.73030\n",
      "train loss in 4 epoch in 10200 batch: 0.39169\n",
      "val loss in 4 epoch: 0.73287\n",
      "train loss in 4 epoch in 10400 batch: 0.13858\n",
      "val loss in 4 epoch: 0.73931\n",
      "train loss in 4 epoch in 10600 batch: 0.28958\n",
      "val loss in 4 epoch: 0.73997\n",
      "train loss in 4 epoch in 10800 batch: 0.28876\n",
      "val loss in 4 epoch: 0.73916\n",
      "train loss in 4 epoch in 11000 batch: 0.59995\n",
      "val loss in 4 epoch: 0.74522\n",
      "train loss in 4 epoch in 11200 batch: 0.48368\n",
      "val loss in 4 epoch: 0.73947\n",
      "train loss in 4 epoch in 11400 batch: 1.12908\n",
      "val loss in 4 epoch: 0.74297\n",
      "train loss in 4 epoch in 11600 batch: 0.81124\n",
      "val loss in 4 epoch: 0.73046\n",
      "train loss in 4 epoch in 11800 batch: 0.86379\n",
      "val loss in 4 epoch: 0.73967\n",
      "train loss in 4 epoch in 12000 batch: 0.23354\n",
      "val loss in 4 epoch: 0.73528\n",
      "train loss in 4 epoch in 12200 batch: 0.54014\n",
      "val loss in 4 epoch: 0.75429\n",
      "train loss in 4 epoch in 12400 batch: 0.20568\n",
      "val loss in 4 epoch: 0.75297\n",
      "train loss in 4 epoch in 12600 batch: 0.95674\n",
      "val loss in 4 epoch: 0.72906\n",
      "train loss in 4 epoch in 12800 batch: 0.78043\n",
      "val loss in 4 epoch: 0.73280\n",
      "train loss in 4 epoch in 13000 batch: 1.44561\n",
      "val loss in 4 epoch: 0.74053\n",
      "train loss in 4 epoch in 13200 batch: 1.35915\n",
      "val loss in 4 epoch: 0.74517\n",
      "train loss in 4 epoch in 13400 batch: 1.21040\n",
      "val loss in 4 epoch: 0.73481\n",
      "train loss in 4 epoch in 13600 batch: 0.76207\n",
      "val loss in 4 epoch: 0.73062\n",
      "train loss in 4 epoch in 13800 batch: 0.66698\n",
      "val loss in 4 epoch: 0.73320\n",
      "train loss in 4 epoch in 14000 batch: 0.83368\n",
      "val loss in 4 epoch: 0.72971\n",
      "train loss in 4 epoch in 14200 batch: 1.25710\n",
      "val loss in 4 epoch: 0.72943\n",
      "train loss in 4 epoch in 14400 batch: 1.24602\n",
      "val loss in 4 epoch: 0.73446\n",
      "train loss in 4 epoch in 14600 batch: 0.78764\n",
      "val loss in 4 epoch: 0.72986\n",
      "train loss in 4 epoch in 14800 batch: 0.14826\n",
      "val loss in 4 epoch: 0.72832\n",
      "train loss in 4 epoch in 15000 batch: 0.34594\n",
      "val loss in 4 epoch: 0.74171\n",
      "train loss in 4 epoch in 15200 batch: 1.14600\n",
      "val loss in 4 epoch: 0.73573\n",
      "train loss in 4 epoch in 15400 batch: 0.59149\n",
      "val loss in 4 epoch: 0.73071\n",
      "train loss in 4 epoch in 15600 batch: 0.27278\n",
      "val loss in 4 epoch: 0.74497\n",
      "train loss in 4 epoch in 15800 batch: 0.38357\n",
      "val loss in 4 epoch: 0.73097\n",
      "train loss in 4 epoch in 16000 batch: 1.02075\n",
      "val loss in 4 epoch: 0.73074\n",
      "train loss in 4 epoch in 16200 batch: 0.66852\n",
      "val loss in 4 epoch: 0.73557\n"
     ]
    }
   ],
   "source": [
    "# v16\n",
    "INPUT_F = 36\n",
    "H = 256\n",
    "DROP_P = 0.1\n",
    "\n",
    "model = NNWithAttention(INPUT_F, DROP_P, H)\n",
    "\n",
    "def policy_gradient(logits_batch, rewards_batch, actions_batch, _):\n",
    "    loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    logits_batch = torch.reshape(logits_batch, (-1, 100))\n",
    "    policy = loss(logits_batch, actions_batch)\n",
    "    out = torch.mean(policy * rewards_batch)\n",
    "    return out\n",
    "\n",
    "learn(nagiss_top_10[0], nagiss_top_10[1], model,\n",
    "      freq=200, batch_size=16,lr=1e-4, epochs=4, criterion=policy_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "598.0 585.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "593.0 613.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "648.0 609.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "621.0 557.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "580.0 569.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "639.0 696.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "625.0 621.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "654.0 687.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "638.0 623.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "659.0 610.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4482659057750409,\n",
       " 8.5,\n",
       " 35.44643846707311,\n",
       " 0.7,\n",
       " 'tmp/b_0.8650894478131601.py',\n",
       " 'tmp/b_0.8547409200351491.py',\n",
       " array([598., 593., 648., 621., 580., 639., 625., 654., 638., 659.]),\n",
       " array([585., 613., 609., 557., 569., 696., 621., 687., 623., 610.]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v16\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,False\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=False\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "621.0 614.0 tmp/b_0.7746478479790496.py tmp/b_0.1629299312838185.py\n",
      "621.0 631.0 tmp/b_0.7746478479790496.py tmp/b_0.1629299312838185.py\n",
      "621.0 646.0 tmp/b_0.7746478479790496.py tmp/b_0.1629299312838185.py\n",
      "647.0 697.0 tmp/b_0.7746478479790496.py tmp/b_0.1629299312838185.py\n",
      "599.0 585.0 tmp/b_0.7746478479790496.py tmp/b_0.1629299312838185.py\n",
      "599.0 626.0 tmp/b_0.7746478479790496.py tmp/b_0.1629299312838185.py\n",
      "666.0 671.0 tmp/b_0.7746478479790496.py tmp/b_0.1629299312838185.py\n",
      "639.0 616.0 tmp/b_0.7746478479790496.py tmp/b_0.1629299312838185.py\n",
      "629.0 609.0 tmp/b_0.7746478479790496.py tmp/b_0.1629299312838185.py\n",
      "611.0 590.0 tmp/b_0.7746478479790496.py tmp/b_0.1629299312838185.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6658738488700195,\n",
       " -3.2,\n",
       " 23.434163095788165,\n",
       " 0.5,\n",
       " 'tmp/b_0.7746478479790496.py',\n",
       " 'tmp/b_0.1629299312838185.py',\n",
       " array([621., 621., 621., 647., 599., 599., 666., 639., 629., 611.]),\n",
       " array([614., 631., 646., 697., 585., 626., 671., 616., 609., 590.]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v16 vs neural with mean\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,False\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 200 batch: 1.37473\n",
      "val loss in 1 epoch: 0.95067\n",
      "train loss in 1 epoch in 400 batch: 0.39388\n",
      "val loss in 1 epoch: 0.91126\n",
      "train loss in 1 epoch in 600 batch: 0.63603\n",
      "val loss in 1 epoch: 0.88273\n",
      "train loss in 1 epoch in 800 batch: 0.60204\n",
      "val loss in 1 epoch: 0.87285\n",
      "train loss in 1 epoch in 1000 batch: 0.09691\n",
      "val loss in 1 epoch: 0.87556\n",
      "train loss in 1 epoch in 1200 batch: 0.64043\n",
      "val loss in 1 epoch: 0.85667\n",
      "train loss in 1 epoch in 1400 batch: 1.10523\n",
      "val loss in 1 epoch: 0.84905\n",
      "train loss in 1 epoch in 1600 batch: 0.17899\n",
      "val loss in 1 epoch: 0.84598\n",
      "train loss in 1 epoch in 1800 batch: 0.29390\n",
      "val loss in 1 epoch: 0.84049\n",
      "train loss in 1 epoch in 2000 batch: 0.40744\n",
      "val loss in 1 epoch: 0.84137\n",
      "train loss in 1 epoch in 2200 batch: 1.00212\n",
      "val loss in 1 epoch: 0.83570\n",
      "train loss in 1 epoch in 2400 batch: 0.32799\n",
      "val loss in 1 epoch: 0.83877\n",
      "train loss in 1 epoch in 2600 batch: 1.40992\n",
      "val loss in 1 epoch: 0.83460\n",
      "train loss in 1 epoch in 2800 batch: 1.72974\n",
      "val loss in 1 epoch: 0.82925\n",
      "train loss in 1 epoch in 3000 batch: 0.31842\n",
      "val loss in 1 epoch: 0.82671\n",
      "train loss in 1 epoch in 3200 batch: 0.27932\n",
      "val loss in 1 epoch: 0.82859\n",
      "train loss in 1 epoch in 3400 batch: 0.70605\n",
      "val loss in 1 epoch: 0.83239\n",
      "train loss in 1 epoch in 3600 batch: 1.24923\n",
      "val loss in 1 epoch: 0.82057\n",
      "train loss in 1 epoch in 3800 batch: 0.96482\n",
      "val loss in 1 epoch: 0.82179\n",
      "train loss in 1 epoch in 4000 batch: 0.90899\n",
      "val loss in 1 epoch: 0.82877\n",
      "train loss in 1 epoch in 4200 batch: 0.55466\n",
      "val loss in 1 epoch: 0.82827\n",
      "train loss in 1 epoch in 4400 batch: 1.33737\n",
      "val loss in 1 epoch: 0.82399\n",
      "train loss in 1 epoch in 4600 batch: 2.07560\n",
      "val loss in 1 epoch: 0.81994\n",
      "train loss in 1 epoch in 4800 batch: 0.35979\n",
      "val loss in 1 epoch: 0.81361\n",
      "train loss in 1 epoch in 5000 batch: 0.67728\n",
      "val loss in 1 epoch: 0.81694\n",
      "train loss in 1 epoch in 5200 batch: 0.97669\n",
      "val loss in 1 epoch: 0.81641\n",
      "train loss in 1 epoch in 5400 batch: 0.77027\n",
      "val loss in 1 epoch: 0.81502\n",
      "train loss in 1 epoch in 5600 batch: 1.71762\n",
      "val loss in 1 epoch: 0.80908\n",
      "train loss in 1 epoch in 5800 batch: 0.61258\n",
      "val loss in 1 epoch: 0.81097\n",
      "train loss in 1 epoch in 6000 batch: 0.43693\n",
      "val loss in 1 epoch: 0.81495\n",
      "train loss in 1 epoch in 6200 batch: 0.81453\n",
      "val loss in 1 epoch: 0.81265\n",
      "train loss in 1 epoch in 6400 batch: 1.13293\n",
      "val loss in 1 epoch: 0.80907\n",
      "train loss in 1 epoch in 6600 batch: 1.21762\n",
      "val loss in 1 epoch: 0.80595\n",
      "train loss in 1 epoch in 6800 batch: 1.60654\n",
      "val loss in 1 epoch: 0.80568\n",
      "train loss in 1 epoch in 7000 batch: 0.62907\n",
      "val loss in 1 epoch: 0.81329\n",
      "train loss in 1 epoch in 7200 batch: 0.18968\n",
      "val loss in 1 epoch: 0.80989\n",
      "train loss in 1 epoch in 7400 batch: 0.80842\n",
      "val loss in 1 epoch: 0.80545\n",
      "train loss in 1 epoch in 7600 batch: 0.73362\n",
      "val loss in 1 epoch: 0.80883\n",
      "train loss in 1 epoch in 7800 batch: 0.72012\n",
      "val loss in 1 epoch: 0.81026\n",
      "train loss in 1 epoch in 8000 batch: 0.67040\n",
      "val loss in 1 epoch: 0.81069\n",
      "train loss in 1 epoch in 8200 batch: 0.55920\n",
      "val loss in 1 epoch: 0.81907\n",
      "train loss in 1 epoch in 8400 batch: 0.66882\n",
      "val loss in 1 epoch: 0.81855\n",
      "train loss in 1 epoch in 8600 batch: 0.99115\n",
      "val loss in 1 epoch: 0.80655\n",
      "train loss in 1 epoch in 8800 batch: 0.61172\n",
      "val loss in 1 epoch: 0.80796\n",
      "train loss in 1 epoch in 9000 batch: 1.64097\n",
      "val loss in 1 epoch: 0.80456\n",
      "train loss in 1 epoch in 9200 batch: 0.77273\n",
      "val loss in 1 epoch: 0.80807\n",
      "train loss in 1 epoch in 9400 batch: 1.01590\n",
      "val loss in 1 epoch: 0.80509\n",
      "train loss in 1 epoch in 9600 batch: 1.02742\n",
      "val loss in 1 epoch: 0.80066\n",
      "train loss in 1 epoch in 9800 batch: 0.50755\n",
      "val loss in 1 epoch: 0.80779\n",
      "train loss in 1 epoch in 10000 batch: 0.50139\n",
      "val loss in 1 epoch: 0.79501\n",
      "train loss in 1 epoch in 10200 batch: 1.04405\n",
      "val loss in 1 epoch: 0.81061\n",
      "train loss in 1 epoch in 10400 batch: 0.71079\n",
      "val loss in 1 epoch: 0.80239\n",
      "train loss in 1 epoch in 10600 batch: 0.52814\n",
      "val loss in 1 epoch: 0.79663\n",
      "train loss in 1 epoch in 10800 batch: 0.71474\n",
      "val loss in 1 epoch: 0.79738\n",
      "train loss in 1 epoch in 11000 batch: 1.54163\n",
      "val loss in 1 epoch: 0.80321\n",
      "train loss in 1 epoch in 11200 batch: 0.37980\n",
      "val loss in 1 epoch: 0.79495\n",
      "train loss in 1 epoch in 11400 batch: 0.47856\n",
      "val loss in 1 epoch: 0.79534\n",
      "train loss in 1 epoch in 11600 batch: 0.74899\n",
      "val loss in 1 epoch: 0.79473\n",
      "train loss in 1 epoch in 11800 batch: 1.64911\n",
      "val loss in 1 epoch: 0.79733\n",
      "train loss in 1 epoch in 12000 batch: 0.44719\n",
      "val loss in 1 epoch: 0.79291\n",
      "train loss in 1 epoch in 12200 batch: 0.40353\n",
      "val loss in 1 epoch: 0.79240\n",
      "train loss in 1 epoch in 12400 batch: 0.95012\n",
      "val loss in 1 epoch: 0.79545\n",
      "train loss in 1 epoch in 12600 batch: 0.77245\n",
      "val loss in 1 epoch: 0.78778\n",
      "train loss in 1 epoch in 12800 batch: 0.11972\n",
      "val loss in 1 epoch: 0.78797\n",
      "train loss in 1 epoch in 13000 batch: 0.76363\n",
      "val loss in 1 epoch: 0.78997\n",
      "train loss in 1 epoch in 13200 batch: 1.08940\n",
      "val loss in 1 epoch: 0.79478\n",
      "train loss in 1 epoch in 13400 batch: 0.69851\n",
      "val loss in 1 epoch: 0.79126\n",
      "train loss in 1 epoch in 13600 batch: 0.00000\n",
      "val loss in 1 epoch: 0.78671\n",
      "train loss in 1 epoch in 13800 batch: 1.02737\n",
      "val loss in 1 epoch: 0.79188\n",
      "train loss in 1 epoch in 14000 batch: 0.64447\n",
      "val loss in 1 epoch: 0.78796\n",
      "train loss in 1 epoch in 14200 batch: 0.74750\n",
      "val loss in 1 epoch: 0.78090\n",
      "train loss in 1 epoch in 14400 batch: 0.49486\n",
      "val loss in 1 epoch: 0.77869\n",
      "train loss in 1 epoch in 14600 batch: 1.20736\n",
      "val loss in 1 epoch: 0.78292\n",
      "train loss in 1 epoch in 14800 batch: 0.56777\n",
      "val loss in 1 epoch: 0.77917\n",
      "train loss in 1 epoch in 15000 batch: 0.80427\n",
      "val loss in 1 epoch: 0.79286\n",
      "train loss in 1 epoch in 15200 batch: 1.06836\n",
      "val loss in 1 epoch: 0.78301\n",
      "train loss in 1 epoch in 15400 batch: 1.36279\n",
      "val loss in 1 epoch: 0.78065\n",
      "train loss in 1 epoch in 15600 batch: 0.85948\n",
      "val loss in 1 epoch: 0.78122\n",
      "train loss in 1 epoch in 15800 batch: 1.12167\n",
      "val loss in 1 epoch: 0.77826\n",
      "train loss in 1 epoch in 16000 batch: 0.80580\n",
      "val loss in 1 epoch: 0.77629\n",
      "train loss in 1 epoch in 16200 batch: 0.33761\n",
      "val loss in 1 epoch: 0.77485\n",
      "train loss in 2 epoch in 200 batch: 0.84643\n",
      "val loss in 2 epoch: 0.81920\n",
      "train loss in 2 epoch in 400 batch: 0.43044\n",
      "val loss in 2 epoch: 0.77645\n",
      "train loss in 2 epoch in 600 batch: 0.28954\n",
      "val loss in 2 epoch: 0.77549\n",
      "train loss in 2 epoch in 800 batch: 0.57284\n",
      "val loss in 2 epoch: 0.77506\n",
      "train loss in 2 epoch in 1000 batch: 0.91120\n",
      "val loss in 2 epoch: 0.77202\n",
      "train loss in 2 epoch in 1200 batch: 0.04074\n",
      "val loss in 2 epoch: 0.77398\n",
      "train loss in 2 epoch in 1400 batch: 1.10799\n",
      "val loss in 2 epoch: 0.76829\n",
      "train loss in 2 epoch in 1600 batch: 1.62301\n",
      "val loss in 2 epoch: 0.77120\n",
      "train loss in 2 epoch in 1800 batch: 1.09923\n",
      "val loss in 2 epoch: 0.77231\n",
      "train loss in 2 epoch in 2000 batch: 0.28223\n",
      "val loss in 2 epoch: 0.77089\n",
      "train loss in 2 epoch in 2200 batch: 0.22918\n",
      "val loss in 2 epoch: 0.76547\n",
      "train loss in 2 epoch in 2400 batch: 0.98935\n",
      "val loss in 2 epoch: 0.76782\n",
      "train loss in 2 epoch in 2600 batch: 0.44431\n",
      "val loss in 2 epoch: 0.76462\n",
      "train loss in 2 epoch in 2800 batch: 0.88015\n",
      "val loss in 2 epoch: 0.75697\n",
      "train loss in 2 epoch in 3000 batch: 0.75851\n",
      "val loss in 2 epoch: 0.77885\n",
      "train loss in 2 epoch in 3200 batch: 0.88494\n",
      "val loss in 2 epoch: 0.75995\n",
      "train loss in 2 epoch in 3400 batch: 0.64439\n",
      "val loss in 2 epoch: 0.76422\n",
      "train loss in 2 epoch in 3600 batch: 0.59964\n",
      "val loss in 2 epoch: 0.75784\n",
      "train loss in 2 epoch in 3800 batch: 0.50003\n",
      "val loss in 2 epoch: 0.76503\n",
      "train loss in 2 epoch in 4000 batch: 0.71045\n",
      "val loss in 2 epoch: 0.75958\n",
      "train loss in 2 epoch in 4200 batch: 0.79803\n",
      "val loss in 2 epoch: 0.75768\n",
      "train loss in 2 epoch in 4400 batch: 1.14755\n",
      "val loss in 2 epoch: 0.76101\n",
      "train loss in 2 epoch in 4600 batch: 1.50483\n",
      "val loss in 2 epoch: 0.76542\n",
      "train loss in 2 epoch in 4800 batch: 0.19463\n",
      "val loss in 2 epoch: 0.76452\n",
      "train loss in 2 epoch in 5000 batch: 0.86811\n",
      "val loss in 2 epoch: 0.76000\n",
      "train loss in 2 epoch in 5200 batch: 1.26617\n",
      "val loss in 2 epoch: 0.75967\n",
      "train loss in 2 epoch in 5400 batch: 0.80796\n",
      "val loss in 2 epoch: 0.75863\n",
      "train loss in 2 epoch in 5600 batch: 1.05491\n",
      "val loss in 2 epoch: 0.76335\n",
      "train loss in 2 epoch in 5800 batch: 1.42986\n",
      "val loss in 2 epoch: 0.75189\n",
      "train loss in 2 epoch in 6000 batch: 0.70966\n",
      "val loss in 2 epoch: 0.75840\n",
      "train loss in 2 epoch in 6200 batch: 0.93298\n",
      "val loss in 2 epoch: 0.77034\n",
      "train loss in 2 epoch in 6400 batch: 0.62118\n",
      "val loss in 2 epoch: 0.75025\n",
      "train loss in 2 epoch in 6600 batch: 0.91205\n",
      "val loss in 2 epoch: 0.76560\n",
      "train loss in 2 epoch in 6800 batch: 1.06809\n",
      "val loss in 2 epoch: 0.74993\n",
      "train loss in 2 epoch in 7000 batch: 0.86790\n",
      "val loss in 2 epoch: 0.76260\n",
      "train loss in 2 epoch in 7200 batch: 0.81920\n",
      "val loss in 2 epoch: 0.75627\n",
      "train loss in 2 epoch in 7400 batch: 0.77192\n",
      "val loss in 2 epoch: 0.75577\n",
      "train loss in 2 epoch in 7600 batch: 1.46442\n",
      "val loss in 2 epoch: 0.76020\n",
      "train loss in 2 epoch in 7800 batch: 0.83865\n",
      "val loss in 2 epoch: 0.76317\n",
      "train loss in 2 epoch in 8000 batch: 0.68274\n",
      "val loss in 2 epoch: 0.75913\n",
      "train loss in 2 epoch in 8200 batch: 0.90925\n",
      "val loss in 2 epoch: 0.77170\n",
      "train loss in 2 epoch in 8400 batch: 0.03263\n",
      "val loss in 2 epoch: 0.76263\n",
      "train loss in 2 epoch in 8600 batch: 1.13957\n",
      "val loss in 2 epoch: 0.75297\n",
      "train loss in 2 epoch in 8800 batch: 0.57073\n",
      "val loss in 2 epoch: 0.76170\n",
      "train loss in 2 epoch in 9000 batch: 0.61597\n",
      "val loss in 2 epoch: 0.75174\n",
      "train loss in 2 epoch in 9200 batch: 0.78615\n",
      "val loss in 2 epoch: 0.74401\n",
      "train loss in 2 epoch in 9400 batch: 0.18948\n",
      "val loss in 2 epoch: 0.75261\n",
      "train loss in 2 epoch in 9600 batch: 0.37216\n",
      "val loss in 2 epoch: 0.74554\n",
      "train loss in 2 epoch in 9800 batch: 0.29239\n",
      "val loss in 2 epoch: 0.75481\n",
      "train loss in 2 epoch in 10000 batch: 0.11223\n",
      "val loss in 2 epoch: 0.75282\n",
      "train loss in 2 epoch in 10200 batch: 0.74356\n",
      "val loss in 2 epoch: 0.76690\n",
      "train loss in 2 epoch in 10400 batch: 0.93794\n",
      "val loss in 2 epoch: 0.75398\n",
      "train loss in 2 epoch in 10600 batch: 0.75193\n",
      "val loss in 2 epoch: 0.74581\n",
      "train loss in 2 epoch in 10800 batch: 1.43852\n",
      "val loss in 2 epoch: 0.75891\n",
      "train loss in 2 epoch in 11000 batch: 1.57105\n",
      "val loss in 2 epoch: 0.74614\n",
      "train loss in 2 epoch in 11200 batch: 0.87084\n",
      "val loss in 2 epoch: 0.75254\n",
      "train loss in 2 epoch in 11400 batch: 1.13759\n",
      "val loss in 2 epoch: 0.74926\n",
      "train loss in 2 epoch in 11600 batch: 0.62695\n",
      "val loss in 2 epoch: 0.74777\n",
      "train loss in 2 epoch in 11800 batch: 0.78860\n",
      "val loss in 2 epoch: 0.74913\n",
      "train loss in 2 epoch in 12000 batch: 0.48142\n",
      "val loss in 2 epoch: 0.74568\n",
      "train loss in 2 epoch in 12200 batch: 0.26632\n",
      "val loss in 2 epoch: 0.74316\n",
      "train loss in 2 epoch in 12400 batch: 0.57699\n",
      "val loss in 2 epoch: 0.74563\n",
      "train loss in 2 epoch in 12600 batch: 1.62271\n",
      "val loss in 2 epoch: 0.74930\n",
      "train loss in 2 epoch in 12800 batch: 0.60893\n",
      "val loss in 2 epoch: 0.75907\n",
      "train loss in 2 epoch in 13000 batch: 1.26748\n",
      "val loss in 2 epoch: 0.74203\n",
      "train loss in 2 epoch in 13200 batch: 0.33562\n",
      "val loss in 2 epoch: 0.74163\n",
      "train loss in 2 epoch in 13400 batch: 0.92013\n",
      "val loss in 2 epoch: 0.74342\n",
      "train loss in 2 epoch in 13600 batch: 0.53552\n",
      "val loss in 2 epoch: 0.75191\n",
      "train loss in 2 epoch in 13800 batch: 0.37401\n",
      "val loss in 2 epoch: 0.74681\n",
      "train loss in 2 epoch in 14000 batch: 0.68416\n",
      "val loss in 2 epoch: 0.74624\n",
      "train loss in 2 epoch in 14200 batch: 0.68597\n",
      "val loss in 2 epoch: 0.74467\n",
      "train loss in 2 epoch in 14400 batch: 0.29253\n",
      "val loss in 2 epoch: 0.74284\n",
      "train loss in 2 epoch in 14600 batch: 0.59122\n",
      "val loss in 2 epoch: 0.74698\n",
      "train loss in 2 epoch in 14800 batch: 0.85287\n",
      "val loss in 2 epoch: 0.73823\n",
      "train loss in 2 epoch in 15000 batch: 1.04986\n",
      "val loss in 2 epoch: 0.75247\n",
      "train loss in 2 epoch in 15200 batch: 0.75235\n",
      "val loss in 2 epoch: 0.74183\n",
      "train loss in 2 epoch in 15400 batch: 0.21376\n",
      "val loss in 2 epoch: 0.73764\n",
      "train loss in 2 epoch in 15600 batch: 1.07569\n",
      "val loss in 2 epoch: 0.75217\n",
      "train loss in 2 epoch in 15800 batch: 0.51241\n",
      "val loss in 2 epoch: 0.73974\n",
      "train loss in 2 epoch in 16000 batch: 0.21020\n",
      "val loss in 2 epoch: 0.75160\n",
      "train loss in 2 epoch in 16200 batch: 0.31164\n",
      "val loss in 2 epoch: 0.74632\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-add36bf831df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m learn(nagiss_top_10[0], nagiss_top_10[1], model,\n\u001b[0m\u001b[1;32m     16\u001b[0m       freq=200, batch_size=16,lr=1e-4, epochs=4, criterion=policy_gradient)\n",
      "\u001b[0;32m<ipython-input-3-2d7d1938b15a>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(train, val, model_ff, epochs, batch_size, shuffle, freq, lr, criterion)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids_nn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mX_train_shuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mids_nn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0my_train_shuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mids_nn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0ma_train_shuffled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mids_nn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# v17\n",
    "INPUT_F = 36\n",
    "H = 256\n",
    "DROP_P = 0.5\n",
    "\n",
    "model = NNWithAttention(INPUT_F, DROP_P, H)\n",
    "\n",
    "def policy_gradient(logits_batch, rewards_batch, actions_batch, _):\n",
    "    loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    logits_batch = torch.reshape(logits_batch, (-1, 100))\n",
    "    policy = loss(logits_batch, actions_batch)\n",
    "    out = torch.mean(policy * rewards_batch)\n",
    "    return out\n",
    "\n",
    "learn(nagiss_top_10[0], nagiss_top_10[1], model,\n",
    "      freq=200, batch_size=16,lr=1e-4, epochs=2, criterion=policy_gradient)  # stoped manualy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v17')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "672.0 635.0 tmp/b_0.9715554847378113.py tmp/b_0.7203789564181606.py\n",
      "666.0 633.0 tmp/b_0.9715554847378113.py tmp/b_0.7203789564181606.py\n",
      "698.0 668.0 tmp/b_0.9715554847378113.py tmp/b_0.7203789564181606.py\n",
      "679.0 653.0 tmp/b_0.9715554847378113.py tmp/b_0.7203789564181606.py\n",
      "681.0 648.0 tmp/b_0.9715554847378113.py tmp/b_0.7203789564181606.py\n",
      "645.0 627.0 tmp/b_0.9715554847378113.py tmp/b_0.7203789564181606.py\n",
      "733.0 699.0 tmp/b_0.9715554847378113.py tmp/b_0.7203789564181606.py\n",
      "601.0 605.0 tmp/b_0.9715554847378113.py tmp/b_0.7203789564181606.py\n",
      "623.0 638.0 tmp/b_0.9715554847378113.py tmp/b_0.7203789564181606.py\n",
      "658.0 616.0 tmp/b_0.9715554847378113.py tmp/b_0.7203789564181606.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2.9026572458509727e-05,\n",
       " 23.4,\n",
       " 17.69858751426226,\n",
       " 0.8,\n",
       " 'tmp/b_0.9715554847378113.py',\n",
       " 'tmp/b_0.7203789564181606.py',\n",
       " array([672., 666., 698., 679., 681., 645., 733., 601., 623., 658.]),\n",
       " array([635., 633., 668., 653., 648., 627., 699., 605., 638., 616.]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v17\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,False\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=False\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "737.0 637.0 tmp/b_0.7786155387529811.py tmp/b_0.9054717873967233.py\n",
      "625.0 579.0 tmp/b_0.7786155387529811.py tmp/b_0.9054717873967233.py\n",
      "592.0 616.0 tmp/b_0.7786155387529811.py tmp/b_0.9054717873967233.py\n",
      "598.0 562.0 tmp/b_0.7786155387529811.py tmp/b_0.9054717873967233.py\n",
      "651.0 640.0 tmp/b_0.7786155387529811.py tmp/b_0.9054717873967233.py\n",
      "580.0 582.0 tmp/b_0.7786155387529811.py tmp/b_0.9054717873967233.py\n",
      "603.0 601.0 tmp/b_0.7786155387529811.py tmp/b_0.9054717873967233.py\n",
      "642.0 593.0 tmp/b_0.7786155387529811.py tmp/b_0.9054717873967233.py\n",
      "673.0 686.0 tmp/b_0.7786155387529811.py tmp/b_0.9054717873967233.py\n",
      "591.0 582.0 tmp/b_0.7786155387529811.py tmp/b_0.9054717873967233.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.05257606029925348,\n",
       " 21.4,\n",
       " 34.911889092399456,\n",
       " 0.7,\n",
       " 'tmp/b_0.7786155387529811.py',\n",
       " 'tmp/b_0.9054717873967233.py',\n",
       " array([737., 625., 592., 598., 651., 580., 603., 642., 673., 591.]),\n",
       " array([637., 579., 616., 562., 640., 582., 601., 593., 686., 582.]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v17 vs neural with mean\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,False\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 200 batch: 1.18956\n",
      "val loss in 1 epoch: 0.95670\n",
      "train loss in 1 epoch in 400 batch: 0.37571\n",
      "val loss in 1 epoch: 0.90069\n",
      "train loss in 1 epoch in 600 batch: 0.63046\n",
      "val loss in 1 epoch: 0.89248\n",
      "train loss in 1 epoch in 800 batch: 0.56747\n",
      "val loss in 1 epoch: 0.87396\n",
      "train loss in 1 epoch in 1000 batch: 0.05146\n",
      "val loss in 1 epoch: 0.87221\n",
      "train loss in 1 epoch in 1200 batch: 0.64635\n",
      "val loss in 1 epoch: 0.85244\n",
      "train loss in 1 epoch in 1400 batch: 1.05073\n",
      "val loss in 1 epoch: 0.84647\n",
      "train loss in 1 epoch in 1600 batch: 0.14167\n",
      "val loss in 1 epoch: 0.84404\n",
      "train loss in 1 epoch in 1800 batch: 0.27423\n",
      "val loss in 1 epoch: 0.83871\n",
      "train loss in 1 epoch in 2000 batch: 0.42207\n",
      "val loss in 1 epoch: 0.83573\n",
      "train loss in 1 epoch in 2200 batch: 1.13866\n",
      "val loss in 1 epoch: 0.83807\n",
      "train loss in 1 epoch in 2400 batch: 0.34878\n",
      "val loss in 1 epoch: 0.83783\n",
      "train loss in 1 epoch in 2600 batch: 1.37678\n",
      "val loss in 1 epoch: 0.84028\n",
      "train loss in 1 epoch in 2800 batch: 1.48652\n",
      "val loss in 1 epoch: 0.83592\n",
      "train loss in 1 epoch in 3000 batch: 0.29396\n",
      "val loss in 1 epoch: 0.82654\n",
      "train loss in 1 epoch in 3200 batch: 0.29202\n",
      "val loss in 1 epoch: 0.82766\n",
      "train loss in 1 epoch in 3400 batch: 0.64006\n",
      "val loss in 1 epoch: 0.82285\n",
      "train loss in 1 epoch in 3600 batch: 1.27213\n",
      "val loss in 1 epoch: 0.82345\n",
      "train loss in 1 epoch in 3800 batch: 0.75895\n",
      "val loss in 1 epoch: 0.82014\n",
      "train loss in 1 epoch in 4000 batch: 0.91072\n",
      "val loss in 1 epoch: 0.83355\n",
      "train loss in 1 epoch in 4200 batch: 0.56546\n",
      "val loss in 1 epoch: 0.82748\n",
      "train loss in 1 epoch in 4400 batch: 1.33965\n",
      "val loss in 1 epoch: 0.82083\n",
      "train loss in 1 epoch in 4600 batch: 2.00470\n",
      "val loss in 1 epoch: 0.81931\n",
      "train loss in 1 epoch in 4800 batch: 0.32178\n",
      "val loss in 1 epoch: 0.81387\n",
      "train loss in 1 epoch in 5000 batch: 0.67795\n",
      "val loss in 1 epoch: 0.81862\n",
      "train loss in 1 epoch in 5200 batch: 0.97730\n",
      "val loss in 1 epoch: 0.81885\n",
      "train loss in 1 epoch in 5400 batch: 0.82448\n",
      "val loss in 1 epoch: 0.81257\n",
      "train loss in 1 epoch in 5600 batch: 1.91663\n",
      "val loss in 1 epoch: 0.81163\n",
      "train loss in 1 epoch in 5800 batch: 0.62309\n",
      "val loss in 1 epoch: 0.81179\n",
      "train loss in 1 epoch in 6000 batch: 0.40710\n",
      "val loss in 1 epoch: 0.80773\n",
      "train loss in 1 epoch in 6200 batch: 0.86705\n",
      "val loss in 1 epoch: 0.81237\n",
      "train loss in 1 epoch in 6400 batch: 1.17223\n",
      "val loss in 1 epoch: 0.81111\n",
      "train loss in 1 epoch in 6600 batch: 1.19344\n",
      "val loss in 1 epoch: 0.80407\n",
      "train loss in 1 epoch in 6800 batch: 1.63946\n",
      "val loss in 1 epoch: 0.80030\n",
      "train loss in 1 epoch in 7000 batch: 0.63770\n",
      "val loss in 1 epoch: 0.80458\n",
      "train loss in 1 epoch in 7200 batch: 0.25110\n",
      "val loss in 1 epoch: 0.81713\n",
      "train loss in 1 epoch in 7400 batch: 0.80932\n",
      "val loss in 1 epoch: 0.80383\n",
      "train loss in 1 epoch in 7600 batch: 0.77628\n",
      "val loss in 1 epoch: 0.80995\n",
      "train loss in 1 epoch in 7800 batch: 0.67777\n",
      "val loss in 1 epoch: 0.80279\n",
      "train loss in 1 epoch in 8000 batch: 0.67352\n",
      "val loss in 1 epoch: 0.80327\n",
      "train loss in 1 epoch in 8200 batch: 0.56043\n",
      "val loss in 1 epoch: 0.81260\n",
      "train loss in 1 epoch in 8400 batch: 0.69347\n",
      "val loss in 1 epoch: 0.82690\n",
      "train loss in 1 epoch in 8600 batch: 1.01891\n",
      "val loss in 1 epoch: 0.80061\n",
      "train loss in 1 epoch in 8800 batch: 0.56933\n",
      "val loss in 1 epoch: 0.80522\n",
      "train loss in 1 epoch in 9000 batch: 1.65109\n",
      "val loss in 1 epoch: 0.81776\n",
      "train loss in 1 epoch in 9200 batch: 0.71579\n",
      "val loss in 1 epoch: 0.80637\n",
      "train loss in 1 epoch in 9400 batch: 1.02540\n",
      "val loss in 1 epoch: 0.79962\n",
      "train loss in 1 epoch in 9600 batch: 1.08341\n",
      "val loss in 1 epoch: 0.79778\n",
      "train loss in 1 epoch in 9800 batch: 0.46503\n",
      "val loss in 1 epoch: 0.79692\n",
      "train loss in 1 epoch in 10000 batch: 0.44456\n",
      "val loss in 1 epoch: 0.79329\n",
      "train loss in 1 epoch in 10200 batch: 1.06508\n",
      "val loss in 1 epoch: 0.80340\n",
      "train loss in 1 epoch in 10400 batch: 0.76466\n",
      "val loss in 1 epoch: 0.79416\n",
      "train loss in 1 epoch in 10600 batch: 0.50569\n",
      "val loss in 1 epoch: 0.79739\n",
      "train loss in 1 epoch in 10800 batch: 0.81959\n",
      "val loss in 1 epoch: 0.79705\n",
      "train loss in 1 epoch in 11000 batch: 1.64425\n",
      "val loss in 1 epoch: 0.80296\n",
      "train loss in 1 epoch in 11200 batch: 0.37423\n",
      "val loss in 1 epoch: 0.80210\n",
      "train loss in 1 epoch in 11400 batch: 0.53827\n",
      "val loss in 1 epoch: 0.79169\n",
      "train loss in 1 epoch in 11600 batch: 0.85056\n",
      "val loss in 1 epoch: 0.79432\n",
      "train loss in 1 epoch in 11800 batch: 1.52792\n",
      "val loss in 1 epoch: 0.78830\n",
      "train loss in 1 epoch in 12000 batch: 0.46094\n",
      "val loss in 1 epoch: 0.79655\n",
      "train loss in 1 epoch in 12200 batch: 0.48189\n",
      "val loss in 1 epoch: 0.78638\n",
      "train loss in 1 epoch in 12400 batch: 0.83296\n",
      "val loss in 1 epoch: 0.79424\n",
      "train loss in 1 epoch in 12600 batch: 0.75801\n",
      "val loss in 1 epoch: 0.79455\n",
      "train loss in 1 epoch in 12800 batch: 0.11097\n",
      "val loss in 1 epoch: 0.78420\n",
      "train loss in 1 epoch in 13000 batch: 0.81350\n",
      "val loss in 1 epoch: 0.79887\n",
      "train loss in 1 epoch in 13200 batch: 1.07905\n",
      "val loss in 1 epoch: 0.79474\n",
      "train loss in 1 epoch in 13400 batch: 0.83024\n",
      "val loss in 1 epoch: 0.79493\n",
      "train loss in 1 epoch in 13600 batch: 0.00000\n",
      "val loss in 1 epoch: 0.79089\n",
      "train loss in 1 epoch in 13800 batch: 1.15669\n",
      "val loss in 1 epoch: 0.79318\n",
      "train loss in 1 epoch in 14000 batch: 0.86732\n",
      "val loss in 1 epoch: 0.78196\n",
      "train loss in 1 epoch in 14200 batch: 0.75105\n",
      "val loss in 1 epoch: 0.78887\n",
      "train loss in 1 epoch in 14400 batch: 0.51355\n",
      "val loss in 1 epoch: 0.78505\n",
      "train loss in 1 epoch in 14600 batch: 1.17609\n",
      "val loss in 1 epoch: 0.79767\n",
      "train loss in 1 epoch in 14800 batch: 0.56503\n",
      "val loss in 1 epoch: 0.78483\n",
      "train loss in 1 epoch in 15000 batch: 0.71016\n",
      "val loss in 1 epoch: 0.78674\n",
      "train loss in 1 epoch in 15200 batch: 1.14154\n",
      "val loss in 1 epoch: 0.79201\n",
      "train loss in 1 epoch in 15400 batch: 1.42195\n",
      "val loss in 1 epoch: 0.78540\n",
      "train loss in 1 epoch in 15600 batch: 0.75592\n",
      "val loss in 1 epoch: 0.80328\n",
      "train loss in 1 epoch in 15800 batch: 0.95187\n",
      "val loss in 1 epoch: 0.78990\n",
      "train loss in 1 epoch in 16000 batch: 0.80417\n",
      "val loss in 1 epoch: 0.79099\n",
      "train loss in 1 epoch in 16200 batch: 0.39669\n",
      "val loss in 1 epoch: 0.78248\n",
      "train loss in 2 epoch in 200 batch: 1.00048\n",
      "val loss in 2 epoch: 0.83103\n",
      "train loss in 2 epoch in 400 batch: 0.55026\n",
      "val loss in 2 epoch: 0.79187\n",
      "train loss in 2 epoch in 600 batch: 0.37640\n",
      "val loss in 2 epoch: 0.78448\n",
      "train loss in 2 epoch in 800 batch: 0.59134\n",
      "val loss in 2 epoch: 0.78846\n",
      "train loss in 2 epoch in 1000 batch: 0.87674\n",
      "val loss in 2 epoch: 0.80011\n",
      "train loss in 2 epoch in 1200 batch: 0.04338\n",
      "val loss in 2 epoch: 0.77989\n",
      "train loss in 2 epoch in 1400 batch: 1.10184\n",
      "val loss in 2 epoch: 0.78004\n",
      "train loss in 2 epoch in 1600 batch: 1.72443\n",
      "val loss in 2 epoch: 0.78824\n",
      "train loss in 2 epoch in 1800 batch: 1.16149\n",
      "val loss in 2 epoch: 0.78452\n",
      "train loss in 2 epoch in 2000 batch: 0.43907\n",
      "val loss in 2 epoch: 0.78642\n",
      "train loss in 2 epoch in 2200 batch: 0.25737\n",
      "val loss in 2 epoch: 0.78011\n",
      "train loss in 2 epoch in 2400 batch: 1.08894\n",
      "val loss in 2 epoch: 0.77661\n",
      "train loss in 2 epoch in 2600 batch: 0.47049\n",
      "val loss in 2 epoch: 0.77643\n",
      "train loss in 2 epoch in 2800 batch: 0.84049\n",
      "val loss in 2 epoch: 0.78335\n",
      "train loss in 2 epoch in 3000 batch: 0.84230\n",
      "val loss in 2 epoch: 0.79976\n",
      "train loss in 2 epoch in 3200 batch: 0.92468\n",
      "val loss in 2 epoch: 0.77671\n",
      "train loss in 2 epoch in 3400 batch: 0.57784\n",
      "val loss in 2 epoch: 0.77822\n",
      "train loss in 2 epoch in 3600 batch: 0.53500\n",
      "val loss in 2 epoch: 0.77639\n",
      "train loss in 2 epoch in 3800 batch: 0.47012\n",
      "val loss in 2 epoch: 0.77340\n",
      "train loss in 2 epoch in 4000 batch: 0.67822\n",
      "val loss in 2 epoch: 0.78114\n",
      "train loss in 2 epoch in 4200 batch: 1.07219\n",
      "val loss in 2 epoch: 0.78305\n",
      "train loss in 2 epoch in 4400 batch: 1.38974\n",
      "val loss in 2 epoch: 0.78272\n",
      "train loss in 2 epoch in 4600 batch: 1.48213\n",
      "val loss in 2 epoch: 0.77720\n",
      "train loss in 2 epoch in 4800 batch: 0.18617\n",
      "val loss in 2 epoch: 0.78157\n",
      "train loss in 2 epoch in 5000 batch: 0.84729\n",
      "val loss in 2 epoch: 0.77742\n",
      "train loss in 2 epoch in 5200 batch: 1.19609\n",
      "val loss in 2 epoch: 0.77735\n",
      "train loss in 2 epoch in 5400 batch: 0.85988\n",
      "val loss in 2 epoch: 0.78156\n",
      "train loss in 2 epoch in 5600 batch: 1.08145\n",
      "val loss in 2 epoch: 0.78011\n",
      "train loss in 2 epoch in 5800 batch: 1.41136\n",
      "val loss in 2 epoch: 0.77255\n",
      "train loss in 2 epoch in 6000 batch: 0.69400\n",
      "val loss in 2 epoch: 0.77191\n",
      "train loss in 2 epoch in 6200 batch: 0.98229\n",
      "val loss in 2 epoch: 0.78991\n",
      "train loss in 2 epoch in 6400 batch: 0.58362\n",
      "val loss in 2 epoch: 0.78186\n",
      "train loss in 2 epoch in 6600 batch: 1.14031\n",
      "val loss in 2 epoch: 0.77455\n",
      "train loss in 2 epoch in 6800 batch: 1.06529\n",
      "val loss in 2 epoch: 0.76828\n",
      "train loss in 2 epoch in 7000 batch: 1.04212\n",
      "val loss in 2 epoch: 0.77175\n",
      "train loss in 2 epoch in 7200 batch: 0.89531\n",
      "val loss in 2 epoch: 0.77858\n",
      "train loss in 2 epoch in 7400 batch: 0.78256\n",
      "val loss in 2 epoch: 0.76931\n",
      "train loss in 2 epoch in 7600 batch: 1.94884\n",
      "val loss in 2 epoch: 0.77283\n",
      "train loss in 2 epoch in 7800 batch: 0.88101\n",
      "val loss in 2 epoch: 0.77520\n",
      "train loss in 2 epoch in 8000 batch: 0.69167\n",
      "val loss in 2 epoch: 0.77272\n",
      "train loss in 2 epoch in 8200 batch: 0.92869\n",
      "val loss in 2 epoch: 0.80310\n",
      "train loss in 2 epoch in 8400 batch: 0.09495\n",
      "val loss in 2 epoch: 0.77671\n",
      "train loss in 2 epoch in 8600 batch: 1.08007\n",
      "val loss in 2 epoch: 0.77128\n",
      "train loss in 2 epoch in 8800 batch: 0.56096\n",
      "val loss in 2 epoch: 0.77103\n",
      "train loss in 2 epoch in 9000 batch: 0.60784\n",
      "val loss in 2 epoch: 0.76775\n",
      "train loss in 2 epoch in 9200 batch: 0.86263\n",
      "val loss in 2 epoch: 0.76977\n",
      "train loss in 2 epoch in 9400 batch: 0.21210\n",
      "val loss in 2 epoch: 0.76709\n",
      "train loss in 2 epoch in 9600 batch: 0.38193\n",
      "val loss in 2 epoch: 0.76857\n",
      "train loss in 2 epoch in 9800 batch: 0.25614\n",
      "val loss in 2 epoch: 0.77177\n",
      "train loss in 2 epoch in 10000 batch: 0.13085\n",
      "val loss in 2 epoch: 0.77326\n",
      "train loss in 2 epoch in 10200 batch: 0.70876\n",
      "val loss in 2 epoch: 0.77549\n",
      "train loss in 2 epoch in 10400 batch: 0.88883\n",
      "val loss in 2 epoch: 0.77507\n",
      "train loss in 2 epoch in 10600 batch: 0.71362\n",
      "val loss in 2 epoch: 0.76808\n",
      "train loss in 2 epoch in 10800 batch: 1.23437\n",
      "val loss in 2 epoch: 0.78420\n",
      "train loss in 2 epoch in 11000 batch: 1.62354\n",
      "val loss in 2 epoch: 0.76886\n",
      "train loss in 2 epoch in 11200 batch: 0.83503\n",
      "val loss in 2 epoch: 0.77696\n",
      "train loss in 2 epoch in 11400 batch: 1.15011\n",
      "val loss in 2 epoch: 0.76945\n",
      "train loss in 2 epoch in 11600 batch: 0.68285\n",
      "val loss in 2 epoch: 0.76451\n",
      "train loss in 2 epoch in 11800 batch: 0.90460\n",
      "val loss in 2 epoch: 0.76801\n",
      "train loss in 2 epoch in 12000 batch: 0.54538\n",
      "val loss in 2 epoch: 0.76962\n",
      "train loss in 2 epoch in 12200 batch: 0.32031\n",
      "val loss in 2 epoch: 0.76812\n",
      "train loss in 2 epoch in 12400 batch: 0.57704\n",
      "val loss in 2 epoch: 0.76900\n",
      "train loss in 2 epoch in 12600 batch: 1.72443\n",
      "val loss in 2 epoch: 0.76615\n",
      "train loss in 2 epoch in 12800 batch: 0.71444\n",
      "val loss in 2 epoch: 0.77341\n",
      "train loss in 2 epoch in 13000 batch: 1.23774\n",
      "val loss in 2 epoch: 0.76424\n",
      "train loss in 2 epoch in 13200 batch: 0.34012\n",
      "val loss in 2 epoch: 0.76391\n",
      "train loss in 2 epoch in 13400 batch: 0.92129\n",
      "val loss in 2 epoch: 0.76625\n",
      "train loss in 2 epoch in 13600 batch: 0.55462\n",
      "val loss in 2 epoch: 0.76448\n",
      "train loss in 2 epoch in 13800 batch: 0.44477\n",
      "val loss in 2 epoch: 0.76321\n",
      "train loss in 2 epoch in 14000 batch: 0.68627\n",
      "val loss in 2 epoch: 0.76593\n",
      "train loss in 2 epoch in 14200 batch: 0.66114\n",
      "val loss in 2 epoch: 0.76508\n",
      "train loss in 2 epoch in 14400 batch: 0.33121\n",
      "val loss in 2 epoch: 0.76677\n",
      "train loss in 2 epoch in 14600 batch: 0.55512\n",
      "val loss in 2 epoch: 0.77009\n",
      "train loss in 2 epoch in 14800 batch: 0.85903\n",
      "val loss in 2 epoch: 0.76972\n",
      "train loss in 2 epoch in 15000 batch: 1.10712\n",
      "val loss in 2 epoch: 0.76703\n",
      "train loss in 2 epoch in 15200 batch: 0.74126\n",
      "val loss in 2 epoch: 0.76947\n",
      "train loss in 2 epoch in 15400 batch: 0.30598\n",
      "val loss in 2 epoch: 0.76144\n",
      "train loss in 2 epoch in 15600 batch: 0.85768\n",
      "val loss in 2 epoch: 0.76149\n",
      "train loss in 2 epoch in 15800 batch: 0.50796\n",
      "val loss in 2 epoch: 0.76230\n",
      "train loss in 2 epoch in 16000 batch: 0.02353\n",
      "val loss in 2 epoch: 0.77144\n",
      "train loss in 2 epoch in 16200 batch: 0.29751\n",
      "val loss in 2 epoch: 0.76237\n",
      "train loss in 3 epoch in 200 batch: 1.26255\n",
      "val loss in 3 epoch: 0.83454\n",
      "train loss in 3 epoch in 400 batch: 0.91519\n",
      "val loss in 3 epoch: 0.76886\n",
      "train loss in 3 epoch in 600 batch: 0.25689\n",
      "val loss in 3 epoch: 0.77011\n",
      "train loss in 3 epoch in 800 batch: 0.74508\n",
      "val loss in 3 epoch: 0.75932\n",
      "train loss in 3 epoch in 1000 batch: 0.97452\n",
      "val loss in 3 epoch: 0.75799\n",
      "train loss in 3 epoch in 1200 batch: 0.98929\n",
      "val loss in 3 epoch: 0.76953\n",
      "train loss in 3 epoch in 1400 batch: 0.36004\n",
      "val loss in 3 epoch: 0.76534\n",
      "train loss in 3 epoch in 1600 batch: 2.38940\n",
      "val loss in 3 epoch: 0.76342\n",
      "train loss in 3 epoch in 1800 batch: 0.15303\n",
      "val loss in 3 epoch: 0.76162\n",
      "train loss in 3 epoch in 2000 batch: 0.31549\n",
      "val loss in 3 epoch: 0.75509\n",
      "train loss in 3 epoch in 2200 batch: 0.44829\n",
      "val loss in 3 epoch: 0.76589\n",
      "train loss in 3 epoch in 2400 batch: 0.15324\n",
      "val loss in 3 epoch: 0.76337\n",
      "train loss in 3 epoch in 2600 batch: 1.00585\n",
      "val loss in 3 epoch: 0.76701\n",
      "train loss in 3 epoch in 2800 batch: 0.96218\n",
      "val loss in 3 epoch: 0.76884\n",
      "train loss in 3 epoch in 3000 batch: 0.28463\n",
      "val loss in 3 epoch: 0.76122\n",
      "train loss in 3 epoch in 3200 batch: 0.64330\n",
      "val loss in 3 epoch: 0.78122\n",
      "train loss in 3 epoch in 3400 batch: 0.53110\n",
      "val loss in 3 epoch: 0.76026\n",
      "train loss in 3 epoch in 3600 batch: 0.26257\n",
      "val loss in 3 epoch: 0.76450\n",
      "train loss in 3 epoch in 3800 batch: 0.39222\n",
      "val loss in 3 epoch: 0.76520\n",
      "train loss in 3 epoch in 4000 batch: 0.70492\n",
      "val loss in 3 epoch: 0.76187\n",
      "train loss in 3 epoch in 4200 batch: 0.49366\n",
      "val loss in 3 epoch: 0.76164\n",
      "train loss in 3 epoch in 4400 batch: 0.98649\n",
      "val loss in 3 epoch: 0.75959\n",
      "train loss in 3 epoch in 4600 batch: 0.76155\n",
      "val loss in 3 epoch: 0.77469\n",
      "train loss in 3 epoch in 4800 batch: 0.81822\n",
      "val loss in 3 epoch: 0.75771\n",
      "train loss in 3 epoch in 5000 batch: 0.88043\n",
      "val loss in 3 epoch: 0.76747\n",
      "train loss in 3 epoch in 5200 batch: 1.28578\n",
      "val loss in 3 epoch: 0.76366\n",
      "train loss in 3 epoch in 5400 batch: 0.86045\n",
      "val loss in 3 epoch: 0.77376\n",
      "train loss in 3 epoch in 5600 batch: 0.67549\n",
      "val loss in 3 epoch: 0.75938\n",
      "train loss in 3 epoch in 5800 batch: 0.51326\n",
      "val loss in 3 epoch: 0.76240\n",
      "train loss in 3 epoch in 6000 batch: 0.65700\n",
      "val loss in 3 epoch: 0.75654\n",
      "train loss in 3 epoch in 6200 batch: 0.47507\n",
      "val loss in 3 epoch: 0.75624\n",
      "train loss in 3 epoch in 6400 batch: 0.99812\n",
      "val loss in 3 epoch: 0.75652\n",
      "train loss in 3 epoch in 6600 batch: 0.99489\n",
      "val loss in 3 epoch: 0.75949\n",
      "train loss in 3 epoch in 6800 batch: 0.02785\n",
      "val loss in 3 epoch: 0.75458\n",
      "train loss in 3 epoch in 7000 batch: 0.83362\n",
      "val loss in 3 epoch: 0.75269\n",
      "train loss in 3 epoch in 7200 batch: 1.19528\n",
      "val loss in 3 epoch: 0.76128\n",
      "train loss in 3 epoch in 7400 batch: 0.02174\n",
      "val loss in 3 epoch: 0.75387\n",
      "train loss in 3 epoch in 7600 batch: 0.57783\n",
      "val loss in 3 epoch: 0.76912\n",
      "train loss in 3 epoch in 7800 batch: 0.90438\n",
      "val loss in 3 epoch: 0.75691\n",
      "train loss in 3 epoch in 8000 batch: 0.13924\n",
      "val loss in 3 epoch: 0.76681\n",
      "train loss in 3 epoch in 8200 batch: 0.65873\n",
      "val loss in 3 epoch: 0.75469\n",
      "train loss in 3 epoch in 8400 batch: 1.69846\n",
      "val loss in 3 epoch: 0.75874\n",
      "train loss in 3 epoch in 8600 batch: 0.77854\n",
      "val loss in 3 epoch: 0.75737\n",
      "train loss in 3 epoch in 8800 batch: 0.29514\n",
      "val loss in 3 epoch: 0.75281\n",
      "train loss in 3 epoch in 9000 batch: 1.03888\n",
      "val loss in 3 epoch: 0.76153\n",
      "train loss in 3 epoch in 9200 batch: 0.06159\n",
      "val loss in 3 epoch: 0.75530\n",
      "train loss in 3 epoch in 9400 batch: 1.04043\n",
      "val loss in 3 epoch: 0.75914\n",
      "train loss in 3 epoch in 9600 batch: 0.03961\n",
      "val loss in 3 epoch: 0.76347\n",
      "train loss in 3 epoch in 9800 batch: 0.33197\n",
      "val loss in 3 epoch: 0.75937\n",
      "train loss in 3 epoch in 10000 batch: 0.36554\n",
      "val loss in 3 epoch: 0.76279\n",
      "train loss in 3 epoch in 10200 batch: 0.56885\n",
      "val loss in 3 epoch: 0.75947\n",
      "train loss in 3 epoch in 10400 batch: 0.08971\n",
      "val loss in 3 epoch: 0.75128\n",
      "train loss in 3 epoch in 10600 batch: 0.04602\n",
      "val loss in 3 epoch: 0.76616\n",
      "train loss in 3 epoch in 10800 batch: 0.66997\n",
      "val loss in 3 epoch: 0.75194\n",
      "train loss in 3 epoch in 11000 batch: 0.22382\n",
      "val loss in 3 epoch: 0.75948\n",
      "train loss in 3 epoch in 11200 batch: 0.39769\n",
      "val loss in 3 epoch: 0.75079\n",
      "train loss in 3 epoch in 11400 batch: 0.50142\n",
      "val loss in 3 epoch: 0.75669\n",
      "train loss in 3 epoch in 11600 batch: 0.95245\n",
      "val loss in 3 epoch: 0.75925\n",
      "train loss in 3 epoch in 11800 batch: 0.62909\n",
      "val loss in 3 epoch: 0.75854\n",
      "train loss in 3 epoch in 12000 batch: 1.03843\n",
      "val loss in 3 epoch: 0.76281\n",
      "train loss in 3 epoch in 12200 batch: 0.42458\n",
      "val loss in 3 epoch: 0.75620\n",
      "train loss in 3 epoch in 12400 batch: 1.68379\n",
      "val loss in 3 epoch: 0.75949\n",
      "train loss in 3 epoch in 12600 batch: 1.23073\n",
      "val loss in 3 epoch: 0.75200\n",
      "train loss in 3 epoch in 12800 batch: 1.18075\n",
      "val loss in 3 epoch: 0.75306\n",
      "train loss in 3 epoch in 13000 batch: 0.21859\n",
      "val loss in 3 epoch: 0.75428\n",
      "train loss in 3 epoch in 13200 batch: 1.45749\n",
      "val loss in 3 epoch: 0.74776\n",
      "train loss in 3 epoch in 13400 batch: 0.85137\n",
      "val loss in 3 epoch: 0.74802\n",
      "train loss in 3 epoch in 13600 batch: 0.33772\n",
      "val loss in 3 epoch: 0.75020\n",
      "train loss in 3 epoch in 13800 batch: 0.59976\n",
      "val loss in 3 epoch: 0.75251\n",
      "train loss in 3 epoch in 14000 batch: 0.91476\n",
      "val loss in 3 epoch: 0.75408\n",
      "train loss in 3 epoch in 14200 batch: 0.30987\n",
      "val loss in 3 epoch: 0.75717\n",
      "train loss in 3 epoch in 14400 batch: 0.80855\n",
      "val loss in 3 epoch: 0.74883\n",
      "train loss in 3 epoch in 14600 batch: 0.77301\n",
      "val loss in 3 epoch: 0.75130\n",
      "train loss in 3 epoch in 14800 batch: 0.94656\n",
      "val loss in 3 epoch: 0.75241\n",
      "train loss in 3 epoch in 15000 batch: 0.90677\n",
      "val loss in 3 epoch: 0.75191\n",
      "train loss in 3 epoch in 15200 batch: 0.80554\n",
      "val loss in 3 epoch: 0.74994\n",
      "train loss in 3 epoch in 15400 batch: 0.74200\n",
      "val loss in 3 epoch: 0.74992\n",
      "train loss in 3 epoch in 15600 batch: 0.63269\n",
      "val loss in 3 epoch: 0.75622\n",
      "train loss in 3 epoch in 15800 batch: 0.89919\n",
      "val loss in 3 epoch: 0.75834\n",
      "train loss in 3 epoch in 16000 batch: 1.54011\n",
      "val loss in 3 epoch: 0.75251\n",
      "train loss in 3 epoch in 16200 batch: 0.56315\n",
      "val loss in 3 epoch: 0.74571\n",
      "train loss in 4 epoch in 200 batch: 1.03643\n",
      "val loss in 4 epoch: 0.81746\n",
      "train loss in 4 epoch in 400 batch: 0.21647\n",
      "val loss in 4 epoch: 0.75997\n",
      "train loss in 4 epoch in 600 batch: 0.92796\n",
      "val loss in 4 epoch: 0.75157\n",
      "train loss in 4 epoch in 800 batch: 0.31752\n",
      "val loss in 4 epoch: 0.75076\n",
      "train loss in 4 epoch in 1000 batch: 0.48065\n",
      "val loss in 4 epoch: 0.76000\n",
      "train loss in 4 epoch in 1200 batch: 0.33884\n",
      "val loss in 4 epoch: 0.74882\n",
      "train loss in 4 epoch in 1400 batch: 0.63055\n",
      "val loss in 4 epoch: 0.74792\n",
      "train loss in 4 epoch in 1600 batch: 0.02766\n",
      "val loss in 4 epoch: 0.75059\n",
      "train loss in 4 epoch in 1800 batch: 1.27582\n",
      "val loss in 4 epoch: 0.75088\n",
      "train loss in 4 epoch in 2000 batch: 1.21310\n",
      "val loss in 4 epoch: 0.74915\n",
      "train loss in 4 epoch in 2200 batch: 0.01710\n",
      "val loss in 4 epoch: 0.76080\n",
      "train loss in 4 epoch in 2400 batch: 0.84755\n",
      "val loss in 4 epoch: 0.74919\n",
      "train loss in 4 epoch in 2600 batch: 0.73901\n",
      "val loss in 4 epoch: 0.74936\n",
      "train loss in 4 epoch in 2800 batch: 0.97641\n",
      "val loss in 4 epoch: 0.74757\n",
      "train loss in 4 epoch in 3000 batch: 0.78779\n",
      "val loss in 4 epoch: 0.75034\n",
      "train loss in 4 epoch in 3200 batch: 0.57692\n",
      "val loss in 4 epoch: 0.74526\n",
      "train loss in 4 epoch in 3400 batch: 0.98679\n",
      "val loss in 4 epoch: 0.75334\n",
      "train loss in 4 epoch in 3600 batch: 1.20611\n",
      "val loss in 4 epoch: 0.74620\n",
      "train loss in 4 epoch in 3800 batch: 0.52468\n",
      "val loss in 4 epoch: 0.75277\n",
      "train loss in 4 epoch in 4000 batch: 0.53703\n",
      "val loss in 4 epoch: 0.74105\n",
      "train loss in 4 epoch in 4200 batch: 0.47940\n",
      "val loss in 4 epoch: 0.74467\n",
      "train loss in 4 epoch in 4400 batch: 0.00604\n",
      "val loss in 4 epoch: 0.75266\n",
      "train loss in 4 epoch in 4600 batch: 0.42233\n",
      "val loss in 4 epoch: 0.74666\n",
      "train loss in 4 epoch in 4800 batch: 0.63377\n",
      "val loss in 4 epoch: 0.74380\n",
      "train loss in 4 epoch in 5000 batch: 0.65864\n",
      "val loss in 4 epoch: 0.74595\n",
      "train loss in 4 epoch in 5200 batch: 0.68804\n",
      "val loss in 4 epoch: 0.74523\n",
      "train loss in 4 epoch in 5400 batch: 0.73997\n",
      "val loss in 4 epoch: 0.74142\n",
      "train loss in 4 epoch in 5600 batch: 0.65952\n",
      "val loss in 4 epoch: 0.73929\n",
      "train loss in 4 epoch in 5800 batch: 0.33864\n",
      "val loss in 4 epoch: 0.74475\n",
      "train loss in 4 epoch in 6000 batch: 0.66944\n",
      "val loss in 4 epoch: 0.74036\n",
      "train loss in 4 epoch in 6200 batch: 0.48843\n",
      "val loss in 4 epoch: 0.74337\n",
      "train loss in 4 epoch in 6400 batch: 0.76286\n",
      "val loss in 4 epoch: 0.74931\n",
      "train loss in 4 epoch in 6600 batch: 0.53415\n",
      "val loss in 4 epoch: 0.74433\n",
      "train loss in 4 epoch in 6800 batch: 0.20649\n",
      "val loss in 4 epoch: 0.75217\n",
      "train loss in 4 epoch in 7000 batch: 0.77740\n",
      "val loss in 4 epoch: 0.75377\n",
      "train loss in 4 epoch in 7200 batch: 0.89484\n",
      "val loss in 4 epoch: 0.74698\n",
      "train loss in 4 epoch in 7400 batch: 0.49226\n",
      "val loss in 4 epoch: 0.74696\n",
      "train loss in 4 epoch in 7600 batch: 0.23664\n",
      "val loss in 4 epoch: 0.74674\n",
      "train loss in 4 epoch in 7800 batch: 0.76470\n",
      "val loss in 4 epoch: 0.74621\n",
      "train loss in 4 epoch in 8000 batch: 0.40506\n",
      "val loss in 4 epoch: 0.77055\n",
      "train loss in 4 epoch in 8200 batch: 1.02409\n",
      "val loss in 4 epoch: 0.74134\n",
      "train loss in 4 epoch in 8400 batch: 0.73911\n",
      "val loss in 4 epoch: 0.75071\n",
      "train loss in 4 epoch in 8600 batch: 1.43493\n",
      "val loss in 4 epoch: 0.73966\n",
      "train loss in 4 epoch in 8800 batch: 0.76751\n",
      "val loss in 4 epoch: 0.74614\n",
      "train loss in 4 epoch in 9000 batch: 0.37681\n",
      "val loss in 4 epoch: 0.74928\n",
      "train loss in 4 epoch in 9200 batch: 1.09096\n",
      "val loss in 4 epoch: 0.74386\n",
      "train loss in 4 epoch in 9400 batch: 1.33213\n",
      "val loss in 4 epoch: 0.74306\n",
      "train loss in 4 epoch in 9600 batch: 0.35508\n",
      "val loss in 4 epoch: 0.74844\n",
      "train loss in 4 epoch in 9800 batch: 0.30832\n",
      "val loss in 4 epoch: 0.74731\n",
      "train loss in 4 epoch in 10000 batch: 0.70293\n",
      "val loss in 4 epoch: 0.75061\n",
      "train loss in 4 epoch in 10200 batch: 0.45899\n",
      "val loss in 4 epoch: 0.74233\n",
      "train loss in 4 epoch in 10400 batch: 0.04382\n",
      "val loss in 4 epoch: 0.75199\n",
      "train loss in 4 epoch in 10600 batch: 0.27573\n",
      "val loss in 4 epoch: 0.75352\n",
      "train loss in 4 epoch in 10800 batch: 0.29210\n",
      "val loss in 4 epoch: 0.74629\n",
      "train loss in 4 epoch in 11000 batch: 0.58277\n",
      "val loss in 4 epoch: 0.74258\n",
      "train loss in 4 epoch in 11200 batch: 0.45787\n",
      "val loss in 4 epoch: 0.75081\n",
      "train loss in 4 epoch in 11400 batch: 1.12966\n",
      "val loss in 4 epoch: 0.74466\n",
      "train loss in 4 epoch in 11600 batch: 0.85912\n",
      "val loss in 4 epoch: 0.74319\n",
      "train loss in 4 epoch in 11800 batch: 1.05977\n",
      "val loss in 4 epoch: 0.74414\n",
      "train loss in 4 epoch in 12000 batch: 0.22622\n",
      "val loss in 4 epoch: 0.73765\n",
      "train loss in 4 epoch in 12200 batch: 0.60382\n",
      "val loss in 4 epoch: 0.73933\n",
      "train loss in 4 epoch in 12400 batch: 0.22066\n",
      "val loss in 4 epoch: 0.75630\n",
      "train loss in 4 epoch in 12600 batch: 0.90868\n",
      "val loss in 4 epoch: 0.73661\n",
      "train loss in 4 epoch in 12800 batch: 0.81656\n",
      "val loss in 4 epoch: 0.74511\n",
      "train loss in 4 epoch in 13000 batch: 1.45356\n",
      "val loss in 4 epoch: 0.74116\n",
      "train loss in 4 epoch in 13200 batch: 1.31800\n",
      "val loss in 4 epoch: 0.75327\n",
      "train loss in 4 epoch in 13400 batch: 1.26536\n",
      "val loss in 4 epoch: 0.73969\n",
      "train loss in 4 epoch in 13600 batch: 0.84327\n",
      "val loss in 4 epoch: 0.74090\n",
      "train loss in 4 epoch in 13800 batch: 0.68784\n",
      "val loss in 4 epoch: 0.73850\n",
      "train loss in 4 epoch in 14000 batch: 0.89997\n",
      "val loss in 4 epoch: 0.74446\n",
      "train loss in 4 epoch in 14200 batch: 1.35858\n",
      "val loss in 4 epoch: 0.74113\n",
      "train loss in 4 epoch in 14400 batch: 0.91620\n",
      "val loss in 4 epoch: 0.74187\n",
      "train loss in 4 epoch in 14600 batch: 0.88351\n",
      "val loss in 4 epoch: 0.73975\n",
      "train loss in 4 epoch in 14800 batch: 0.15905\n",
      "val loss in 4 epoch: 0.73779\n",
      "train loss in 4 epoch in 15000 batch: 0.41919\n",
      "val loss in 4 epoch: 0.74982\n",
      "train loss in 4 epoch in 15200 batch: 1.11240\n",
      "val loss in 4 epoch: 0.73914\n",
      "train loss in 4 epoch in 15400 batch: 0.65390\n",
      "val loss in 4 epoch: 0.73950\n",
      "train loss in 4 epoch in 15600 batch: 0.28683\n",
      "val loss in 4 epoch: 0.75209\n",
      "train loss in 4 epoch in 15800 batch: 0.41292\n",
      "val loss in 4 epoch: 0.73982\n",
      "train loss in 4 epoch in 16000 batch: 1.08372\n",
      "val loss in 4 epoch: 0.73772\n",
      "train loss in 4 epoch in 16200 batch: 0.61727\n",
      "val loss in 4 epoch: 0.74404\n"
     ]
    }
   ],
   "source": [
    "# v18\n",
    "INPUT_F = 36\n",
    "H = 256\n",
    "DROP_P = 0.5\n",
    "\n",
    "model = NNWithAttention(INPUT_F, DROP_P, H)\n",
    "\n",
    "def policy_gradient(logits_batch, rewards_batch, actions_batch, _):\n",
    "    loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    logits_batch = torch.reshape(logits_batch, (-1, 100))\n",
    "    policy = loss(logits_batch, actions_batch)\n",
    "    out = torch.mean(policy * rewards_batch)\n",
    "    return out\n",
    "\n",
    "learn(nagiss_top_10[0], nagiss_top_10[1], model,\n",
    "      freq=200, batch_size=16,lr=1e-4, epochs=4, criterion=policy_gradient)  # stoped manualy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "566.0 555.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "600.0 616.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "567.0 574.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "638.0 626.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "566.0 544.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "629.0 556.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "620.0 624.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "631.0 556.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "666.0 650.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n",
      "599.0 606.0 tmp/b_0.8650894478131601.py tmp/b_0.8547409200351491.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.06907289544872972,\n",
       " 17.5,\n",
       " 30.44092639851816,\n",
       " 0.6,\n",
       " 'tmp/b_0.8650894478131601.py',\n",
       " 'tmp/b_0.8547409200351491.py',\n",
       " array([566., 600., 567., 638., 566., 629., 620., 631., 666., 599.]),\n",
       " array([555., 616., 574., 626., 544., 556., 624., 556., 650., 606.]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v18\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,False\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=False\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 200 batch: 1.07600\n",
      "val loss in 1 epoch: 0.97636\n",
      "train loss in 1 epoch in 400 batch: 0.40775\n",
      "val loss in 1 epoch: 0.92396\n",
      "train loss in 1 epoch in 600 batch: 0.63201\n",
      "val loss in 1 epoch: 0.90262\n",
      "train loss in 1 epoch in 800 batch: 0.52119\n",
      "val loss in 1 epoch: 0.89054\n",
      "train loss in 1 epoch in 1000 batch: 0.08349\n",
      "val loss in 1 epoch: 0.89122\n",
      "train loss in 1 epoch in 1200 batch: 0.71327\n",
      "val loss in 1 epoch: 0.88878\n",
      "train loss in 1 epoch in 1400 batch: 1.11178\n",
      "val loss in 1 epoch: 0.88482\n",
      "train loss in 1 epoch in 1600 batch: 0.14835\n",
      "val loss in 1 epoch: 0.87769\n",
      "train loss in 1 epoch in 1800 batch: 0.31868\n",
      "val loss in 1 epoch: 0.86475\n",
      "train loss in 1 epoch in 2000 batch: 0.43355\n",
      "val loss in 1 epoch: 0.86681\n",
      "train loss in 1 epoch in 2200 batch: 1.10838\n",
      "val loss in 1 epoch: 0.87277\n",
      "train loss in 1 epoch in 2400 batch: 0.39577\n",
      "val loss in 1 epoch: 0.86857\n",
      "train loss in 1 epoch in 2600 batch: 1.37946\n",
      "val loss in 1 epoch: 0.86005\n",
      "train loss in 1 epoch in 2800 batch: 1.47551\n",
      "val loss in 1 epoch: 0.86091\n",
      "train loss in 1 epoch in 3000 batch: 0.36968\n",
      "val loss in 1 epoch: 0.85355\n",
      "train loss in 1 epoch in 3200 batch: 0.31977\n",
      "val loss in 1 epoch: 0.85236\n",
      "train loss in 1 epoch in 3400 batch: 0.82889\n",
      "val loss in 1 epoch: 0.85771\n",
      "train loss in 1 epoch in 3600 batch: 1.24917\n",
      "val loss in 1 epoch: 0.84471\n",
      "train loss in 1 epoch in 3800 batch: 0.90185\n",
      "val loss in 1 epoch: 0.85781\n",
      "train loss in 1 epoch in 4000 batch: 0.91250\n",
      "val loss in 1 epoch: 0.85347\n",
      "train loss in 1 epoch in 4200 batch: 0.56992\n",
      "val loss in 1 epoch: 0.85492\n",
      "train loss in 1 epoch in 4400 batch: 1.43720\n",
      "val loss in 1 epoch: 0.84702\n",
      "train loss in 1 epoch in 4600 batch: 2.01302\n",
      "val loss in 1 epoch: 0.83933\n",
      "train loss in 1 epoch in 4800 batch: 0.43032\n",
      "val loss in 1 epoch: 0.84182\n",
      "train loss in 1 epoch in 5000 batch: 0.75304\n",
      "val loss in 1 epoch: 0.84104\n",
      "train loss in 1 epoch in 5200 batch: 0.95521\n",
      "val loss in 1 epoch: 0.84249\n",
      "train loss in 1 epoch in 5400 batch: 0.89466\n",
      "val loss in 1 epoch: 0.84305\n",
      "train loss in 1 epoch in 5600 batch: 1.87196\n",
      "val loss in 1 epoch: 0.83423\n",
      "train loss in 1 epoch in 5800 batch: 0.68325\n",
      "val loss in 1 epoch: 0.83079\n",
      "train loss in 1 epoch in 6000 batch: 0.44111\n",
      "val loss in 1 epoch: 0.83572\n",
      "train loss in 1 epoch in 6200 batch: 0.76567\n",
      "val loss in 1 epoch: 0.83492\n",
      "train loss in 1 epoch in 6400 batch: 1.25003\n",
      "val loss in 1 epoch: 0.83576\n",
      "train loss in 1 epoch in 6600 batch: 1.21175\n",
      "val loss in 1 epoch: 0.82712\n",
      "train loss in 1 epoch in 6800 batch: 1.51300\n",
      "val loss in 1 epoch: 0.83269\n",
      "train loss in 1 epoch in 7000 batch: 0.69917\n",
      "val loss in 1 epoch: 0.82576\n",
      "train loss in 1 epoch in 7200 batch: 0.23259\n",
      "val loss in 1 epoch: 0.83095\n",
      "train loss in 1 epoch in 7400 batch: 0.77925\n",
      "val loss in 1 epoch: 0.82773\n",
      "train loss in 1 epoch in 7600 batch: 0.82931\n",
      "val loss in 1 epoch: 0.83565\n",
      "train loss in 1 epoch in 7800 batch: 0.77843\n",
      "val loss in 1 epoch: 0.83194\n",
      "train loss in 1 epoch in 8000 batch: 0.63864\n",
      "val loss in 1 epoch: 0.83344\n",
      "train loss in 1 epoch in 8200 batch: 0.59637\n",
      "val loss in 1 epoch: 0.84307\n",
      "train loss in 1 epoch in 8400 batch: 0.80073\n",
      "val loss in 1 epoch: 0.83625\n",
      "train loss in 1 epoch in 8600 batch: 1.17370\n",
      "val loss in 1 epoch: 0.82151\n",
      "train loss in 1 epoch in 8800 batch: 0.59201\n",
      "val loss in 1 epoch: 0.83017\n",
      "train loss in 1 epoch in 9000 batch: 1.52851\n",
      "val loss in 1 epoch: 0.82710\n",
      "train loss in 1 epoch in 9200 batch: 0.74194\n",
      "val loss in 1 epoch: 0.82036\n",
      "train loss in 1 epoch in 9400 batch: 1.04976\n",
      "val loss in 1 epoch: 0.82158\n",
      "train loss in 1 epoch in 9600 batch: 1.10557\n",
      "val loss in 1 epoch: 0.81841\n",
      "train loss in 1 epoch in 9800 batch: 0.53088\n",
      "val loss in 1 epoch: 0.81467\n",
      "train loss in 1 epoch in 10000 batch: 0.43738\n",
      "val loss in 1 epoch: 0.81320\n",
      "train loss in 1 epoch in 10200 batch: 1.01609\n",
      "val loss in 1 epoch: 0.82221\n",
      "train loss in 1 epoch in 10400 batch: 0.78626\n",
      "val loss in 1 epoch: 0.82230\n",
      "train loss in 1 epoch in 10600 batch: 0.51276\n",
      "val loss in 1 epoch: 0.82698\n",
      "train loss in 1 epoch in 10800 batch: 0.77430\n",
      "val loss in 1 epoch: 0.81571\n",
      "train loss in 1 epoch in 11000 batch: 1.65386\n",
      "val loss in 1 epoch: 0.82522\n",
      "train loss in 1 epoch in 11200 batch: 0.48794\n",
      "val loss in 1 epoch: 0.82626\n",
      "train loss in 1 epoch in 11400 batch: 0.53905\n",
      "val loss in 1 epoch: 0.81299\n",
      "train loss in 1 epoch in 11600 batch: 0.90668\n",
      "val loss in 1 epoch: 0.82220\n",
      "train loss in 1 epoch in 11800 batch: 1.64411\n",
      "val loss in 1 epoch: 0.81718\n",
      "train loss in 1 epoch in 12000 batch: 0.48321\n",
      "val loss in 1 epoch: 0.82567\n",
      "train loss in 1 epoch in 12200 batch: 0.48633\n",
      "val loss in 1 epoch: 0.81844\n",
      "train loss in 1 epoch in 12400 batch: 0.90076\n",
      "val loss in 1 epoch: 0.82063\n",
      "train loss in 1 epoch in 12600 batch: 0.81830\n",
      "val loss in 1 epoch: 0.82554\n",
      "train loss in 1 epoch in 12800 batch: 0.06317\n",
      "val loss in 1 epoch: 0.81239\n",
      "train loss in 1 epoch in 13000 batch: 0.79864\n",
      "val loss in 1 epoch: 0.81366\n",
      "train loss in 1 epoch in 13200 batch: 1.08738\n",
      "val loss in 1 epoch: 0.82455\n",
      "train loss in 1 epoch in 13400 batch: 0.83905\n",
      "val loss in 1 epoch: 0.81745\n",
      "train loss in 1 epoch in 13600 batch: 0.02984\n",
      "val loss in 1 epoch: 0.81805\n",
      "train loss in 1 epoch in 13800 batch: 1.26891\n",
      "val loss in 1 epoch: 0.81496\n",
      "train loss in 1 epoch in 14000 batch: 0.88299\n",
      "val loss in 1 epoch: 0.80728\n",
      "train loss in 1 epoch in 14200 batch: 0.75256\n",
      "val loss in 1 epoch: 0.80815\n",
      "train loss in 1 epoch in 14400 batch: 0.52473\n",
      "val loss in 1 epoch: 0.81014\n",
      "train loss in 1 epoch in 14600 batch: 1.18002\n",
      "val loss in 1 epoch: 0.82546\n",
      "train loss in 1 epoch in 14800 batch: 0.63808\n",
      "val loss in 1 epoch: 0.81402\n",
      "train loss in 1 epoch in 15000 batch: 0.82655\n",
      "val loss in 1 epoch: 0.81887\n",
      "train loss in 1 epoch in 15200 batch: 1.19108\n",
      "val loss in 1 epoch: 0.82348\n",
      "train loss in 1 epoch in 15400 batch: 1.36448\n",
      "val loss in 1 epoch: 0.81209\n",
      "train loss in 1 epoch in 15600 batch: 0.85023\n",
      "val loss in 1 epoch: 0.81669\n",
      "train loss in 1 epoch in 15800 batch: 0.97721\n",
      "val loss in 1 epoch: 0.81885\n",
      "train loss in 1 epoch in 16000 batch: 0.83889\n",
      "val loss in 1 epoch: 0.81155\n",
      "train loss in 1 epoch in 16200 batch: 0.40207\n",
      "val loss in 1 epoch: 0.80381\n",
      "train loss in 2 epoch in 200 batch: 0.91117\n",
      "val loss in 2 epoch: 0.85566\n",
      "train loss in 2 epoch in 400 batch: 0.60047\n",
      "val loss in 2 epoch: 0.81690\n",
      "train loss in 2 epoch in 600 batch: 0.39026\n",
      "val loss in 2 epoch: 0.80889\n",
      "train loss in 2 epoch in 800 batch: 0.69634\n",
      "val loss in 2 epoch: 0.80730\n",
      "train loss in 2 epoch in 1000 batch: 0.83505\n",
      "val loss in 2 epoch: 0.82142\n",
      "train loss in 2 epoch in 1200 batch: 0.08685\n",
      "val loss in 2 epoch: 0.80836\n",
      "train loss in 2 epoch in 1400 batch: 1.09525\n",
      "val loss in 2 epoch: 0.80422\n",
      "train loss in 2 epoch in 1600 batch: 1.76198\n",
      "val loss in 2 epoch: 0.81586\n",
      "train loss in 2 epoch in 1800 batch: 1.13247\n",
      "val loss in 2 epoch: 0.80413\n",
      "train loss in 2 epoch in 2000 batch: 0.21696\n",
      "val loss in 2 epoch: 0.81393\n",
      "train loss in 2 epoch in 2200 batch: 0.27239\n",
      "val loss in 2 epoch: 0.80768\n",
      "train loss in 2 epoch in 2400 batch: 1.06900\n",
      "val loss in 2 epoch: 0.80465\n",
      "train loss in 2 epoch in 2600 batch: 0.48283\n",
      "val loss in 2 epoch: 0.80328\n",
      "train loss in 2 epoch in 2800 batch: 0.94542\n",
      "val loss in 2 epoch: 0.80505\n",
      "train loss in 2 epoch in 3000 batch: 0.77887\n",
      "val loss in 2 epoch: 0.81451\n",
      "train loss in 2 epoch in 3200 batch: 0.93764\n",
      "val loss in 2 epoch: 0.80418\n",
      "train loss in 2 epoch in 3400 batch: 0.74452\n",
      "val loss in 2 epoch: 0.80641\n",
      "train loss in 2 epoch in 3600 batch: 0.51767\n",
      "val loss in 2 epoch: 0.80233\n",
      "train loss in 2 epoch in 3800 batch: 0.70084\n",
      "val loss in 2 epoch: 0.80023\n",
      "train loss in 2 epoch in 4000 batch: 0.63058\n",
      "val loss in 2 epoch: 0.81131\n",
      "train loss in 2 epoch in 4200 batch: 1.13012\n",
      "val loss in 2 epoch: 0.81750\n",
      "train loss in 2 epoch in 4400 batch: 1.28464\n",
      "val loss in 2 epoch: 0.81023\n",
      "train loss in 2 epoch in 4600 batch: 1.52081\n",
      "val loss in 2 epoch: 0.80010\n",
      "train loss in 2 epoch in 4800 batch: 0.18025\n",
      "val loss in 2 epoch: 0.80725\n",
      "train loss in 2 epoch in 5000 batch: 0.94773\n",
      "val loss in 2 epoch: 0.80251\n",
      "train loss in 2 epoch in 5200 batch: 1.28817\n",
      "val loss in 2 epoch: 0.79903\n",
      "train loss in 2 epoch in 5400 batch: 0.86415\n",
      "val loss in 2 epoch: 0.81199\n",
      "train loss in 2 epoch in 5600 batch: 1.01734\n",
      "val loss in 2 epoch: 0.81299\n",
      "train loss in 2 epoch in 5800 batch: 1.55693\n",
      "val loss in 2 epoch: 0.79593\n",
      "train loss in 2 epoch in 6000 batch: 0.68211\n",
      "val loss in 2 epoch: 0.80625\n",
      "train loss in 2 epoch in 6200 batch: 0.99243\n",
      "val loss in 2 epoch: 0.81729\n",
      "train loss in 2 epoch in 6400 batch: 0.59359\n",
      "val loss in 2 epoch: 0.79875\n",
      "train loss in 2 epoch in 6600 batch: 0.86951\n",
      "val loss in 2 epoch: 0.79471\n",
      "train loss in 2 epoch in 6800 batch: 1.06679\n",
      "val loss in 2 epoch: 0.79255\n",
      "train loss in 2 epoch in 7000 batch: 1.10532\n",
      "val loss in 2 epoch: 0.79926\n",
      "train loss in 2 epoch in 7200 batch: 0.86315\n",
      "val loss in 2 epoch: 0.79989\n",
      "train loss in 2 epoch in 7400 batch: 0.80718\n",
      "val loss in 2 epoch: 0.79900\n",
      "train loss in 2 epoch in 7600 batch: 1.92497\n",
      "val loss in 2 epoch: 0.79568\n",
      "train loss in 2 epoch in 7800 batch: 0.92512\n",
      "val loss in 2 epoch: 0.79742\n",
      "train loss in 2 epoch in 8000 batch: 0.68259\n",
      "val loss in 2 epoch: 0.79980\n",
      "train loss in 2 epoch in 8200 batch: 0.91206\n",
      "val loss in 2 epoch: 0.83044\n",
      "train loss in 2 epoch in 8400 batch: 0.10148\n",
      "val loss in 2 epoch: 0.80608\n",
      "train loss in 2 epoch in 8600 batch: 1.10740\n",
      "val loss in 2 epoch: 0.79084\n",
      "train loss in 2 epoch in 8800 batch: 0.66663\n",
      "val loss in 2 epoch: 0.80603\n",
      "train loss in 2 epoch in 9000 batch: 0.61130\n",
      "val loss in 2 epoch: 0.79981\n",
      "train loss in 2 epoch in 9200 batch: 0.92245\n",
      "val loss in 2 epoch: 0.79670\n",
      "train loss in 2 epoch in 9400 batch: 0.22910\n",
      "val loss in 2 epoch: 0.78783\n",
      "train loss in 2 epoch in 9600 batch: 0.45901\n",
      "val loss in 2 epoch: 0.79840\n",
      "train loss in 2 epoch in 9800 batch: 0.20399\n",
      "val loss in 2 epoch: 0.79495\n",
      "train loss in 2 epoch in 10000 batch: 0.16688\n",
      "val loss in 2 epoch: 0.79122\n",
      "train loss in 2 epoch in 10200 batch: 0.69804\n",
      "val loss in 2 epoch: 0.79533\n",
      "train loss in 2 epoch in 10400 batch: 0.95246\n",
      "val loss in 2 epoch: 0.80041\n",
      "train loss in 2 epoch in 10600 batch: 0.77442\n",
      "val loss in 2 epoch: 0.78969\n",
      "train loss in 2 epoch in 10800 batch: 1.34044\n",
      "val loss in 2 epoch: 0.80335\n",
      "train loss in 2 epoch in 11000 batch: 1.60640\n",
      "val loss in 2 epoch: 0.79405\n",
      "train loss in 2 epoch in 11200 batch: 0.86272\n",
      "val loss in 2 epoch: 0.80664\n",
      "train loss in 2 epoch in 11400 batch: 1.16003\n",
      "val loss in 2 epoch: 0.79521\n",
      "train loss in 2 epoch in 11600 batch: 0.72906\n",
      "val loss in 2 epoch: 0.78818\n",
      "train loss in 2 epoch in 11800 batch: 1.06391\n",
      "val loss in 2 epoch: 0.80366\n",
      "train loss in 2 epoch in 12000 batch: 0.59821\n",
      "val loss in 2 epoch: 0.79021\n",
      "train loss in 2 epoch in 12200 batch: 0.29824\n",
      "val loss in 2 epoch: 0.80749\n",
      "train loss in 2 epoch in 12400 batch: 0.58046\n",
      "val loss in 2 epoch: 0.79250\n",
      "train loss in 2 epoch in 12600 batch: 1.75540\n",
      "val loss in 2 epoch: 0.79715\n",
      "train loss in 2 epoch in 12800 batch: 0.68954\n",
      "val loss in 2 epoch: 0.79734\n",
      "train loss in 2 epoch in 13000 batch: 1.35632\n",
      "val loss in 2 epoch: 0.78777\n",
      "train loss in 2 epoch in 13200 batch: 0.40443\n",
      "val loss in 2 epoch: 0.79353\n",
      "train loss in 2 epoch in 13400 batch: 0.92052\n",
      "val loss in 2 epoch: 0.78842\n",
      "train loss in 2 epoch in 13600 batch: 0.47018\n",
      "val loss in 2 epoch: 0.79056\n",
      "train loss in 2 epoch in 13800 batch: 0.45709\n",
      "val loss in 2 epoch: 0.78675\n",
      "train loss in 2 epoch in 14000 batch: 0.71290\n",
      "val loss in 2 epoch: 0.79168\n",
      "train loss in 2 epoch in 14200 batch: 0.68152\n",
      "val loss in 2 epoch: 0.78961\n",
      "train loss in 2 epoch in 14400 batch: 0.34263\n",
      "val loss in 2 epoch: 0.78475\n",
      "train loss in 2 epoch in 14600 batch: 0.53595\n",
      "val loss in 2 epoch: 0.79322\n",
      "train loss in 2 epoch in 14800 batch: 0.99598\n",
      "val loss in 2 epoch: 0.79108\n",
      "train loss in 2 epoch in 15000 batch: 1.07139\n",
      "val loss in 2 epoch: 0.79046\n",
      "train loss in 2 epoch in 15200 batch: 0.74408\n",
      "val loss in 2 epoch: 0.79114\n",
      "train loss in 2 epoch in 15400 batch: 0.33583\n",
      "val loss in 2 epoch: 0.79074\n",
      "train loss in 2 epoch in 15600 batch: 0.99102\n",
      "val loss in 2 epoch: 0.78583\n",
      "train loss in 2 epoch in 15800 batch: 0.52217\n",
      "val loss in 2 epoch: 0.78644\n",
      "train loss in 2 epoch in 16000 batch: 0.08675\n",
      "val loss in 2 epoch: 0.79237\n",
      "train loss in 2 epoch in 16200 batch: 0.31002\n",
      "val loss in 2 epoch: 0.78563\n",
      "train loss in 3 epoch in 200 batch: 1.36021\n",
      "val loss in 3 epoch: 0.84526\n",
      "train loss in 3 epoch in 400 batch: 0.96882\n",
      "val loss in 3 epoch: 0.78982\n",
      "train loss in 3 epoch in 600 batch: 0.27056\n",
      "val loss in 3 epoch: 0.78589\n",
      "train loss in 3 epoch in 800 batch: 0.74789\n",
      "val loss in 3 epoch: 0.78234\n",
      "train loss in 3 epoch in 1000 batch: 1.06556\n",
      "val loss in 3 epoch: 0.78069\n",
      "train loss in 3 epoch in 1200 batch: 1.17837\n",
      "val loss in 3 epoch: 0.79506\n",
      "train loss in 3 epoch in 1400 batch: 0.41849\n",
      "val loss in 3 epoch: 0.79728\n",
      "train loss in 3 epoch in 1600 batch: 2.43956\n",
      "val loss in 3 epoch: 0.78180\n",
      "train loss in 3 epoch in 1800 batch: 0.20032\n",
      "val loss in 3 epoch: 0.78650\n",
      "train loss in 3 epoch in 2000 batch: 0.32675\n",
      "val loss in 3 epoch: 0.78208\n",
      "train loss in 3 epoch in 2200 batch: 0.46119\n",
      "val loss in 3 epoch: 0.78413\n",
      "train loss in 3 epoch in 2400 batch: 0.11161\n",
      "val loss in 3 epoch: 0.78396\n",
      "train loss in 3 epoch in 2600 batch: 1.03671\n",
      "val loss in 3 epoch: 0.79995\n",
      "train loss in 3 epoch in 2800 batch: 1.03898\n",
      "val loss in 3 epoch: 0.79210\n",
      "train loss in 3 epoch in 3000 batch: 0.32612\n",
      "val loss in 3 epoch: 0.78598\n",
      "train loss in 3 epoch in 3200 batch: 0.64453\n",
      "val loss in 3 epoch: 0.79840\n",
      "train loss in 3 epoch in 3400 batch: 0.56540\n",
      "val loss in 3 epoch: 0.78281\n",
      "train loss in 3 epoch in 3600 batch: 0.27025\n",
      "val loss in 3 epoch: 0.79052\n",
      "train loss in 3 epoch in 3800 batch: 0.42082\n",
      "val loss in 3 epoch: 0.79259\n",
      "train loss in 3 epoch in 4000 batch: 0.73903\n",
      "val loss in 3 epoch: 0.78352\n",
      "train loss in 3 epoch in 4200 batch: 0.56176\n",
      "val loss in 3 epoch: 0.78685\n",
      "train loss in 3 epoch in 4400 batch: 0.98992\n",
      "val loss in 3 epoch: 0.78801\n",
      "train loss in 3 epoch in 4600 batch: 0.71874\n",
      "val loss in 3 epoch: 0.79025\n",
      "train loss in 3 epoch in 4800 batch: 0.90131\n",
      "val loss in 3 epoch: 0.78919\n",
      "train loss in 3 epoch in 5000 batch: 0.93069\n",
      "val loss in 3 epoch: 0.79445\n",
      "train loss in 3 epoch in 5200 batch: 1.33453\n",
      "val loss in 3 epoch: 0.78398\n",
      "train loss in 3 epoch in 5400 batch: 0.85363\n",
      "val loss in 3 epoch: 0.79713\n",
      "train loss in 3 epoch in 5600 batch: 0.72400\n",
      "val loss in 3 epoch: 0.78673\n",
      "train loss in 3 epoch in 5800 batch: 0.42927\n",
      "val loss in 3 epoch: 0.78709\n",
      "train loss in 3 epoch in 6000 batch: 0.62973\n",
      "val loss in 3 epoch: 0.78689\n",
      "train loss in 3 epoch in 6200 batch: 0.58001\n",
      "val loss in 3 epoch: 0.78684\n",
      "train loss in 3 epoch in 6400 batch: 1.07218\n",
      "val loss in 3 epoch: 0.78349\n",
      "train loss in 3 epoch in 6600 batch: 0.98140\n",
      "val loss in 3 epoch: 0.78268\n",
      "train loss in 3 epoch in 6800 batch: 0.06527\n",
      "val loss in 3 epoch: 0.78194\n",
      "train loss in 3 epoch in 7000 batch: 0.88945\n",
      "val loss in 3 epoch: 0.78225\n",
      "train loss in 3 epoch in 7200 batch: 1.03321\n",
      "val loss in 3 epoch: 0.77743\n",
      "train loss in 3 epoch in 7400 batch: 0.04802\n",
      "val loss in 3 epoch: 0.77887\n",
      "train loss in 3 epoch in 7600 batch: 0.57425\n",
      "val loss in 3 epoch: 0.78885\n",
      "train loss in 3 epoch in 7800 batch: 0.92383\n",
      "val loss in 3 epoch: 0.77786\n",
      "train loss in 3 epoch in 8000 batch: 0.25288\n",
      "val loss in 3 epoch: 0.79124\n",
      "train loss in 3 epoch in 8200 batch: 0.70724\n",
      "val loss in 3 epoch: 0.78259\n",
      "train loss in 3 epoch in 8400 batch: 1.61399\n",
      "val loss in 3 epoch: 0.77858\n",
      "train loss in 3 epoch in 8600 batch: 0.75127\n",
      "val loss in 3 epoch: 0.77819\n",
      "train loss in 3 epoch in 8800 batch: 0.32648\n",
      "val loss in 3 epoch: 0.78030\n",
      "train loss in 3 epoch in 9000 batch: 0.85084\n",
      "val loss in 3 epoch: 0.77840\n",
      "train loss in 3 epoch in 9200 batch: 0.09388\n",
      "val loss in 3 epoch: 0.77974\n",
      "train loss in 3 epoch in 9400 batch: 1.12049\n",
      "val loss in 3 epoch: 0.77741\n",
      "train loss in 3 epoch in 9600 batch: 0.04222\n",
      "val loss in 3 epoch: 0.78218\n",
      "train loss in 3 epoch in 9800 batch: 0.38768\n",
      "val loss in 3 epoch: 0.78281\n",
      "train loss in 3 epoch in 10000 batch: 0.37954\n",
      "val loss in 3 epoch: 0.78404\n",
      "train loss in 3 epoch in 10200 batch: 0.64904\n",
      "val loss in 3 epoch: 0.78093\n",
      "train loss in 3 epoch in 10400 batch: 0.06770\n",
      "val loss in 3 epoch: 0.77857\n",
      "train loss in 3 epoch in 10600 batch: 0.09204\n",
      "val loss in 3 epoch: 0.79524\n",
      "train loss in 3 epoch in 10800 batch: 0.72487\n",
      "val loss in 3 epoch: 0.78238\n",
      "train loss in 3 epoch in 11000 batch: 0.25160\n",
      "val loss in 3 epoch: 0.77642\n",
      "train loss in 3 epoch in 11200 batch: 0.43299\n",
      "val loss in 3 epoch: 0.77906\n",
      "train loss in 3 epoch in 11400 batch: 0.52555\n",
      "val loss in 3 epoch: 0.78031\n",
      "train loss in 3 epoch in 11600 batch: 1.00045\n",
      "val loss in 3 epoch: 0.78416\n",
      "train loss in 3 epoch in 11800 batch: 0.58723\n",
      "val loss in 3 epoch: 0.77641\n",
      "train loss in 3 epoch in 12000 batch: 1.04517\n",
      "val loss in 3 epoch: 0.78043\n",
      "train loss in 3 epoch in 12200 batch: 0.46994\n",
      "val loss in 3 epoch: 0.78304\n",
      "train loss in 3 epoch in 12400 batch: 1.58757\n",
      "val loss in 3 epoch: 0.78944\n",
      "train loss in 3 epoch in 12600 batch: 1.24303\n",
      "val loss in 3 epoch: 0.77722\n",
      "train loss in 3 epoch in 12800 batch: 1.17492\n",
      "val loss in 3 epoch: 0.78407\n",
      "train loss in 3 epoch in 13000 batch: 0.24452\n",
      "val loss in 3 epoch: 0.77571\n",
      "train loss in 3 epoch in 13200 batch: 1.49440\n",
      "val loss in 3 epoch: 0.78237\n",
      "train loss in 3 epoch in 13400 batch: 1.02063\n",
      "val loss in 3 epoch: 0.77663\n",
      "train loss in 3 epoch in 13600 batch: 0.30545\n",
      "val loss in 3 epoch: 0.78161\n",
      "train loss in 3 epoch in 13800 batch: 0.64307\n",
      "val loss in 3 epoch: 0.78012\n",
      "train loss in 3 epoch in 14000 batch: 0.98531\n",
      "val loss in 3 epoch: 0.78161\n",
      "train loss in 3 epoch in 14200 batch: 0.33424\n",
      "val loss in 3 epoch: 0.78850\n",
      "train loss in 3 epoch in 14400 batch: 0.87617\n",
      "val loss in 3 epoch: 0.78714\n",
      "train loss in 3 epoch in 14600 batch: 0.73393\n",
      "val loss in 3 epoch: 0.78146\n",
      "train loss in 3 epoch in 14800 batch: 1.01213\n",
      "val loss in 3 epoch: 0.77765\n",
      "train loss in 3 epoch in 15000 batch: 0.94234\n",
      "val loss in 3 epoch: 0.77557\n",
      "train loss in 3 epoch in 15200 batch: 0.87775\n",
      "val loss in 3 epoch: 0.77328\n",
      "train loss in 3 epoch in 15400 batch: 0.66132\n",
      "val loss in 3 epoch: 0.77677\n",
      "train loss in 3 epoch in 15600 batch: 0.72355\n",
      "val loss in 3 epoch: 0.77784\n",
      "train loss in 3 epoch in 15800 batch: 0.94691\n",
      "val loss in 3 epoch: 0.77830\n",
      "train loss in 3 epoch in 16000 batch: 1.46452\n",
      "val loss in 3 epoch: 0.77600\n",
      "train loss in 3 epoch in 16200 batch: 0.58592\n",
      "val loss in 3 epoch: 0.78140\n",
      "train loss in 4 epoch in 200 batch: 1.05041\n",
      "val loss in 4 epoch: 0.82543\n",
      "train loss in 4 epoch in 400 batch: 0.22963\n",
      "val loss in 4 epoch: 0.78060\n",
      "train loss in 4 epoch in 600 batch: 0.98273\n",
      "val loss in 4 epoch: 0.77368\n",
      "train loss in 4 epoch in 800 batch: 0.33804\n",
      "val loss in 4 epoch: 0.77871\n",
      "train loss in 4 epoch in 1000 batch: 0.53580\n",
      "val loss in 4 epoch: 0.78330\n",
      "train loss in 4 epoch in 1200 batch: 0.21825\n",
      "val loss in 4 epoch: 0.77375\n",
      "train loss in 4 epoch in 1400 batch: 0.71726\n",
      "val loss in 4 epoch: 0.77765\n",
      "train loss in 4 epoch in 1600 batch: 0.03160\n",
      "val loss in 4 epoch: 0.78099\n",
      "train loss in 4 epoch in 1800 batch: 1.12706\n",
      "val loss in 4 epoch: 0.77544\n",
      "train loss in 4 epoch in 2000 batch: 1.23931\n",
      "val loss in 4 epoch: 0.77872\n",
      "train loss in 4 epoch in 2200 batch: 0.06206\n",
      "val loss in 4 epoch: 0.79887\n",
      "train loss in 4 epoch in 2400 batch: 0.80924\n",
      "val loss in 4 epoch: 0.78796\n",
      "train loss in 4 epoch in 2600 batch: 0.75606\n",
      "val loss in 4 epoch: 0.77709\n",
      "train loss in 4 epoch in 2800 batch: 1.06136\n",
      "val loss in 4 epoch: 0.77514\n",
      "train loss in 4 epoch in 3000 batch: 0.76393\n",
      "val loss in 4 epoch: 0.77393\n",
      "train loss in 4 epoch in 3200 batch: 0.61072\n",
      "val loss in 4 epoch: 0.77139\n",
      "train loss in 4 epoch in 3400 batch: 0.93811\n",
      "val loss in 4 epoch: 0.78633\n",
      "train loss in 4 epoch in 3600 batch: 1.22211\n",
      "val loss in 4 epoch: 0.77284\n",
      "train loss in 4 epoch in 3800 batch: 0.57059\n",
      "val loss in 4 epoch: 0.77795\n",
      "train loss in 4 epoch in 4000 batch: 0.52382\n",
      "val loss in 4 epoch: 0.77302\n",
      "train loss in 4 epoch in 4200 batch: 0.49179\n",
      "val loss in 4 epoch: 0.77220\n",
      "train loss in 4 epoch in 4400 batch: 0.02737\n",
      "val loss in 4 epoch: 0.78816\n",
      "train loss in 4 epoch in 4600 batch: 0.42388\n",
      "val loss in 4 epoch: 0.78449\n",
      "train loss in 4 epoch in 4800 batch: 0.63186\n",
      "val loss in 4 epoch: 0.77285\n",
      "train loss in 4 epoch in 5000 batch: 0.66584\n",
      "val loss in 4 epoch: 0.77501\n",
      "train loss in 4 epoch in 5200 batch: 0.72363\n",
      "val loss in 4 epoch: 0.77145\n",
      "train loss in 4 epoch in 5400 batch: 0.79251\n",
      "val loss in 4 epoch: 0.77392\n",
      "train loss in 4 epoch in 5600 batch: 0.65401\n",
      "val loss in 4 epoch: 0.76807\n",
      "train loss in 4 epoch in 5800 batch: 0.41114\n",
      "val loss in 4 epoch: 0.76966\n",
      "train loss in 4 epoch in 6000 batch: 0.69105\n",
      "val loss in 4 epoch: 0.77067\n",
      "train loss in 4 epoch in 6200 batch: 0.53094\n",
      "val loss in 4 epoch: 0.77484\n",
      "train loss in 4 epoch in 6400 batch: 0.68940\n",
      "val loss in 4 epoch: 0.77727\n",
      "train loss in 4 epoch in 6600 batch: 0.64763\n",
      "val loss in 4 epoch: 0.77505\n",
      "train loss in 4 epoch in 6800 batch: 0.24640\n",
      "val loss in 4 epoch: 0.77290\n",
      "train loss in 4 epoch in 7000 batch: 0.77200\n",
      "val loss in 4 epoch: 0.77726\n",
      "train loss in 4 epoch in 7200 batch: 0.87623\n",
      "val loss in 4 epoch: 0.77919\n",
      "train loss in 4 epoch in 7400 batch: 0.54666\n",
      "val loss in 4 epoch: 0.77950\n",
      "train loss in 4 epoch in 7600 batch: 0.22026\n",
      "val loss in 4 epoch: 0.77382\n",
      "train loss in 4 epoch in 7800 batch: 0.80141\n",
      "val loss in 4 epoch: 0.77015\n",
      "train loss in 4 epoch in 8000 batch: 0.33268\n",
      "val loss in 4 epoch: 0.82371\n",
      "train loss in 4 epoch in 8200 batch: 1.41426\n",
      "val loss in 4 epoch: 0.77036\n",
      "train loss in 4 epoch in 8400 batch: 0.76224\n",
      "val loss in 4 epoch: 0.78366\n",
      "train loss in 4 epoch in 8600 batch: 1.37721\n",
      "val loss in 4 epoch: 0.77231\n",
      "train loss in 4 epoch in 8800 batch: 0.77657\n",
      "val loss in 4 epoch: 0.77379\n",
      "train loss in 4 epoch in 9000 batch: 0.43012\n",
      "val loss in 4 epoch: 0.77731\n",
      "train loss in 4 epoch in 9200 batch: 1.17217\n",
      "val loss in 4 epoch: 0.77271\n",
      "train loss in 4 epoch in 9400 batch: 1.37293\n",
      "val loss in 4 epoch: 0.77203\n",
      "train loss in 4 epoch in 9600 batch: 0.34635\n",
      "val loss in 4 epoch: 0.77143\n",
      "train loss in 4 epoch in 9800 batch: 0.25559\n",
      "val loss in 4 epoch: 0.77420\n",
      "train loss in 4 epoch in 10000 batch: 0.67605\n",
      "val loss in 4 epoch: 0.77217\n",
      "train loss in 4 epoch in 10200 batch: 0.45613\n",
      "val loss in 4 epoch: 0.77359\n",
      "train loss in 4 epoch in 10400 batch: 0.11214\n",
      "val loss in 4 epoch: 0.77784\n",
      "train loss in 4 epoch in 10600 batch: 0.30022\n",
      "val loss in 4 epoch: 0.78144\n",
      "train loss in 4 epoch in 10800 batch: 0.35606\n",
      "val loss in 4 epoch: 0.77778\n",
      "train loss in 4 epoch in 11000 batch: 0.65690\n",
      "val loss in 4 epoch: 0.77586\n",
      "train loss in 4 epoch in 11200 batch: 0.52064\n",
      "val loss in 4 epoch: 0.77557\n",
      "train loss in 4 epoch in 11400 batch: 1.11289\n",
      "val loss in 4 epoch: 0.77159\n",
      "train loss in 4 epoch in 11600 batch: 0.90597\n",
      "val loss in 4 epoch: 0.76810\n",
      "train loss in 4 epoch in 11800 batch: 1.05871\n",
      "val loss in 4 epoch: 0.77414\n",
      "train loss in 4 epoch in 12000 batch: 0.25952\n",
      "val loss in 4 epoch: 0.77933\n",
      "train loss in 4 epoch in 12200 batch: 0.62509\n",
      "val loss in 4 epoch: 0.76634\n",
      "train loss in 4 epoch in 12400 batch: 0.23159\n",
      "val loss in 4 epoch: 0.79307\n",
      "train loss in 4 epoch in 12600 batch: 0.91174\n",
      "val loss in 4 epoch: 0.76836\n",
      "train loss in 4 epoch in 12800 batch: 0.77798\n",
      "val loss in 4 epoch: 0.77356\n",
      "train loss in 4 epoch in 13000 batch: 1.51096\n",
      "val loss in 4 epoch: 0.77289\n",
      "train loss in 4 epoch in 13200 batch: 1.38243\n",
      "val loss in 4 epoch: 0.77366\n",
      "train loss in 4 epoch in 13400 batch: 1.26487\n",
      "val loss in 4 epoch: 0.77459\n",
      "train loss in 4 epoch in 13600 batch: 0.88099\n",
      "val loss in 4 epoch: 0.76976\n",
      "train loss in 4 epoch in 13800 batch: 0.69382\n",
      "val loss in 4 epoch: 0.77175\n",
      "train loss in 4 epoch in 14000 batch: 0.94307\n",
      "val loss in 4 epoch: 0.77207\n",
      "train loss in 4 epoch in 14200 batch: 1.39545\n",
      "val loss in 4 epoch: 0.76393\n",
      "train loss in 4 epoch in 14400 batch: 0.86008\n",
      "val loss in 4 epoch: 0.76922\n",
      "train loss in 4 epoch in 14600 batch: 0.88813\n",
      "val loss in 4 epoch: 0.76381\n",
      "train loss in 4 epoch in 14800 batch: 0.21542\n",
      "val loss in 4 epoch: 0.76140\n",
      "train loss in 4 epoch in 15000 batch: 0.37110\n",
      "val loss in 4 epoch: 0.76980\n",
      "train loss in 4 epoch in 15200 batch: 1.09029\n",
      "val loss in 4 epoch: 0.76637\n",
      "train loss in 4 epoch in 15400 batch: 0.70118\n",
      "val loss in 4 epoch: 0.76759\n",
      "train loss in 4 epoch in 15600 batch: 0.37791\n",
      "val loss in 4 epoch: 0.77495\n",
      "train loss in 4 epoch in 15800 batch: 0.37350\n",
      "val loss in 4 epoch: 0.77159\n",
      "train loss in 4 epoch in 16000 batch: 0.94305\n",
      "val loss in 4 epoch: 0.76524\n",
      "train loss in 4 epoch in 16200 batch: 0.62913\n",
      "val loss in 4 epoch: 0.76910\n"
     ]
    }
   ],
   "source": [
    "# v19\n",
    "INPUT_F = 36\n",
    "H = 256\n",
    "DROP_P = 0.5\n",
    "\n",
    "model = NNWithAttention(INPUT_F, DROP_P, H)\n",
    "\n",
    "def policy_gradient(logits_batch, rewards_batch, actions_batch, _):\n",
    "    loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    logits_batch = torch.reshape(logits_batch, (-1, 100))\n",
    "    probs = torch.nn.Softmax(dim=1)(logits_batch)\n",
    "    entropy = -torch.mean(probs * torch.log(probs))\n",
    "    policy = loss(logits_batch, actions_batch)\n",
    "    out = torch.mean(policy * rewards_batch) + entropy\n",
    "    return out\n",
    "\n",
    "learn(nagiss_top_10[0], nagiss_top_10[1], model,\n",
    "      freq=200, batch_size=16,lr=1e-4, epochs=4, criterion=policy_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "662.0 665.0 tmp/b_0.5231521158081155.py tmp/b_0.6374564363717186.py\n",
      "573.0 581.0 tmp/b_0.5231521158081155.py tmp/b_0.6374564363717186.py\n",
      "632.0 635.0 tmp/b_0.5231521158081155.py tmp/b_0.6374564363717186.py\n",
      "635.0 648.0 tmp/b_0.5231521158081155.py tmp/b_0.6374564363717186.py\n",
      "612.0 584.0 tmp/b_0.5231521158081155.py tmp/b_0.6374564363717186.py\n",
      "577.0 600.0 tmp/b_0.5231521158081155.py tmp/b_0.6374564363717186.py\n",
      "679.0 623.0 tmp/b_0.5231521158081155.py tmp/b_0.6374564363717186.py\n",
      "655.0 630.0 tmp/b_0.5231521158081155.py tmp/b_0.6374564363717186.py\n",
      "579.0 649.0 tmp/b_0.5231521158081155.py tmp/b_0.6374564363717186.py\n",
      "636.0 622.0 tmp/b_0.5231521158081155.py tmp/b_0.6374564363717186.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9765543186290638,\n",
       " 0.3,\n",
       " 32.28017967731902,\n",
       " 0.4,\n",
       " 'tmp/b_0.5231521158081155.py',\n",
       " 'tmp/b_0.6374564363717186.py',\n",
       " array([662., 573., 632., 635., 612., 577., 679., 655., 579., 636.]),\n",
       " array([665., 581., 635., 648., 584., 600., 623., 630., 649., 622.]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v19\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,False\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=False\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617.0 638.0 tmp/b_0.06293206958092812.py tmp/b_0.7981795034322536.py\n",
      "646.0 628.0 tmp/b_0.06293206958092812.py tmp/b_0.7981795034322536.py\n",
      "640.0 563.0 tmp/b_0.06293206958092812.py tmp/b_0.7981795034322536.py\n",
      "590.0 580.0 tmp/b_0.06293206958092812.py tmp/b_0.7981795034322536.py\n",
      "606.0 576.0 tmp/b_0.06293206958092812.py tmp/b_0.7981795034322536.py\n",
      "677.0 633.0 tmp/b_0.06293206958092812.py tmp/b_0.7981795034322536.py\n",
      "642.0 576.0 tmp/b_0.06293206958092812.py tmp/b_0.7981795034322536.py\n",
      "652.0 608.0 tmp/b_0.06293206958092812.py tmp/b_0.7981795034322536.py\n",
      "592.0 544.0 tmp/b_0.06293206958092812.py tmp/b_0.7981795034322536.py\n",
      "660.0 661.0 tmp/b_0.06293206958092812.py tmp/b_0.7981795034322536.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0005469605380743128,\n",
       " 31.5,\n",
       " 28.817529387509957,\n",
       " 0.8,\n",
       " 'tmp/b_0.06293206958092812.py',\n",
       " 'tmp/b_0.7981795034322536.py',\n",
       " array([617., 646., 640., 590., 606., 677., 642., 652., 592., 660.]),\n",
       " array([638., 628., 563., 580., 576., 633., 576., 608., 544., 661.]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v19 vs neural with mean\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,False\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### next data iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = 18836001  # best up to 20210107\n",
    "utils.web.download_all_sessions_for_submission(sub, 'sessions/20210107/{}'.format(sub), delta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sessions/20210107/18836001/ ['sessions/20210107/18836001/9709690.json', 'sessions/20210107/18836001/8531135.json', 'sessions/20210107/18836001/10067762.json', 'sessions/20210107/18836001/8528656.json', 'sessions/20210107/18836001/9521864.json', 'sessions/20210107/18836001/8526170.json', 'sessions/20210107/18836001/9434139.json', 'sessions/20210107/18836001/9886335.json', 'sessions/20210107/18836001/8918337.json', 'sessions/20210107/18836001/8537521.json', 'sessions/20210107/18836001/8533974.json', 'sessions/20210107/18836001/8792232.json', 'sessions/20210107/18836001/9354441.json', 'sessions/20210107/18836001/9975622.json', 'sessions/20210107/18836001/10031677.json', 'sessions/20210107/18836001/8529362.json', 'sessions/20210107/18836001/9321023.json', 'sessions/20210107/18836001/8839728.json', 'sessions/20210107/18836001/8533305.json', 'sessions/20210107/18836001/10049893.json', 'sessions/20210107/18836001/8603318.json', 'sessions/20210107/18836001/10163463.json', 'sessions/20210107/18836001/8528334.json', 'sessions/20210107/18836001/9674237.json', 'sessions/20210107/18836001/9404652.json', 'sessions/20210107/18836001/9530595.json', 'sessions/20210107/18836001/9068126.json', 'sessions/20210107/18836001/8529031.json', 'sessions/20210107/18836001/8887869.json', 'sessions/20210107/18836001/10102921.json', 'sessions/20210107/18836001/9237884.json', 'sessions/20210107/18836001/9565776.json', 'sessions/20210107/18836001/8534434.json', 'sessions/20210107/18836001/9035151.json', 'sessions/20210107/18836001/8523349.json', 'sessions/20210107/18836001/8824253.json', 'sessions/20210107/18836001/8584654.json', 'sessions/20210107/18836001/9728908.json', 'sessions/20210107/18836001/9018838.json', 'sessions/20210107/18836001/9903857.json', 'sessions/20210107/18836001/8564969.json', 'sessions/20210107/18836001/9547060.json', 'sessions/20210107/18836001/9437283.json', 'sessions/20210107/18836001/8536479.json', 'sessions/20210107/18836001/10219485.json', 'sessions/20210107/18836001/9762946.json', 'sessions/20210107/18836001/8536838.json', 'sessions/20210107/18836001/8527950.json', 'sessions/20210107/18836001/8614789.json', 'sessions/20210107/18836001/8534700.json', 'sessions/20210107/18836001/8523704.json', 'sessions/20210107/18836001/8537881.json', 'sessions/20210107/18836001/8686508.json', 'sessions/20210107/18836001/8531847.json', 'sessions/20210107/18836001/9797048.json', 'sessions/20210107/18836001/8717731.json', 'sessions/20210107/18836001/8625172.json', 'sessions/20210107/18836001/8527233.json', 'sessions/20210107/18836001/8536137.json', 'sessions/20210107/18836001/8524409.json', 'sessions/20210107/18836001/8661105.json', 'sessions/20210107/18836001/10238585.json', 'sessions/20210107/18836001/9254949.json', 'sessions/20210107/18836001/8532226.json', 'sessions/20210107/18836001/9833215.json', 'sessions/20210107/18836001/9084422.json', 'sessions/20210107/18836001/8966201.json', 'sessions/20210107/18836001/9221235.json', 'sessions/20210107/18836001/8535409.json', 'sessions/20210107/18836001/8527623.json', 'sessions/20210107/18836001/9116168.json', 'sessions/20210107/18836001/8982647.json', 'sessions/20210107/18836001/8526531.json', 'sessions/20210107/18836001/9289202.json', 'sessions/20210107/18836001/8733832.json', 'sessions/20210107/18836001/9779370.json', 'sessions/20210107/18836001/9422459.json', 'sessions/20210107/18836001/9338053.json', 'sessions/20210107/18836001/8524054.json', 'sessions/20210107/18836001/8547595.json', 'sessions/20210107/18836001/8808064.json', 'sessions/20210107/18836001/8525468.json', 'sessions/20210107/18836001/9745730.json', 'sessions/20210107/18836001/9170333.json', 'sessions/20210107/18836001/8903070.json', 'sessions/20210107/18836001/9153392.json', 'sessions/20210107/18836001/8648564.json', 'sessions/20210107/18836001/8949672.json', 'sessions/20210107/18836001/9957891.json', 'sessions/20210107/18836001/9204676.json', 'sessions/20210107/18836001/8533638.json', 'sessions/20210107/18836001/9051720.json', 'sessions/20210107/18836001/8530517.json', 'sessions/20210107/18836001/9270960.json', 'sessions/20210107/18836001/8529761.json', 'sessions/20210107/18836001/8554699.json', 'sessions/20210107/18836001/8526908.json', 'sessions/20210107/18836001/8762977.json', 'sessions/20210107/18836001/8535762.json', 'sessions/20210107/18836001/8528404.json', 'sessions/20210107/18836001/8574669.json', 'sessions/20210107/18836001/10181706.json', 'sessions/20210107/18836001/9868946.json', 'sessions/20210107/18836001/10013870.json', 'sessions/20210107/18836001/10133648.json', 'sessions/20210107/18836001/9370931.json', 'sessions/20210107/18836001/8532571.json', 'sessions/20210107/18836001/8524763.json', 'sessions/20210107/18836001/8933632.json', 'sessions/20210107/18836001/8535036.json', 'sessions/20210107/18836001/8532966.json', 'sessions/20210107/18836001/8531537.json', 'sessions/20210107/18836001/9135431.json', 'sessions/20210107/18836001/9851112.json', 'sessions/20210107/18836001/8537158.json', 'sessions/20210107/18836001/9939515.json', 'sessions/20210107/18836001/9600241.json', 'sessions/20210107/18836001/9638793.json', 'sessions/20210107/18836001/8749149.json', 'sessions/20210107/18836001/10144467.json', 'sessions/20210107/18836001/8591151.json', 'sessions/20210107/18836001/9921646.json', 'sessions/20210107/18836001/8673121.json', 'sessions/20210107/18836001/8530783.json', 'sessions/20210107/18836001/9629966.json', 'sessions/20210107/18836001/9305144.json', 'sessions/20210107/18836001/10085572.json', 'sessions/20210107/18836001/9486809.json', 'sessions/20210107/18836001/8855606.json', 'sessions/20210107/18836001/8522776.json', 'sessions/20210107/18836001/9994443.json', 'sessions/20210107/18836001/10200481.json', 'sessions/20210107/18836001/9691350.json', 'sessions/20210107/18836001/9503464.json', 'sessions/20210107/18836001/9612735.json', 'sessions/20210107/18836001/8530069.json', 'sessions/20210107/18836001/8525115.json', 'sessions/20210107/18836001/8542244.json', 'sessions/20210107/18836001/8776904.json', 'sessions/20210107/18836001/8635880.json', 'sessions/20210107/18836001/9583375.json', 'sessions/20210107/18836001/8702096.json', 'sessions/20210107/18836001/9453089.json', 'sessions/20210107/18836001/9657200.json', 'sessions/20210107/18836001/9388173.json', 'sessions/20210107/18836001/9001178.json', 'sessions/20210107/18836001/9470045.json', 'sessions/20210107/18836001/9188198.json', 'sessions/20210107/18836001/8525821.json', 'sessions/20210107/18836001/8871636.json', 'sessions/20210107/18836001/10121609.json', 'sessions/20210107/18836001/9099994.json', 'sessions/20210107/18836001/9815289.json']\n",
      "error while parse session sessions/20210107/18836001/8824253.json : \n",
      "error while parse session sessions/20210107/18836001/8591151.json : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [05:55<00:00,  2.93s/it]\n",
      "100%|██████████| 30/30 [00:51<00:00,  1.72s/it]\n"
     ]
    }
   ],
   "source": [
    "nagiss_v2 = snowden.collect_dataset_from_dir(\n",
    "    neural_agent_class,\n",
    "    'sessions/20210107/18836001/',\n",
    "    val_ratio=2/10,\n",
    "    n_jobs=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((261869, 100, 36), (241879, 3600))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nagiss_top_10[0].X.shape, nagiss_v2[0].X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_dataset(nagiss_v2[0], nagiss_v2[1], 36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = utils.snowden.Dataset(None, None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.X = np.concatenate([nagiss_top_10[0].X, nagiss_v2[0].X], axis=0)\n",
    "data.y = np.concatenate([nagiss_top_10[0].y, nagiss_v2[0].y], axis=0)\n",
    "data.actions = np.concatenate([nagiss_top_10[0].actions, nagiss_v2[0].actions], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(503748, 100, 36)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss in 1 epoch in 200 batch: 0.64893\n",
      "val loss in 1 epoch: 1.01453\n",
      "train loss in 1 epoch in 400 batch: 0.34390\n",
      "val loss in 1 epoch: 0.91446\n",
      "train loss in 1 epoch in 600 batch: 0.04847\n",
      "val loss in 1 epoch: 0.90182\n",
      "train loss in 1 epoch in 800 batch: 0.60163\n",
      "val loss in 1 epoch: 0.89363\n",
      "train loss in 1 epoch in 1000 batch: 0.27664\n",
      "val loss in 1 epoch: 0.88565\n",
      "train loss in 1 epoch in 1200 batch: 0.67074\n",
      "val loss in 1 epoch: 0.87791\n",
      "train loss in 1 epoch in 1400 batch: 0.85610\n",
      "val loss in 1 epoch: 0.86868\n",
      "train loss in 1 epoch in 1600 batch: 1.36075\n",
      "val loss in 1 epoch: 0.86483\n",
      "train loss in 1 epoch in 1800 batch: 1.58412\n",
      "val loss in 1 epoch: 0.85623\n",
      "train loss in 1 epoch in 2000 batch: 0.42443\n",
      "val loss in 1 epoch: 0.85023\n",
      "train loss in 1 epoch in 2200 batch: 0.86434\n",
      "val loss in 1 epoch: 0.84689\n",
      "train loss in 1 epoch in 2400 batch: 0.65696\n",
      "val loss in 1 epoch: 0.84276\n",
      "train loss in 1 epoch in 2600 batch: 0.37096\n",
      "val loss in 1 epoch: 0.84024\n",
      "train loss in 1 epoch in 2800 batch: 0.20658\n",
      "val loss in 1 epoch: 0.83790\n",
      "train loss in 1 epoch in 3000 batch: 0.88822\n",
      "val loss in 1 epoch: 0.83621\n",
      "train loss in 1 epoch in 3200 batch: 0.52901\n",
      "val loss in 1 epoch: 0.83566\n",
      "train loss in 1 epoch in 3400 batch: 0.83387\n",
      "val loss in 1 epoch: 0.83474\n",
      "train loss in 1 epoch in 3600 batch: 0.33008\n",
      "val loss in 1 epoch: 0.83210\n",
      "train loss in 1 epoch in 3800 batch: 0.80605\n",
      "val loss in 1 epoch: 0.83218\n",
      "train loss in 1 epoch in 4000 batch: 0.95926\n",
      "val loss in 1 epoch: 0.83628\n",
      "train loss in 1 epoch in 4200 batch: 1.25295\n",
      "val loss in 1 epoch: 0.83198\n",
      "train loss in 1 epoch in 4400 batch: 0.88050\n",
      "val loss in 1 epoch: 0.83208\n",
      "train loss in 1 epoch in 4600 batch: 1.18215\n",
      "val loss in 1 epoch: 0.83149\n",
      "train loss in 1 epoch in 4800 batch: 0.77300\n",
      "val loss in 1 epoch: 0.83139\n",
      "train loss in 1 epoch in 5000 batch: 0.92726\n",
      "val loss in 1 epoch: 0.82836\n",
      "train loss in 1 epoch in 5200 batch: 1.12667\n",
      "val loss in 1 epoch: 0.82787\n",
      "train loss in 1 epoch in 5400 batch: 1.22410\n",
      "val loss in 1 epoch: 0.82928\n",
      "train loss in 1 epoch in 5600 batch: 0.87672\n",
      "val loss in 1 epoch: 0.82624\n",
      "train loss in 1 epoch in 5800 batch: 0.15748\n",
      "val loss in 1 epoch: 0.82540\n",
      "train loss in 1 epoch in 6000 batch: 0.53558\n",
      "val loss in 1 epoch: 0.82529\n",
      "train loss in 1 epoch in 6200 batch: 0.72581\n",
      "val loss in 1 epoch: 0.82521\n",
      "train loss in 1 epoch in 6400 batch: 0.77152\n",
      "val loss in 1 epoch: 0.82667\n",
      "train loss in 1 epoch in 6600 batch: 0.90532\n",
      "val loss in 1 epoch: 0.82522\n",
      "train loss in 1 epoch in 6800 batch: 0.59366\n",
      "val loss in 1 epoch: 0.82513\n",
      "train loss in 1 epoch in 7000 batch: 0.25975\n",
      "val loss in 1 epoch: 0.82689\n",
      "train loss in 1 epoch in 7200 batch: 0.02878\n",
      "val loss in 1 epoch: 0.82634\n",
      "train loss in 1 epoch in 7400 batch: 0.57948\n",
      "val loss in 1 epoch: 0.82451\n",
      "train loss in 1 epoch in 7600 batch: 1.62373\n",
      "val loss in 1 epoch: 0.82365\n",
      "train loss in 1 epoch in 7800 batch: 0.57973\n",
      "val loss in 1 epoch: 0.82361\n",
      "train loss in 1 epoch in 8000 batch: 1.01359\n",
      "val loss in 1 epoch: 0.82269\n",
      "train loss in 1 epoch in 8200 batch: 1.82927\n",
      "val loss in 1 epoch: 0.82248\n",
      "train loss in 1 epoch in 8400 batch: 0.97648\n",
      "val loss in 1 epoch: 0.82212\n",
      "train loss in 1 epoch in 8600 batch: 0.94535\n",
      "val loss in 1 epoch: 0.82230\n",
      "train loss in 1 epoch in 8800 batch: 1.94831\n",
      "val loss in 1 epoch: 0.82113\n",
      "train loss in 1 epoch in 9000 batch: 1.47716\n",
      "val loss in 1 epoch: 0.82141\n",
      "train loss in 1 epoch in 9200 batch: 0.51433\n",
      "val loss in 1 epoch: 0.82132\n",
      "train loss in 1 epoch in 9400 batch: 1.24337\n",
      "val loss in 1 epoch: 0.82294\n",
      "train loss in 1 epoch in 9600 batch: 0.50458\n",
      "val loss in 1 epoch: 0.82522\n",
      "train loss in 1 epoch in 9800 batch: 1.83616\n",
      "val loss in 1 epoch: 0.81996\n",
      "train loss in 1 epoch in 10000 batch: 0.98619\n",
      "val loss in 1 epoch: 0.81956\n",
      "train loss in 1 epoch in 10200 batch: 0.02479\n",
      "val loss in 1 epoch: 0.81996\n",
      "train loss in 1 epoch in 10400 batch: 0.56350\n",
      "val loss in 1 epoch: 0.81945\n",
      "train loss in 1 epoch in 10600 batch: 0.95748\n",
      "val loss in 1 epoch: 0.82021\n",
      "train loss in 1 epoch in 10800 batch: 0.30544\n",
      "val loss in 1 epoch: 0.82002\n",
      "train loss in 1 epoch in 11000 batch: 0.85524\n",
      "val loss in 1 epoch: 0.81936\n",
      "train loss in 1 epoch in 11200 batch: 0.72842\n",
      "val loss in 1 epoch: 0.81916\n",
      "train loss in 1 epoch in 11400 batch: 0.44219\n",
      "val loss in 1 epoch: 0.81932\n",
      "train loss in 1 epoch in 11600 batch: 0.48413\n",
      "val loss in 1 epoch: 0.81764\n",
      "train loss in 1 epoch in 11800 batch: 0.70170\n",
      "val loss in 1 epoch: 0.81888\n",
      "train loss in 1 epoch in 12000 batch: 0.49003\n",
      "val loss in 1 epoch: 0.81753\n",
      "train loss in 1 epoch in 12200 batch: 1.14314\n",
      "val loss in 1 epoch: 0.82206\n",
      "train loss in 1 epoch in 12400 batch: 0.61169\n",
      "val loss in 1 epoch: 0.81667\n",
      "train loss in 1 epoch in 12600 batch: 1.20665\n",
      "val loss in 1 epoch: 0.81629\n",
      "train loss in 1 epoch in 12800 batch: 0.95744\n",
      "val loss in 1 epoch: 0.81865\n",
      "train loss in 1 epoch in 13000 batch: 0.33467\n",
      "val loss in 1 epoch: 0.81933\n",
      "train loss in 1 epoch in 13200 batch: 0.64644\n",
      "val loss in 1 epoch: 0.81665\n",
      "train loss in 1 epoch in 13400 batch: 0.45031\n",
      "val loss in 1 epoch: 0.81742\n",
      "train loss in 1 epoch in 13600 batch: 0.26584\n",
      "val loss in 1 epoch: 0.81593\n",
      "train loss in 1 epoch in 13800 batch: 0.58635\n",
      "val loss in 1 epoch: 0.81725\n",
      "train loss in 1 epoch in 14000 batch: 0.59211\n",
      "val loss in 1 epoch: 0.81570\n",
      "train loss in 1 epoch in 14200 batch: 0.23243\n",
      "val loss in 1 epoch: 0.81733\n",
      "train loss in 1 epoch in 14400 batch: 0.69870\n",
      "val loss in 1 epoch: 0.81536\n",
      "train loss in 1 epoch in 14600 batch: 0.90387\n",
      "val loss in 1 epoch: 0.81585\n",
      "train loss in 1 epoch in 14800 batch: 1.76286\n",
      "val loss in 1 epoch: 0.81603\n",
      "train loss in 1 epoch in 15000 batch: 1.35755\n",
      "val loss in 1 epoch: 0.81426\n",
      "train loss in 1 epoch in 15200 batch: 1.21091\n",
      "val loss in 1 epoch: 0.81419\n",
      "train loss in 1 epoch in 15400 batch: 0.31522\n",
      "val loss in 1 epoch: 0.81348\n",
      "train loss in 1 epoch in 15600 batch: 0.48336\n",
      "val loss in 1 epoch: 0.81391\n",
      "train loss in 1 epoch in 15800 batch: 0.79670\n",
      "val loss in 1 epoch: 0.81600\n",
      "train loss in 1 epoch in 16000 batch: 2.08060\n",
      "val loss in 1 epoch: 0.81343\n",
      "train loss in 1 epoch in 16200 batch: 1.21414\n",
      "val loss in 1 epoch: 0.81624\n",
      "train loss in 1 epoch in 16400 batch: 1.41843\n",
      "val loss in 1 epoch: 0.81416\n",
      "train loss in 1 epoch in 16600 batch: 0.92181\n",
      "val loss in 1 epoch: 0.81274\n",
      "train loss in 1 epoch in 16800 batch: 0.36296\n",
      "val loss in 1 epoch: 0.81639\n",
      "train loss in 1 epoch in 17000 batch: 0.75966\n",
      "val loss in 1 epoch: 0.81389\n",
      "train loss in 1 epoch in 17200 batch: 0.73430\n",
      "val loss in 1 epoch: 0.81160\n",
      "train loss in 1 epoch in 17400 batch: 1.00329\n",
      "val loss in 1 epoch: 0.81296\n",
      "train loss in 1 epoch in 17600 batch: 1.12482\n",
      "val loss in 1 epoch: 0.81263\n",
      "train loss in 1 epoch in 17800 batch: 0.52511\n",
      "val loss in 1 epoch: 0.81195\n",
      "train loss in 1 epoch in 18000 batch: 0.82997\n",
      "val loss in 1 epoch: 0.81192\n",
      "train loss in 1 epoch in 18200 batch: 0.16669\n",
      "val loss in 1 epoch: 0.81062\n",
      "train loss in 1 epoch in 18400 batch: 0.27893\n",
      "val loss in 1 epoch: 0.81038\n",
      "train loss in 1 epoch in 18600 batch: 0.94822\n",
      "val loss in 1 epoch: 0.81036\n",
      "train loss in 1 epoch in 18800 batch: 1.13237\n",
      "val loss in 1 epoch: 0.80976\n",
      "train loss in 1 epoch in 19000 batch: 0.71637\n",
      "val loss in 1 epoch: 0.81022\n",
      "train loss in 1 epoch in 19200 batch: 0.83199\n",
      "val loss in 1 epoch: 0.81395\n",
      "train loss in 1 epoch in 19400 batch: 1.03238\n",
      "val loss in 1 epoch: 0.81115\n",
      "train loss in 1 epoch in 19600 batch: 1.02453\n",
      "val loss in 1 epoch: 0.80950\n",
      "train loss in 1 epoch in 19800 batch: 0.83237\n",
      "val loss in 1 epoch: 0.80888\n",
      "train loss in 1 epoch in 20000 batch: 0.46027\n",
      "val loss in 1 epoch: 0.81068\n",
      "train loss in 1 epoch in 20200 batch: 0.40726\n",
      "val loss in 1 epoch: 0.81115\n",
      "train loss in 1 epoch in 20400 batch: 0.52863\n",
      "val loss in 1 epoch: 0.80970\n",
      "train loss in 1 epoch in 20600 batch: 1.02760\n",
      "val loss in 1 epoch: 0.80808\n",
      "train loss in 1 epoch in 20800 batch: 0.42255\n",
      "val loss in 1 epoch: 0.80841\n",
      "train loss in 1 epoch in 21000 batch: 0.34590\n",
      "val loss in 1 epoch: 0.81116\n",
      "train loss in 1 epoch in 21200 batch: 0.56396\n",
      "val loss in 1 epoch: 0.80765\n",
      "train loss in 1 epoch in 21400 batch: 0.93055\n",
      "val loss in 1 epoch: 0.80956\n",
      "train loss in 1 epoch in 21600 batch: 1.65217\n",
      "val loss in 1 epoch: 0.80970\n",
      "train loss in 1 epoch in 21800 batch: 1.60363\n",
      "val loss in 1 epoch: 0.80747\n",
      "train loss in 1 epoch in 22000 batch: 1.22341\n",
      "val loss in 1 epoch: 0.80987\n",
      "train loss in 1 epoch in 22200 batch: 0.24608\n",
      "val loss in 1 epoch: 0.80827\n",
      "train loss in 1 epoch in 22400 batch: 0.25733\n",
      "val loss in 1 epoch: 0.81348\n",
      "train loss in 1 epoch in 22600 batch: 0.34234\n",
      "val loss in 1 epoch: 0.80555\n",
      "train loss in 1 epoch in 22800 batch: 0.89757\n",
      "val loss in 1 epoch: 0.80574\n",
      "train loss in 1 epoch in 23000 batch: 1.22006\n",
      "val loss in 1 epoch: 0.80598\n",
      "train loss in 1 epoch in 23200 batch: 1.40670\n",
      "val loss in 1 epoch: 0.80587\n",
      "train loss in 1 epoch in 23400 batch: 1.44796\n",
      "val loss in 1 epoch: 0.80578\n",
      "train loss in 1 epoch in 23600 batch: 0.50646\n",
      "val loss in 1 epoch: 0.80837\n",
      "train loss in 1 epoch in 23800 batch: 1.04376\n",
      "val loss in 1 epoch: 0.80613\n",
      "train loss in 1 epoch in 24000 batch: 0.72090\n",
      "val loss in 1 epoch: 0.80688\n",
      "train loss in 1 epoch in 24200 batch: 0.87844\n",
      "val loss in 1 epoch: 0.80640\n",
      "train loss in 1 epoch in 24400 batch: 0.51577\n",
      "val loss in 1 epoch: 0.80529\n",
      "train loss in 1 epoch in 24600 batch: 0.57117\n",
      "val loss in 1 epoch: 0.80465\n",
      "train loss in 1 epoch in 24800 batch: 0.70968\n",
      "val loss in 1 epoch: 0.80463\n",
      "train loss in 1 epoch in 25000 batch: 0.83795\n",
      "val loss in 1 epoch: 0.80488\n",
      "train loss in 1 epoch in 25200 batch: 1.19495\n",
      "val loss in 1 epoch: 0.80425\n",
      "train loss in 1 epoch in 25400 batch: 0.65712\n",
      "val loss in 1 epoch: 0.80404\n",
      "train loss in 1 epoch in 25600 batch: 0.52008\n",
      "val loss in 1 epoch: 0.80555\n",
      "train loss in 1 epoch in 25800 batch: 0.17184\n",
      "val loss in 1 epoch: 0.80397\n",
      "train loss in 1 epoch in 26000 batch: 0.66422\n",
      "val loss in 1 epoch: 0.80319\n",
      "train loss in 1 epoch in 26200 batch: 1.34807\n",
      "val loss in 1 epoch: 0.80413\n",
      "train loss in 1 epoch in 26400 batch: 1.11830\n",
      "val loss in 1 epoch: 0.80354\n",
      "train loss in 1 epoch in 26600 batch: 0.53564\n",
      "val loss in 1 epoch: 0.80312\n",
      "train loss in 1 epoch in 26800 batch: 0.00834\n",
      "val loss in 1 epoch: 0.80387\n",
      "train loss in 1 epoch in 27000 batch: 1.19025\n",
      "val loss in 1 epoch: 0.80305\n",
      "train loss in 1 epoch in 27200 batch: 0.46013\n",
      "val loss in 1 epoch: 0.80401\n",
      "train loss in 1 epoch in 27400 batch: 0.45413\n",
      "val loss in 1 epoch: 0.80348\n",
      "train loss in 1 epoch in 27600 batch: 1.53917\n",
      "val loss in 1 epoch: 0.80725\n",
      "train loss in 1 epoch in 27800 batch: 0.53622\n",
      "val loss in 1 epoch: 0.80230\n",
      "train loss in 1 epoch in 28000 batch: 0.70409\n",
      "val loss in 1 epoch: 0.80208\n",
      "train loss in 1 epoch in 28200 batch: 0.34138\n",
      "val loss in 1 epoch: 0.80439\n",
      "train loss in 1 epoch in 28400 batch: 0.84153\n",
      "val loss in 1 epoch: 0.80291\n",
      "train loss in 1 epoch in 28600 batch: 0.59982\n",
      "val loss in 1 epoch: 0.80281\n",
      "train loss in 1 epoch in 28800 batch: 0.38852\n",
      "val loss in 1 epoch: 0.80121\n",
      "train loss in 1 epoch in 29000 batch: 1.26011\n",
      "val loss in 1 epoch: 0.80195\n",
      "train loss in 1 epoch in 29200 batch: 0.43163\n",
      "val loss in 1 epoch: 0.80293\n",
      "train loss in 1 epoch in 29400 batch: 1.00299\n",
      "val loss in 1 epoch: 0.80230\n",
      "train loss in 1 epoch in 29600 batch: 0.83946\n",
      "val loss in 1 epoch: 0.80091\n",
      "train loss in 1 epoch in 29800 batch: 1.19384\n",
      "val loss in 1 epoch: 0.80544\n",
      "train loss in 1 epoch in 30000 batch: 0.92379\n",
      "val loss in 1 epoch: 0.80122\n",
      "train loss in 1 epoch in 30200 batch: 1.13081\n",
      "val loss in 1 epoch: 0.80087\n",
      "train loss in 1 epoch in 30400 batch: 0.57241\n",
      "val loss in 1 epoch: 0.80042\n",
      "train loss in 1 epoch in 30600 batch: 0.49804\n",
      "val loss in 1 epoch: 0.80024\n",
      "train loss in 1 epoch in 30800 batch: 0.76049\n",
      "val loss in 1 epoch: 0.79975\n",
      "train loss in 1 epoch in 31000 batch: 1.08378\n",
      "val loss in 1 epoch: 0.80014\n",
      "train loss in 1 epoch in 31200 batch: 1.07262\n",
      "val loss in 1 epoch: 0.80410\n",
      "train loss in 1 epoch in 31400 batch: 1.10150\n",
      "val loss in 1 epoch: 0.79898\n",
      "train loss in 2 epoch in 200 batch: 1.08687\n",
      "val loss in 2 epoch: 0.80106\n",
      "train loss in 2 epoch in 400 batch: 0.70497\n",
      "val loss in 2 epoch: 0.79997\n",
      "train loss in 2 epoch in 600 batch: 0.46392\n",
      "val loss in 2 epoch: 0.80070\n",
      "train loss in 2 epoch in 800 batch: 1.18647\n",
      "val loss in 2 epoch: 0.79970\n",
      "train loss in 2 epoch in 1000 batch: 0.85301\n",
      "val loss in 2 epoch: 0.79898\n",
      "train loss in 2 epoch in 1200 batch: 0.67242\n",
      "val loss in 2 epoch: 0.79848\n",
      "train loss in 2 epoch in 1400 batch: 1.43645\n",
      "val loss in 2 epoch: 0.79807\n",
      "train loss in 2 epoch in 1600 batch: 1.22636\n",
      "val loss in 2 epoch: 0.79900\n",
      "train loss in 2 epoch in 1800 batch: 1.85223\n",
      "val loss in 2 epoch: 0.79792\n",
      "train loss in 2 epoch in 2000 batch: 1.17767\n",
      "val loss in 2 epoch: 0.79853\n",
      "train loss in 2 epoch in 2200 batch: 0.53282\n",
      "val loss in 2 epoch: 0.79703\n",
      "train loss in 2 epoch in 2400 batch: 0.50101\n",
      "val loss in 2 epoch: 0.79726\n",
      "train loss in 2 epoch in 2600 batch: 0.52602\n",
      "val loss in 2 epoch: 0.79730\n",
      "train loss in 2 epoch in 2800 batch: 0.20355\n",
      "val loss in 2 epoch: 0.79809\n",
      "train loss in 2 epoch in 3000 batch: 0.99462\n",
      "val loss in 2 epoch: 0.79841\n",
      "train loss in 2 epoch in 3200 batch: 0.14349\n",
      "val loss in 2 epoch: 0.79934\n",
      "train loss in 2 epoch in 3400 batch: 1.11409\n",
      "val loss in 2 epoch: 0.79868\n",
      "train loss in 2 epoch in 3600 batch: 0.83455\n",
      "val loss in 2 epoch: 0.79657\n",
      "train loss in 2 epoch in 3800 batch: 0.99582\n",
      "val loss in 2 epoch: 0.79856\n",
      "train loss in 2 epoch in 4000 batch: 1.01840\n",
      "val loss in 2 epoch: 0.79976\n",
      "train loss in 2 epoch in 4200 batch: 0.35224\n",
      "val loss in 2 epoch: 0.79736\n",
      "train loss in 2 epoch in 4400 batch: 0.84120\n",
      "val loss in 2 epoch: 0.79630\n",
      "train loss in 2 epoch in 4600 batch: 0.30224\n",
      "val loss in 2 epoch: 0.79972\n",
      "train loss in 2 epoch in 4800 batch: 1.28739\n",
      "val loss in 2 epoch: 0.79780\n",
      "train loss in 2 epoch in 5000 batch: 0.32774\n",
      "val loss in 2 epoch: 0.79681\n",
      "train loss in 2 epoch in 5200 batch: 0.39789\n",
      "val loss in 2 epoch: 0.79640\n",
      "train loss in 2 epoch in 5400 batch: 0.90813\n",
      "val loss in 2 epoch: 0.79666\n",
      "train loss in 2 epoch in 5600 batch: 0.62624\n",
      "val loss in 2 epoch: 0.80448\n",
      "train loss in 2 epoch in 5800 batch: 0.80056\n",
      "val loss in 2 epoch: 0.79622\n",
      "train loss in 2 epoch in 6000 batch: 0.28553\n",
      "val loss in 2 epoch: 0.79669\n",
      "train loss in 2 epoch in 6200 batch: 0.27666\n",
      "val loss in 2 epoch: 0.79611\n",
      "train loss in 2 epoch in 6400 batch: 1.19354\n",
      "val loss in 2 epoch: 0.79866\n",
      "train loss in 2 epoch in 6600 batch: 0.50446\n",
      "val loss in 2 epoch: 0.79966\n",
      "train loss in 2 epoch in 6800 batch: 0.05375\n",
      "val loss in 2 epoch: 0.79857\n",
      "train loss in 2 epoch in 7000 batch: 0.03474\n",
      "val loss in 2 epoch: 0.79889\n",
      "train loss in 2 epoch in 7200 batch: 1.17635\n",
      "val loss in 2 epoch: 0.79487\n",
      "train loss in 2 epoch in 7400 batch: 0.53416\n",
      "val loss in 2 epoch: 0.79582\n",
      "train loss in 2 epoch in 7600 batch: 1.09635\n",
      "val loss in 2 epoch: 0.80237\n",
      "train loss in 2 epoch in 7800 batch: 0.74319\n",
      "val loss in 2 epoch: 0.79556\n",
      "train loss in 2 epoch in 8000 batch: 0.79576\n",
      "val loss in 2 epoch: 0.79579\n",
      "train loss in 2 epoch in 8200 batch: 0.27082\n",
      "val loss in 2 epoch: 0.79678\n",
      "train loss in 2 epoch in 8400 batch: 0.54332\n",
      "val loss in 2 epoch: 0.80042\n",
      "train loss in 2 epoch in 8600 batch: 0.64103\n",
      "val loss in 2 epoch: 0.79520\n",
      "train loss in 2 epoch in 8800 batch: 0.85310\n",
      "val loss in 2 epoch: 0.79327\n",
      "train loss in 2 epoch in 9000 batch: 0.82921\n",
      "val loss in 2 epoch: 0.79743\n",
      "train loss in 2 epoch in 9200 batch: 0.38652\n",
      "val loss in 2 epoch: 0.79442\n",
      "train loss in 2 epoch in 9400 batch: 0.84000\n",
      "val loss in 2 epoch: 0.79615\n",
      "train loss in 2 epoch in 9600 batch: 1.27995\n",
      "val loss in 2 epoch: 0.79515\n",
      "train loss in 2 epoch in 9800 batch: 0.98984\n",
      "val loss in 2 epoch: 0.79762\n",
      "train loss in 2 epoch in 10000 batch: 1.36375\n",
      "val loss in 2 epoch: 0.79531\n",
      "train loss in 2 epoch in 10200 batch: 0.65100\n",
      "val loss in 2 epoch: 0.79458\n",
      "train loss in 2 epoch in 10400 batch: 0.52457\n",
      "val loss in 2 epoch: 0.79387\n",
      "train loss in 2 epoch in 10600 batch: 0.65892\n",
      "val loss in 2 epoch: 0.79555\n",
      "train loss in 2 epoch in 10800 batch: 0.77061\n",
      "val loss in 2 epoch: 0.79603\n",
      "train loss in 2 epoch in 11000 batch: 0.47983\n",
      "val loss in 2 epoch: 0.79466\n",
      "train loss in 2 epoch in 11200 batch: 1.02517\n",
      "val loss in 2 epoch: 0.79342\n",
      "train loss in 2 epoch in 11400 batch: 1.05567\n",
      "val loss in 2 epoch: 0.79311\n",
      "train loss in 2 epoch in 11600 batch: 1.88808\n",
      "val loss in 2 epoch: 0.79432\n",
      "train loss in 2 epoch in 11800 batch: 1.01856\n",
      "val loss in 2 epoch: 0.79456\n",
      "train loss in 2 epoch in 12000 batch: 1.25159\n",
      "val loss in 2 epoch: 0.79359\n",
      "train loss in 2 epoch in 12200 batch: 1.03291\n",
      "val loss in 2 epoch: 0.79301\n",
      "train loss in 2 epoch in 12400 batch: 1.41939\n",
      "val loss in 2 epoch: 0.79321\n",
      "train loss in 2 epoch in 12600 batch: 0.44041\n",
      "val loss in 2 epoch: 0.79240\n",
      "train loss in 2 epoch in 12800 batch: 1.06178\n",
      "val loss in 2 epoch: 0.79251\n",
      "train loss in 2 epoch in 13000 batch: 1.15174\n",
      "val loss in 2 epoch: 0.79180\n",
      "train loss in 2 epoch in 13200 batch: 0.72937\n",
      "val loss in 2 epoch: 0.79303\n",
      "train loss in 2 epoch in 13400 batch: 0.17494\n",
      "val loss in 2 epoch: 0.79439\n",
      "train loss in 2 epoch in 13600 batch: 1.21738\n",
      "val loss in 2 epoch: 0.79156\n",
      "train loss in 2 epoch in 13800 batch: 1.05572\n",
      "val loss in 2 epoch: 0.79282\n",
      "train loss in 2 epoch in 14000 batch: 1.26560\n",
      "val loss in 2 epoch: 0.79144\n",
      "train loss in 2 epoch in 14200 batch: 0.91818\n",
      "val loss in 2 epoch: 0.79396\n",
      "train loss in 2 epoch in 14400 batch: 1.06419\n",
      "val loss in 2 epoch: 0.79394\n",
      "train loss in 2 epoch in 14600 batch: 1.62818\n",
      "val loss in 2 epoch: 0.79140\n",
      "train loss in 2 epoch in 14800 batch: 0.54459\n",
      "val loss in 2 epoch: 0.79159\n",
      "train loss in 2 epoch in 15000 batch: 0.59332\n",
      "val loss in 2 epoch: 0.79102\n",
      "train loss in 2 epoch in 15200 batch: 0.40396\n",
      "val loss in 2 epoch: 0.79351\n",
      "train loss in 2 epoch in 15400 batch: 0.38690\n",
      "val loss in 2 epoch: 0.79099\n",
      "train loss in 2 epoch in 15600 batch: 0.53314\n",
      "val loss in 2 epoch: 0.79151\n",
      "train loss in 2 epoch in 15800 batch: 0.35253\n",
      "val loss in 2 epoch: 0.79332\n",
      "train loss in 2 epoch in 16000 batch: 1.99928\n",
      "val loss in 2 epoch: 0.79373\n",
      "train loss in 2 epoch in 16200 batch: 0.60240\n",
      "val loss in 2 epoch: 0.79369\n",
      "train loss in 2 epoch in 16400 batch: 0.06506\n",
      "val loss in 2 epoch: 0.79184\n",
      "train loss in 2 epoch in 16600 batch: 0.47103\n",
      "val loss in 2 epoch: 0.79057\n",
      "train loss in 2 epoch in 16800 batch: 0.84543\n",
      "val loss in 2 epoch: 0.79007\n",
      "train loss in 2 epoch in 17000 batch: 1.37702\n",
      "val loss in 2 epoch: 0.79153\n",
      "train loss in 2 epoch in 17200 batch: 0.77148\n",
      "val loss in 2 epoch: 0.79122\n",
      "train loss in 2 epoch in 17400 batch: 0.59051\n",
      "val loss in 2 epoch: 0.79339\n",
      "train loss in 2 epoch in 17600 batch: 0.50681\n",
      "val loss in 2 epoch: 0.79170\n",
      "train loss in 2 epoch in 17800 batch: 0.71048\n",
      "val loss in 2 epoch: 0.79288\n",
      "train loss in 2 epoch in 18000 batch: 1.23417\n",
      "val loss in 2 epoch: 0.79098\n",
      "train loss in 2 epoch in 18200 batch: 0.91314\n",
      "val loss in 2 epoch: 0.78903\n",
      "train loss in 2 epoch in 18400 batch: 0.62760\n",
      "val loss in 2 epoch: 0.78971\n",
      "train loss in 2 epoch in 18600 batch: 0.69062\n",
      "val loss in 2 epoch: 0.79012\n",
      "train loss in 2 epoch in 18800 batch: 0.59221\n",
      "val loss in 2 epoch: 0.79140\n",
      "train loss in 2 epoch in 19000 batch: 1.72636\n",
      "val loss in 2 epoch: 0.79122\n",
      "train loss in 2 epoch in 19200 batch: 1.17631\n",
      "val loss in 2 epoch: 0.78893\n",
      "train loss in 2 epoch in 19400 batch: 1.72856\n",
      "val loss in 2 epoch: 0.79150\n",
      "train loss in 2 epoch in 19600 batch: 0.65048\n",
      "val loss in 2 epoch: 0.79028\n",
      "train loss in 2 epoch in 19800 batch: 0.58485\n",
      "val loss in 2 epoch: 0.78908\n",
      "train loss in 2 epoch in 20000 batch: 0.66233\n",
      "val loss in 2 epoch: 0.79283\n",
      "train loss in 2 epoch in 20200 batch: 1.01859\n",
      "val loss in 2 epoch: 0.79017\n",
      "train loss in 2 epoch in 20400 batch: 1.09773\n",
      "val loss in 2 epoch: 0.79151\n",
      "train loss in 2 epoch in 20600 batch: 1.01917\n",
      "val loss in 2 epoch: 0.79076\n",
      "train loss in 2 epoch in 20800 batch: 0.90901\n",
      "val loss in 2 epoch: 0.78818\n",
      "train loss in 2 epoch in 21000 batch: 0.19684\n",
      "val loss in 2 epoch: 0.78925\n",
      "train loss in 2 epoch in 21200 batch: 0.48626\n",
      "val loss in 2 epoch: 0.78801\n",
      "train loss in 2 epoch in 21400 batch: 0.48674\n",
      "val loss in 2 epoch: 0.78831\n",
      "train loss in 2 epoch in 21600 batch: 0.21481\n",
      "val loss in 2 epoch: 0.78902\n",
      "train loss in 2 epoch in 21800 batch: 1.36985\n",
      "val loss in 2 epoch: 0.78873\n",
      "train loss in 2 epoch in 22000 batch: 0.97791\n",
      "val loss in 2 epoch: 0.79262\n",
      "train loss in 2 epoch in 22200 batch: 1.21169\n",
      "val loss in 2 epoch: 0.79184\n",
      "train loss in 2 epoch in 22400 batch: 0.33406\n",
      "val loss in 2 epoch: 0.79002\n",
      "train loss in 2 epoch in 22600 batch: 0.09505\n",
      "val loss in 2 epoch: 0.78925\n",
      "train loss in 2 epoch in 22800 batch: 0.86987\n",
      "val loss in 2 epoch: 0.78812\n",
      "train loss in 2 epoch in 23000 batch: 1.67225\n",
      "val loss in 2 epoch: 0.78852\n",
      "train loss in 2 epoch in 23200 batch: 0.60686\n",
      "val loss in 2 epoch: 0.79137\n",
      "train loss in 2 epoch in 23400 batch: 0.62171\n",
      "val loss in 2 epoch: 0.79116\n",
      "train loss in 2 epoch in 23600 batch: 1.56074\n",
      "val loss in 2 epoch: 0.78815\n",
      "train loss in 2 epoch in 23800 batch: 0.85633\n",
      "val loss in 2 epoch: 0.78954\n",
      "train loss in 2 epoch in 24000 batch: 0.20772\n",
      "val loss in 2 epoch: 0.78896\n",
      "train loss in 2 epoch in 24200 batch: 0.94080\n",
      "val loss in 2 epoch: 0.78807\n",
      "train loss in 2 epoch in 24400 batch: 0.83056\n",
      "val loss in 2 epoch: 0.79041\n",
      "train loss in 2 epoch in 24600 batch: 1.19152\n",
      "val loss in 2 epoch: 0.79096\n",
      "train loss in 2 epoch in 24800 batch: 0.03176\n",
      "val loss in 2 epoch: 0.78733\n",
      "train loss in 2 epoch in 25000 batch: 0.24844\n",
      "val loss in 2 epoch: 0.78998\n",
      "train loss in 2 epoch in 25200 batch: 1.12251\n",
      "val loss in 2 epoch: 0.78926\n",
      "train loss in 2 epoch in 25400 batch: 0.83239\n",
      "val loss in 2 epoch: 0.78819\n",
      "train loss in 2 epoch in 25600 batch: 1.18204\n",
      "val loss in 2 epoch: 0.79049\n",
      "train loss in 2 epoch in 25800 batch: 0.34998\n",
      "val loss in 2 epoch: 0.79068\n",
      "train loss in 2 epoch in 26000 batch: 1.61906\n",
      "val loss in 2 epoch: 0.78943\n",
      "train loss in 2 epoch in 26200 batch: 1.05648\n",
      "val loss in 2 epoch: 0.78843\n",
      "train loss in 2 epoch in 26400 batch: 1.41015\n",
      "val loss in 2 epoch: 0.78801\n",
      "train loss in 2 epoch in 26600 batch: 1.59547\n",
      "val loss in 2 epoch: 0.79376\n",
      "train loss in 2 epoch in 26800 batch: 0.56789\n",
      "val loss in 2 epoch: 0.78852\n",
      "train loss in 2 epoch in 27000 batch: 1.34113\n",
      "val loss in 2 epoch: 0.78701\n",
      "train loss in 2 epoch in 27200 batch: 0.84655\n",
      "val loss in 2 epoch: 0.78736\n",
      "train loss in 2 epoch in 27400 batch: 0.60714\n",
      "val loss in 2 epoch: 0.78974\n",
      "train loss in 2 epoch in 27600 batch: 0.84343\n",
      "val loss in 2 epoch: 0.79098\n",
      "train loss in 2 epoch in 27800 batch: 0.55514\n",
      "val loss in 2 epoch: 0.78819\n",
      "train loss in 2 epoch in 28000 batch: 0.50383\n",
      "val loss in 2 epoch: 0.78768\n",
      "train loss in 2 epoch in 28200 batch: 0.31029\n",
      "val loss in 2 epoch: 0.78964\n",
      "train loss in 2 epoch in 28400 batch: 1.12930\n",
      "val loss in 2 epoch: 0.78721\n",
      "train loss in 2 epoch in 28600 batch: 1.28596\n",
      "val loss in 2 epoch: 0.79004\n",
      "train loss in 2 epoch in 28800 batch: 0.53222\n",
      "val loss in 2 epoch: 0.78699\n",
      "train loss in 2 epoch in 29000 batch: 0.63273\n",
      "val loss in 2 epoch: 0.78790\n",
      "train loss in 2 epoch in 29200 batch: 1.06323\n",
      "val loss in 2 epoch: 0.79180\n",
      "train loss in 2 epoch in 29400 batch: 1.56652\n",
      "val loss in 2 epoch: 0.78672\n",
      "train loss in 2 epoch in 29600 batch: 1.22477\n",
      "val loss in 2 epoch: 0.78669\n",
      "train loss in 2 epoch in 29800 batch: 1.15527\n",
      "val loss in 2 epoch: 0.78817\n",
      "train loss in 2 epoch in 30000 batch: 0.31552\n",
      "val loss in 2 epoch: 0.78614\n",
      "train loss in 2 epoch in 30200 batch: 0.53533\n",
      "val loss in 2 epoch: 0.78681\n",
      "train loss in 2 epoch in 30400 batch: 0.87830\n",
      "val loss in 2 epoch: 0.78975\n",
      "train loss in 2 epoch in 30600 batch: 0.62949\n",
      "val loss in 2 epoch: 0.78873\n",
      "train loss in 2 epoch in 30800 batch: 1.35541\n",
      "val loss in 2 epoch: 0.78794\n",
      "train loss in 2 epoch in 31000 batch: 0.54984\n",
      "val loss in 2 epoch: 0.78775\n",
      "train loss in 2 epoch in 31200 batch: 0.65100\n",
      "val loss in 2 epoch: 0.78946\n",
      "train loss in 2 epoch in 31400 batch: 1.44900\n",
      "val loss in 2 epoch: 0.78726\n",
      "train loss in 3 epoch in 200 batch: 1.26853\n",
      "val loss in 3 epoch: 0.79192\n",
      "train loss in 3 epoch in 400 batch: 0.57605\n",
      "val loss in 3 epoch: 0.78913\n",
      "train loss in 3 epoch in 600 batch: 1.14203\n",
      "val loss in 3 epoch: 0.78757\n",
      "train loss in 3 epoch in 800 batch: 0.38943\n",
      "val loss in 3 epoch: 0.78787\n",
      "train loss in 3 epoch in 1000 batch: 0.55086\n",
      "val loss in 3 epoch: 0.78977\n",
      "train loss in 3 epoch in 1200 batch: 1.09833\n",
      "val loss in 3 epoch: 0.78715\n",
      "train loss in 3 epoch in 1400 batch: 0.94139\n",
      "val loss in 3 epoch: 0.78817\n",
      "train loss in 3 epoch in 1600 batch: 0.24862\n",
      "val loss in 3 epoch: 0.78771\n",
      "train loss in 3 epoch in 1800 batch: 0.59306\n",
      "val loss in 3 epoch: 0.78789\n",
      "train loss in 3 epoch in 2000 batch: 0.55188\n",
      "val loss in 3 epoch: 0.78788\n",
      "train loss in 3 epoch in 2200 batch: 1.06701\n",
      "val loss in 3 epoch: 0.79541\n",
      "train loss in 3 epoch in 2400 batch: 1.32465\n",
      "val loss in 3 epoch: 0.78718\n",
      "train loss in 3 epoch in 2600 batch: 0.91433\n",
      "val loss in 3 epoch: 0.78791\n",
      "train loss in 3 epoch in 2800 batch: 0.01467\n",
      "val loss in 3 epoch: 0.78748\n",
      "train loss in 3 epoch in 3000 batch: 1.19182\n",
      "val loss in 3 epoch: 0.78774\n",
      "train loss in 3 epoch in 3200 batch: 0.50392\n",
      "val loss in 3 epoch: 0.78694\n",
      "train loss in 3 epoch in 3400 batch: 1.34387\n",
      "val loss in 3 epoch: 0.78722\n",
      "train loss in 3 epoch in 3600 batch: 0.98414\n",
      "val loss in 3 epoch: 0.78768\n",
      "train loss in 3 epoch in 3800 batch: 1.19956\n",
      "val loss in 3 epoch: 0.79000\n",
      "train loss in 3 epoch in 4000 batch: 0.76669\n",
      "val loss in 3 epoch: 0.78735\n",
      "train loss in 3 epoch in 4200 batch: 0.72426\n",
      "val loss in 3 epoch: 0.78866\n",
      "train loss in 3 epoch in 4400 batch: 0.88034\n",
      "val loss in 3 epoch: 0.78758\n",
      "train loss in 3 epoch in 4600 batch: 0.71473\n",
      "val loss in 3 epoch: 0.78660\n",
      "train loss in 3 epoch in 4800 batch: 0.95004\n",
      "val loss in 3 epoch: 0.78852\n",
      "train loss in 3 epoch in 5000 batch: 0.13175\n",
      "val loss in 3 epoch: 0.78752\n",
      "train loss in 3 epoch in 5200 batch: 0.20575\n",
      "val loss in 3 epoch: 0.78701\n",
      "train loss in 3 epoch in 5400 batch: 0.20123\n",
      "val loss in 3 epoch: 0.78597\n",
      "train loss in 3 epoch in 5600 batch: 0.52956\n",
      "val loss in 3 epoch: 0.78501\n",
      "train loss in 3 epoch in 5800 batch: 0.34500\n",
      "val loss in 3 epoch: 0.78636\n",
      "train loss in 3 epoch in 6000 batch: 1.24297\n",
      "val loss in 3 epoch: 0.78612\n",
      "train loss in 3 epoch in 6200 batch: 0.56946\n",
      "val loss in 3 epoch: 0.78618\n",
      "train loss in 3 epoch in 6400 batch: 0.51528\n",
      "val loss in 3 epoch: 0.78854\n",
      "train loss in 3 epoch in 6600 batch: 0.86445\n",
      "val loss in 3 epoch: 0.78794\n",
      "train loss in 3 epoch in 6800 batch: 0.52478\n",
      "val loss in 3 epoch: 0.78689\n",
      "train loss in 3 epoch in 7000 batch: 0.51441\n",
      "val loss in 3 epoch: 0.78590\n",
      "train loss in 3 epoch in 7200 batch: 0.29145\n",
      "val loss in 3 epoch: 0.78730\n",
      "train loss in 3 epoch in 7400 batch: 0.46901\n",
      "val loss in 3 epoch: 0.79155\n",
      "train loss in 3 epoch in 7600 batch: 0.78680\n",
      "val loss in 3 epoch: 0.78557\n",
      "train loss in 3 epoch in 7800 batch: 0.54642\n",
      "val loss in 3 epoch: 0.78447\n",
      "train loss in 3 epoch in 8000 batch: 0.39097\n",
      "val loss in 3 epoch: 0.78733\n",
      "train loss in 3 epoch in 8200 batch: 0.59272\n",
      "val loss in 3 epoch: 0.78561\n",
      "train loss in 3 epoch in 8400 batch: 0.83053\n",
      "val loss in 3 epoch: 0.78491\n",
      "train loss in 3 epoch in 8600 batch: 0.91041\n",
      "val loss in 3 epoch: 0.78711\n",
      "train loss in 3 epoch in 8800 batch: 0.80475\n",
      "val loss in 3 epoch: 0.78723\n",
      "train loss in 3 epoch in 9000 batch: 0.27945\n",
      "val loss in 3 epoch: 0.78806\n",
      "train loss in 3 epoch in 9200 batch: 0.76060\n",
      "val loss in 3 epoch: 0.78621\n",
      "train loss in 3 epoch in 9400 batch: 1.36988\n",
      "val loss in 3 epoch: 0.78620\n",
      "train loss in 3 epoch in 9600 batch: 0.82952\n",
      "val loss in 3 epoch: 0.78436\n",
      "train loss in 3 epoch in 9800 batch: 1.41054\n",
      "val loss in 3 epoch: 0.78619\n",
      "train loss in 3 epoch in 10000 batch: 1.40810\n",
      "val loss in 3 epoch: 0.78611\n",
      "train loss in 3 epoch in 10200 batch: 0.54936\n",
      "val loss in 3 epoch: 0.78704\n",
      "train loss in 3 epoch in 10400 batch: 1.40163\n",
      "val loss in 3 epoch: 0.78485\n",
      "train loss in 3 epoch in 10600 batch: 0.47233\n",
      "val loss in 3 epoch: 0.78820\n",
      "train loss in 3 epoch in 10800 batch: 1.39534\n",
      "val loss in 3 epoch: 0.78523\n",
      "train loss in 3 epoch in 11000 batch: 0.21444\n",
      "val loss in 3 epoch: 0.78489\n",
      "train loss in 3 epoch in 11200 batch: 0.64261\n",
      "val loss in 3 epoch: 0.78467\n",
      "train loss in 3 epoch in 11400 batch: 0.98742\n",
      "val loss in 3 epoch: 0.78502\n",
      "train loss in 3 epoch in 11600 batch: 0.23942\n",
      "val loss in 3 epoch: 0.78629\n",
      "train loss in 3 epoch in 11800 batch: 1.49239\n",
      "val loss in 3 epoch: 0.78401\n",
      "train loss in 3 epoch in 12000 batch: 0.29676\n",
      "val loss in 3 epoch: 0.78398\n",
      "train loss in 3 epoch in 12200 batch: 0.88439\n",
      "val loss in 3 epoch: 0.78601\n",
      "train loss in 3 epoch in 12400 batch: 0.85059\n",
      "val loss in 3 epoch: 0.78379\n",
      "train loss in 3 epoch in 12600 batch: 1.37787\n",
      "val loss in 3 epoch: 0.78577\n",
      "train loss in 3 epoch in 12800 batch: 0.40600\n",
      "val loss in 3 epoch: 0.78695\n",
      "train loss in 3 epoch in 13000 batch: 1.21839\n",
      "val loss in 3 epoch: 0.78585\n",
      "train loss in 3 epoch in 13200 batch: 0.67499\n",
      "val loss in 3 epoch: 0.78393\n",
      "train loss in 3 epoch in 13400 batch: 0.58870\n",
      "val loss in 3 epoch: 0.78533\n",
      "train loss in 3 epoch in 13600 batch: 0.99866\n",
      "val loss in 3 epoch: 0.78473\n",
      "train loss in 3 epoch in 13800 batch: 0.73250\n",
      "val loss in 3 epoch: 0.78815\n",
      "train loss in 3 epoch in 14000 batch: 0.52228\n",
      "val loss in 3 epoch: 0.78504\n",
      "train loss in 3 epoch in 14200 batch: 0.99016\n",
      "val loss in 3 epoch: 0.78586\n",
      "train loss in 3 epoch in 14400 batch: 0.37696\n",
      "val loss in 3 epoch: 0.78738\n",
      "train loss in 3 epoch in 14600 batch: 1.27831\n",
      "val loss in 3 epoch: 0.78580\n",
      "train loss in 3 epoch in 14800 batch: 0.83930\n",
      "val loss in 3 epoch: 0.78689\n",
      "train loss in 3 epoch in 15000 batch: 0.56800\n",
      "val loss in 3 epoch: 0.78530\n",
      "train loss in 3 epoch in 15200 batch: 0.31139\n",
      "val loss in 3 epoch: 0.78725\n",
      "train loss in 3 epoch in 15400 batch: 0.80771\n",
      "val loss in 3 epoch: 0.79239\n",
      "train loss in 3 epoch in 15600 batch: 0.27973\n",
      "val loss in 3 epoch: 0.78430\n",
      "train loss in 3 epoch in 15800 batch: 1.21446\n",
      "val loss in 3 epoch: 0.78371\n",
      "train loss in 3 epoch in 16000 batch: 0.78199\n",
      "val loss in 3 epoch: 0.78466\n",
      "train loss in 3 epoch in 16200 batch: 0.81865\n",
      "val loss in 3 epoch: 0.78614\n",
      "train loss in 3 epoch in 16400 batch: 1.72968\n",
      "val loss in 3 epoch: 0.78381\n",
      "train loss in 3 epoch in 16600 batch: 0.19619\n",
      "val loss in 3 epoch: 0.78912\n",
      "train loss in 3 epoch in 16800 batch: 1.11342\n",
      "val loss in 3 epoch: 0.78305\n",
      "train loss in 3 epoch in 17000 batch: 0.10149\n",
      "val loss in 3 epoch: 0.78383\n",
      "train loss in 3 epoch in 17200 batch: 0.96099\n",
      "val loss in 3 epoch: 0.78538\n",
      "train loss in 3 epoch in 17400 batch: 0.66861\n",
      "val loss in 3 epoch: 0.78598\n",
      "train loss in 3 epoch in 17600 batch: 0.20360\n",
      "val loss in 3 epoch: 0.78457\n",
      "train loss in 3 epoch in 17800 batch: 0.70049\n",
      "val loss in 3 epoch: 0.78347\n",
      "train loss in 3 epoch in 18000 batch: 0.51308\n",
      "val loss in 3 epoch: 0.78403\n",
      "train loss in 3 epoch in 18200 batch: 0.76664\n",
      "val loss in 3 epoch: 0.78230\n",
      "train loss in 3 epoch in 18400 batch: 1.45324\n",
      "val loss in 3 epoch: 0.78447\n",
      "train loss in 3 epoch in 18600 batch: 0.87754\n",
      "val loss in 3 epoch: 0.78616\n",
      "train loss in 3 epoch in 18800 batch: 1.01555\n",
      "val loss in 3 epoch: 0.78188\n",
      "train loss in 3 epoch in 19000 batch: 0.62375\n",
      "val loss in 3 epoch: 0.78337\n",
      "train loss in 3 epoch in 19200 batch: 0.80464\n",
      "val loss in 3 epoch: 0.78466\n",
      "train loss in 3 epoch in 19400 batch: 0.56308\n",
      "val loss in 3 epoch: 0.78590\n",
      "train loss in 3 epoch in 19600 batch: 0.58449\n",
      "val loss in 3 epoch: 0.78343\n",
      "train loss in 3 epoch in 19800 batch: 0.98485\n",
      "val loss in 3 epoch: 0.78248\n",
      "train loss in 3 epoch in 20000 batch: 0.27564\n",
      "val loss in 3 epoch: 0.78235\n",
      "train loss in 3 epoch in 20200 batch: 1.02540\n",
      "val loss in 3 epoch: 0.78368\n",
      "train loss in 3 epoch in 20400 batch: 0.35126\n",
      "val loss in 3 epoch: 0.78623\n",
      "train loss in 3 epoch in 20600 batch: 0.52933\n",
      "val loss in 3 epoch: 0.78501\n",
      "train loss in 3 epoch in 20800 batch: 2.02798\n",
      "val loss in 3 epoch: 0.78594\n",
      "train loss in 3 epoch in 21000 batch: 1.92222\n",
      "val loss in 3 epoch: 0.78271\n",
      "train loss in 3 epoch in 21200 batch: 0.97814\n",
      "val loss in 3 epoch: 0.78434\n",
      "train loss in 3 epoch in 21400 batch: 1.65286\n",
      "val loss in 3 epoch: 0.78541\n",
      "train loss in 3 epoch in 21600 batch: 1.67706\n",
      "val loss in 3 epoch: 0.78334\n",
      "train loss in 3 epoch in 21800 batch: 0.75105\n",
      "val loss in 3 epoch: 0.78509\n",
      "train loss in 3 epoch in 22000 batch: 0.00107\n",
      "val loss in 3 epoch: 0.78401\n",
      "train loss in 3 epoch in 22200 batch: 1.19444\n",
      "val loss in 3 epoch: 0.78526\n",
      "train loss in 3 epoch in 22400 batch: 0.20624\n",
      "val loss in 3 epoch: 0.78754\n",
      "train loss in 3 epoch in 22600 batch: 0.71765\n",
      "val loss in 3 epoch: 0.78526\n",
      "train loss in 3 epoch in 22800 batch: 1.32422\n",
      "val loss in 3 epoch: 0.78535\n",
      "train loss in 3 epoch in 23000 batch: 0.61973\n",
      "val loss in 3 epoch: 0.78290\n",
      "train loss in 3 epoch in 23200 batch: 0.44477\n",
      "val loss in 3 epoch: 0.78191\n",
      "train loss in 3 epoch in 23400 batch: 0.74130\n",
      "val loss in 3 epoch: 0.78364\n",
      "train loss in 3 epoch in 23600 batch: 0.43423\n",
      "val loss in 3 epoch: 0.78364\n",
      "train loss in 3 epoch in 23800 batch: 0.60590\n",
      "val loss in 3 epoch: 0.78354\n",
      "train loss in 3 epoch in 24000 batch: 0.95677\n",
      "val loss in 3 epoch: 0.78468\n",
      "train loss in 3 epoch in 24200 batch: 0.96113\n",
      "val loss in 3 epoch: 0.78454\n",
      "train loss in 3 epoch in 24400 batch: 1.55493\n",
      "val loss in 3 epoch: 0.78232\n",
      "train loss in 3 epoch in 24600 batch: 0.47629\n",
      "val loss in 3 epoch: 0.78124\n",
      "train loss in 3 epoch in 24800 batch: 0.88854\n",
      "val loss in 3 epoch: 0.78251\n",
      "train loss in 3 epoch in 25000 batch: 0.75460\n",
      "val loss in 3 epoch: 0.78603\n",
      "train loss in 3 epoch in 25200 batch: 0.20481\n",
      "val loss in 3 epoch: 0.78240\n",
      "train loss in 3 epoch in 25400 batch: 1.38622\n",
      "val loss in 3 epoch: 0.78230\n",
      "train loss in 3 epoch in 25600 batch: 0.49621\n",
      "val loss in 3 epoch: 0.78481\n",
      "train loss in 3 epoch in 25800 batch: 1.15626\n",
      "val loss in 3 epoch: 0.78353\n",
      "train loss in 3 epoch in 26000 batch: 1.24173\n",
      "val loss in 3 epoch: 0.78376\n",
      "train loss in 3 epoch in 26200 batch: 0.51916\n",
      "val loss in 3 epoch: 0.78399\n",
      "train loss in 3 epoch in 26400 batch: 0.63869\n",
      "val loss in 3 epoch: 0.78278\n",
      "train loss in 3 epoch in 26600 batch: 0.43032\n",
      "val loss in 3 epoch: 0.78254\n",
      "train loss in 3 epoch in 26800 batch: 1.51948\n",
      "val loss in 3 epoch: 0.78201\n",
      "train loss in 3 epoch in 27000 batch: 0.96507\n",
      "val loss in 3 epoch: 0.78226\n",
      "train loss in 3 epoch in 27200 batch: 1.37534\n",
      "val loss in 3 epoch: 0.78469\n",
      "train loss in 3 epoch in 27400 batch: 0.25414\n",
      "val loss in 3 epoch: 0.78237\n",
      "train loss in 3 epoch in 27600 batch: 1.15406\n",
      "val loss in 3 epoch: 0.78174\n",
      "train loss in 3 epoch in 27800 batch: 0.92673\n",
      "val loss in 3 epoch: 0.78176\n",
      "train loss in 3 epoch in 28000 batch: 0.41270\n",
      "val loss in 3 epoch: 0.78465\n",
      "train loss in 3 epoch in 28200 batch: 0.74020\n",
      "val loss in 3 epoch: 0.78220\n",
      "train loss in 3 epoch in 28400 batch: 0.24176\n",
      "val loss in 3 epoch: 0.78286\n",
      "train loss in 3 epoch in 28600 batch: 1.13957\n",
      "val loss in 3 epoch: 0.78226\n",
      "train loss in 3 epoch in 28800 batch: 0.48615\n",
      "val loss in 3 epoch: 0.78481\n",
      "train loss in 3 epoch in 29000 batch: 0.11307\n",
      "val loss in 3 epoch: 0.78357\n",
      "train loss in 3 epoch in 29200 batch: 0.56385\n",
      "val loss in 3 epoch: 0.78573\n",
      "train loss in 3 epoch in 29400 batch: 0.18423\n",
      "val loss in 3 epoch: 0.78132\n",
      "train loss in 3 epoch in 29600 batch: 1.05048\n",
      "val loss in 3 epoch: 0.78244\n",
      "train loss in 3 epoch in 29800 batch: 0.05192\n",
      "val loss in 3 epoch: 0.78277\n",
      "train loss in 3 epoch in 30000 batch: 0.63910\n",
      "val loss in 3 epoch: 0.78281\n",
      "train loss in 3 epoch in 30200 batch: 1.42046\n",
      "val loss in 3 epoch: 0.78276\n",
      "train loss in 3 epoch in 30400 batch: 0.51270\n",
      "val loss in 3 epoch: 0.78561\n",
      "train loss in 3 epoch in 30600 batch: 1.27910\n",
      "val loss in 3 epoch: 0.78629\n",
      "train loss in 3 epoch in 30800 batch: 1.33030\n",
      "val loss in 3 epoch: 0.78062\n",
      "train loss in 3 epoch in 31000 batch: 0.85197\n",
      "val loss in 3 epoch: 0.78258\n",
      "train loss in 3 epoch in 31200 batch: 0.98710\n",
      "val loss in 3 epoch: 0.78325\n",
      "train loss in 3 epoch in 31400 batch: 1.13541\n",
      "val loss in 3 epoch: 0.78111\n",
      "train loss in 4 epoch in 200 batch: 1.28176\n",
      "val loss in 4 epoch: 0.78980\n",
      "train loss in 4 epoch in 400 batch: 0.78229\n",
      "val loss in 4 epoch: 0.78367\n",
      "train loss in 4 epoch in 600 batch: 1.36656\n",
      "val loss in 4 epoch: 0.78312\n",
      "train loss in 4 epoch in 800 batch: 1.09757\n",
      "val loss in 4 epoch: 0.78109\n",
      "train loss in 4 epoch in 1000 batch: 0.39464\n",
      "val loss in 4 epoch: 0.78215\n",
      "train loss in 4 epoch in 1200 batch: 0.38298\n",
      "val loss in 4 epoch: 0.78193\n",
      "train loss in 4 epoch in 1400 batch: 0.59329\n",
      "val loss in 4 epoch: 0.78265\n",
      "train loss in 4 epoch in 1600 batch: 1.15880\n",
      "val loss in 4 epoch: 0.78094\n",
      "train loss in 4 epoch in 1800 batch: 0.03219\n",
      "val loss in 4 epoch: 0.78276\n",
      "train loss in 4 epoch in 2000 batch: 0.43403\n",
      "val loss in 4 epoch: 0.78269\n",
      "train loss in 4 epoch in 2200 batch: 0.56785\n",
      "val loss in 4 epoch: 0.78221\n",
      "train loss in 4 epoch in 2400 batch: 1.76052\n",
      "val loss in 4 epoch: 0.78071\n",
      "train loss in 4 epoch in 2600 batch: 1.22634\n",
      "val loss in 4 epoch: 0.78330\n",
      "train loss in 4 epoch in 2800 batch: 0.97162\n",
      "val loss in 4 epoch: 0.78157\n",
      "train loss in 4 epoch in 3000 batch: 1.28243\n",
      "val loss in 4 epoch: 0.78192\n",
      "train loss in 4 epoch in 3200 batch: 0.09646\n",
      "val loss in 4 epoch: 0.78425\n",
      "train loss in 4 epoch in 3400 batch: 0.49680\n",
      "val loss in 4 epoch: 0.78154\n",
      "train loss in 4 epoch in 3600 batch: 0.35699\n",
      "val loss in 4 epoch: 0.78505\n",
      "train loss in 4 epoch in 3800 batch: 0.99468\n",
      "val loss in 4 epoch: 0.77986\n",
      "train loss in 4 epoch in 4000 batch: 0.65134\n",
      "val loss in 4 epoch: 0.78287\n",
      "train loss in 4 epoch in 4200 batch: 1.69439\n",
      "val loss in 4 epoch: 0.78185\n",
      "train loss in 4 epoch in 4400 batch: 0.40807\n",
      "val loss in 4 epoch: 0.78040\n",
      "train loss in 4 epoch in 4600 batch: 0.81230\n",
      "val loss in 4 epoch: 0.78074\n",
      "train loss in 4 epoch in 4800 batch: 0.67182\n",
      "val loss in 4 epoch: 0.78499\n",
      "train loss in 4 epoch in 5000 batch: 0.25457\n",
      "val loss in 4 epoch: 0.78305\n",
      "train loss in 4 epoch in 5200 batch: 1.29066\n",
      "val loss in 4 epoch: 0.78321\n",
      "train loss in 4 epoch in 5400 batch: 0.34034\n",
      "val loss in 4 epoch: 0.78135\n",
      "train loss in 4 epoch in 5600 batch: 1.25001\n",
      "val loss in 4 epoch: 0.78129\n",
      "train loss in 4 epoch in 5800 batch: 0.23062\n",
      "val loss in 4 epoch: 0.78107\n",
      "train loss in 4 epoch in 6000 batch: 1.20331\n",
      "val loss in 4 epoch: 0.78321\n",
      "train loss in 4 epoch in 6200 batch: 0.83885\n",
      "val loss in 4 epoch: 0.78144\n",
      "train loss in 4 epoch in 6400 batch: 0.92536\n",
      "val loss in 4 epoch: 0.78395\n",
      "train loss in 4 epoch in 6600 batch: 0.60632\n",
      "val loss in 4 epoch: 0.78510\n",
      "train loss in 4 epoch in 6800 batch: 1.15684\n",
      "val loss in 4 epoch: 0.78224\n",
      "train loss in 4 epoch in 7000 batch: 0.44772\n",
      "val loss in 4 epoch: 0.78203\n",
      "train loss in 4 epoch in 7200 batch: 0.15218\n",
      "val loss in 4 epoch: 0.78258\n",
      "train loss in 4 epoch in 7400 batch: 0.62995\n",
      "val loss in 4 epoch: 0.78290\n",
      "train loss in 4 epoch in 7600 batch: 0.58435\n",
      "val loss in 4 epoch: 0.78027\n",
      "train loss in 4 epoch in 7800 batch: 0.53045\n",
      "val loss in 4 epoch: 0.78373\n",
      "train loss in 4 epoch in 8000 batch: 0.34365\n",
      "val loss in 4 epoch: 0.77932\n",
      "train loss in 4 epoch in 8200 batch: 0.76025\n",
      "val loss in 4 epoch: 0.78079\n",
      "train loss in 4 epoch in 8400 batch: 0.56287\n",
      "val loss in 4 epoch: 0.78319\n",
      "train loss in 4 epoch in 8600 batch: 0.70252\n",
      "val loss in 4 epoch: 0.78285\n",
      "train loss in 4 epoch in 8800 batch: 0.35861\n",
      "val loss in 4 epoch: 0.78120\n",
      "train loss in 4 epoch in 9000 batch: 0.51463\n",
      "val loss in 4 epoch: 0.78420\n",
      "train loss in 4 epoch in 9200 batch: 0.73676\n",
      "val loss in 4 epoch: 0.77994\n",
      "train loss in 4 epoch in 9400 batch: 0.74266\n",
      "val loss in 4 epoch: 0.78295\n",
      "train loss in 4 epoch in 9600 batch: 1.34200\n",
      "val loss in 4 epoch: 0.77955\n",
      "train loss in 4 epoch in 9800 batch: 1.26291\n",
      "val loss in 4 epoch: 0.78018\n",
      "train loss in 4 epoch in 10000 batch: 0.91762\n",
      "val loss in 4 epoch: 0.78188\n",
      "train loss in 4 epoch in 10200 batch: 0.69100\n",
      "val loss in 4 epoch: 0.78048\n",
      "train loss in 4 epoch in 10400 batch: 0.81518\n",
      "val loss in 4 epoch: 0.78126\n",
      "train loss in 4 epoch in 10600 batch: 0.29511\n",
      "val loss in 4 epoch: 0.77914\n",
      "train loss in 4 epoch in 10800 batch: 0.82745\n",
      "val loss in 4 epoch: 0.77959\n",
      "train loss in 4 epoch in 11000 batch: 0.36708\n",
      "val loss in 4 epoch: 0.78043\n",
      "train loss in 4 epoch in 11200 batch: 1.16415\n",
      "val loss in 4 epoch: 0.78149\n",
      "train loss in 4 epoch in 11400 batch: 1.08115\n",
      "val loss in 4 epoch: 0.78397\n",
      "train loss in 4 epoch in 11600 batch: 0.49306\n",
      "val loss in 4 epoch: 0.78449\n",
      "train loss in 4 epoch in 11800 batch: 0.42942\n",
      "val loss in 4 epoch: 0.78011\n",
      "train loss in 4 epoch in 12000 batch: 1.17478\n",
      "val loss in 4 epoch: 0.78005\n",
      "train loss in 4 epoch in 12200 batch: 1.71772\n",
      "val loss in 4 epoch: 0.78075\n",
      "train loss in 4 epoch in 12400 batch: 1.30387\n",
      "val loss in 4 epoch: 0.78071\n",
      "train loss in 4 epoch in 12600 batch: 0.51125\n",
      "val loss in 4 epoch: 0.78045\n",
      "train loss in 4 epoch in 12800 batch: 0.96744\n",
      "val loss in 4 epoch: 0.78186\n",
      "train loss in 4 epoch in 13000 batch: 1.34259\n",
      "val loss in 4 epoch: 0.77908\n",
      "train loss in 4 epoch in 13200 batch: 0.60965\n",
      "val loss in 4 epoch: 0.78124\n",
      "train loss in 4 epoch in 13400 batch: 0.30636\n",
      "val loss in 4 epoch: 0.78242\n",
      "train loss in 4 epoch in 13600 batch: 0.82496\n",
      "val loss in 4 epoch: 0.78081\n",
      "train loss in 4 epoch in 13800 batch: 0.86298\n",
      "val loss in 4 epoch: 0.78119\n",
      "train loss in 4 epoch in 14000 batch: 0.00144\n",
      "val loss in 4 epoch: 0.78114\n",
      "train loss in 4 epoch in 14200 batch: 0.34130\n",
      "val loss in 4 epoch: 0.78293\n",
      "train loss in 4 epoch in 14400 batch: 0.29153\n",
      "val loss in 4 epoch: 0.78017\n",
      "train loss in 4 epoch in 14600 batch: 0.18743\n",
      "val loss in 4 epoch: 0.78048\n",
      "train loss in 4 epoch in 14800 batch: 1.24327\n",
      "val loss in 4 epoch: 0.78224\n",
      "train loss in 4 epoch in 15000 batch: 0.57398\n",
      "val loss in 4 epoch: 0.78125\n",
      "train loss in 4 epoch in 15200 batch: 0.94536\n",
      "val loss in 4 epoch: 0.78124\n",
      "train loss in 4 epoch in 15400 batch: 0.86233\n",
      "val loss in 4 epoch: 0.78323\n",
      "train loss in 4 epoch in 15600 batch: 0.03996\n",
      "val loss in 4 epoch: 0.78393\n",
      "train loss in 4 epoch in 15800 batch: 0.29197\n",
      "val loss in 4 epoch: 0.78367\n",
      "train loss in 4 epoch in 16000 batch: 1.10521\n",
      "val loss in 4 epoch: 0.78476\n",
      "train loss in 4 epoch in 16200 batch: 0.73929\n",
      "val loss in 4 epoch: 0.77976\n",
      "train loss in 4 epoch in 16400 batch: 0.19986\n",
      "val loss in 4 epoch: 0.78219\n",
      "train loss in 4 epoch in 16600 batch: 0.16573\n",
      "val loss in 4 epoch: 0.78387\n",
      "train loss in 4 epoch in 16800 batch: 0.56777\n",
      "val loss in 4 epoch: 0.77957\n",
      "train loss in 4 epoch in 17000 batch: 0.85939\n",
      "val loss in 4 epoch: 0.78117\n",
      "train loss in 4 epoch in 17200 batch: 0.24022\n",
      "val loss in 4 epoch: 0.78137\n",
      "train loss in 4 epoch in 17400 batch: 0.63893\n",
      "val loss in 4 epoch: 0.78664\n",
      "train loss in 4 epoch in 17600 batch: 1.26458\n",
      "val loss in 4 epoch: 0.78067\n",
      "train loss in 4 epoch in 17800 batch: 0.02175\n",
      "val loss in 4 epoch: 0.78131\n",
      "train loss in 4 epoch in 18000 batch: 1.15786\n",
      "val loss in 4 epoch: 0.77719\n",
      "train loss in 4 epoch in 18200 batch: 1.96842\n",
      "val loss in 4 epoch: 0.77990\n",
      "train loss in 4 epoch in 18400 batch: 0.36898\n",
      "val loss in 4 epoch: 0.78226\n",
      "train loss in 4 epoch in 18600 batch: 0.37316\n",
      "val loss in 4 epoch: 0.78038\n",
      "train loss in 4 epoch in 18800 batch: 0.69221\n",
      "val loss in 4 epoch: 0.77999\n",
      "train loss in 4 epoch in 19000 batch: 1.22399\n",
      "val loss in 4 epoch: 0.77880\n",
      "train loss in 4 epoch in 19200 batch: 0.54162\n",
      "val loss in 4 epoch: 0.77875\n",
      "train loss in 4 epoch in 19400 batch: 0.03511\n",
      "val loss in 4 epoch: 0.78032\n",
      "train loss in 4 epoch in 19600 batch: 0.09712\n",
      "val loss in 4 epoch: 0.77903\n",
      "train loss in 4 epoch in 19800 batch: 0.40981\n",
      "val loss in 4 epoch: 0.77945\n",
      "train loss in 4 epoch in 20000 batch: 0.24340\n",
      "val loss in 4 epoch: 0.77872\n",
      "train loss in 4 epoch in 20200 batch: 0.22542\n",
      "val loss in 4 epoch: 0.77959\n",
      "train loss in 4 epoch in 20400 batch: 0.78973\n",
      "val loss in 4 epoch: 0.78113\n",
      "train loss in 4 epoch in 20600 batch: 0.81908\n",
      "val loss in 4 epoch: 0.77906\n",
      "train loss in 4 epoch in 20800 batch: 0.31467\n",
      "val loss in 4 epoch: 0.78179\n",
      "train loss in 4 epoch in 21000 batch: 0.69304\n",
      "val loss in 4 epoch: 0.78223\n",
      "train loss in 4 epoch in 21200 batch: 1.04774\n",
      "val loss in 4 epoch: 0.78170\n",
      "train loss in 4 epoch in 21400 batch: 0.00266\n",
      "val loss in 4 epoch: 0.77963\n",
      "train loss in 4 epoch in 21600 batch: 0.83450\n",
      "val loss in 4 epoch: 0.78348\n",
      "train loss in 4 epoch in 21800 batch: 0.85464\n",
      "val loss in 4 epoch: 0.77779\n",
      "train loss in 4 epoch in 22000 batch: 0.85557\n",
      "val loss in 4 epoch: 0.77879\n",
      "train loss in 4 epoch in 22200 batch: 0.59390\n",
      "val loss in 4 epoch: 0.78049\n",
      "train loss in 4 epoch in 22400 batch: 0.89272\n",
      "val loss in 4 epoch: 0.78149\n",
      "train loss in 4 epoch in 22600 batch: 1.23417\n",
      "val loss in 4 epoch: 0.77696\n",
      "train loss in 4 epoch in 22800 batch: 0.74333\n",
      "val loss in 4 epoch: 0.78111\n",
      "train loss in 4 epoch in 23000 batch: 1.10022\n",
      "val loss in 4 epoch: 0.77870\n",
      "train loss in 4 epoch in 23200 batch: 0.43460\n",
      "val loss in 4 epoch: 0.77950\n",
      "train loss in 4 epoch in 23400 batch: 0.49639\n",
      "val loss in 4 epoch: 0.78094\n",
      "train loss in 4 epoch in 23600 batch: 1.00250\n",
      "val loss in 4 epoch: 0.78316\n",
      "train loss in 4 epoch in 23800 batch: 0.25988\n",
      "val loss in 4 epoch: 0.77875\n",
      "train loss in 4 epoch in 24000 batch: 0.26045\n",
      "val loss in 4 epoch: 0.77844\n",
      "train loss in 4 epoch in 24200 batch: 0.00014\n",
      "val loss in 4 epoch: 0.77981\n",
      "train loss in 4 epoch in 24400 batch: 1.35928\n",
      "val loss in 4 epoch: 0.78093\n",
      "train loss in 4 epoch in 24600 batch: 0.46986\n",
      "val loss in 4 epoch: 0.77850\n",
      "train loss in 4 epoch in 24800 batch: 0.01669\n",
      "val loss in 4 epoch: 0.78295\n",
      "train loss in 4 epoch in 25000 batch: 0.31968\n",
      "val loss in 4 epoch: 0.77930\n",
      "train loss in 4 epoch in 25200 batch: 0.73481\n",
      "val loss in 4 epoch: 0.77752\n",
      "train loss in 4 epoch in 25400 batch: 1.48365\n",
      "val loss in 4 epoch: 0.78275\n",
      "train loss in 4 epoch in 25600 batch: 1.38654\n",
      "val loss in 4 epoch: 0.77989\n",
      "train loss in 4 epoch in 25800 batch: 0.62844\n",
      "val loss in 4 epoch: 0.77766\n",
      "train loss in 4 epoch in 26000 batch: 1.34298\n",
      "val loss in 4 epoch: 0.78222\n",
      "train loss in 4 epoch in 26200 batch: 1.06038\n",
      "val loss in 4 epoch: 0.77765\n",
      "train loss in 4 epoch in 26400 batch: 0.59884\n",
      "val loss in 4 epoch: 0.77800\n",
      "train loss in 4 epoch in 26600 batch: 0.95720\n",
      "val loss in 4 epoch: 0.77978\n",
      "train loss in 4 epoch in 26800 batch: 0.63751\n",
      "val loss in 4 epoch: 0.77996\n",
      "train loss in 4 epoch in 27000 batch: 0.37692\n",
      "val loss in 4 epoch: 0.78113\n",
      "train loss in 4 epoch in 27200 batch: 1.03218\n",
      "val loss in 4 epoch: 0.77804\n",
      "train loss in 4 epoch in 27400 batch: 0.27304\n",
      "val loss in 4 epoch: 0.77764\n",
      "train loss in 4 epoch in 27600 batch: 1.58922\n",
      "val loss in 4 epoch: 0.77861\n",
      "train loss in 4 epoch in 27800 batch: 0.95238\n",
      "val loss in 4 epoch: 0.77810\n",
      "train loss in 4 epoch in 28000 batch: 0.60152\n",
      "val loss in 4 epoch: 0.78008\n",
      "train loss in 4 epoch in 28200 batch: 0.35463\n",
      "val loss in 4 epoch: 0.77839\n",
      "train loss in 4 epoch in 28400 batch: 1.00087\n",
      "val loss in 4 epoch: 0.78017\n",
      "train loss in 4 epoch in 28600 batch: 1.01900\n",
      "val loss in 4 epoch: 0.77801\n",
      "train loss in 4 epoch in 28800 batch: 0.20061\n",
      "val loss in 4 epoch: 0.78029\n",
      "train loss in 4 epoch in 29000 batch: 1.09915\n",
      "val loss in 4 epoch: 0.77889\n",
      "train loss in 4 epoch in 29200 batch: 1.05820\n",
      "val loss in 4 epoch: 0.77939\n",
      "train loss in 4 epoch in 29400 batch: 0.59115\n",
      "val loss in 4 epoch: 0.77661\n",
      "train loss in 4 epoch in 29600 batch: 0.89467\n",
      "val loss in 4 epoch: 0.77767\n",
      "train loss in 4 epoch in 29800 batch: 1.18833\n",
      "val loss in 4 epoch: 0.78037\n",
      "train loss in 4 epoch in 30000 batch: 0.16240\n",
      "val loss in 4 epoch: 0.77751\n",
      "train loss in 4 epoch in 30200 batch: 1.00531\n",
      "val loss in 4 epoch: 0.77781\n",
      "train loss in 4 epoch in 30400 batch: 0.87211\n",
      "val loss in 4 epoch: 0.77812\n",
      "train loss in 4 epoch in 30600 batch: 1.08234\n",
      "val loss in 4 epoch: 0.77717\n",
      "train loss in 4 epoch in 30800 batch: 0.24518\n",
      "val loss in 4 epoch: 0.77744\n",
      "train loss in 4 epoch in 31000 batch: 0.97532\n",
      "val loss in 4 epoch: 0.77979\n",
      "train loss in 4 epoch in 31200 batch: 1.32957\n",
      "val loss in 4 epoch: 0.77750\n",
      "train loss in 4 epoch in 31400 batch: 1.06951\n",
      "val loss in 4 epoch: 0.77986\n"
     ]
    }
   ],
   "source": [
    "# v22 - like v15 but x2 data\n",
    "INPUT_F = 36\n",
    "H = 256\n",
    "DROP_P = 0.1\n",
    "\n",
    "model = NNWithCusomFeatures2(INPUT_F, DROP_P, H)\n",
    "\n",
    "def policy_gradient(logits_batch, rewards_batch, actions_batch, _):\n",
    "    loss = nn.CrossEntropyLoss(reduction='none')\n",
    "    logits_batch = torch.reshape(logits_batch, (-1, 100))\n",
    "    policy = loss(logits_batch, actions_batch)\n",
    "    out = torch.mean(policy * rewards_batch)\n",
    "    return out\n",
    "\n",
    "learn(data, nagiss_top_10[1], model,\n",
    "      freq=200, batch_size=16,lr=1e-4, epochs=4, criterion=policy_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'models/nagiss_v22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607.0 571.0 tmp/b_0.11312095728679927.py tmp/b_0.9546663769656143.py\n",
      "683.0 610.0 tmp/b_0.11312095728679927.py tmp/b_0.9546663769656143.py\n",
      "563.0 528.0 tmp/b_0.11312095728679927.py tmp/b_0.9546663769656143.py\n",
      "628.0 635.0 tmp/b_0.11312095728679927.py tmp/b_0.9546663769656143.py\n",
      "616.0 626.0 tmp/b_0.11312095728679927.py tmp/b_0.9546663769656143.py\n",
      "658.0 603.0 tmp/b_0.11312095728679927.py tmp/b_0.9546663769656143.py\n",
      "677.0 670.0 tmp/b_0.11312095728679927.py tmp/b_0.9546663769656143.py\n",
      "623.0 554.0 tmp/b_0.11312095728679927.py tmp/b_0.9546663769656143.py\n",
      "661.0 636.0 tmp/b_0.11312095728679927.py tmp/b_0.9546663769656143.py\n",
      "706.0 669.0 tmp/b_0.11312095728679927.py tmp/b_0.9546663769656143.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.00023920097700547846,\n",
       " 32.0,\n",
       " 27.546324618721822,\n",
       " 0.8,\n",
       " 'tmp/b_0.11312095728679927.py',\n",
       " 'tmp/b_0.9546663769656143.py',\n",
       " array([607., 683., 563., 628., 616., 658., 677., 623., 661., 706.]),\n",
       " array([571., 610., 528., 635., 626., 603., 670., 554., 636., 669.]))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v22\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,False\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=False\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "607.0 568.0 tmp/b_0.41180196492681487.py tmp/b_0.2274189914325433.py\n",
      "590.0 592.0 tmp/b_0.41180196492681487.py tmp/b_0.2274189914325433.py\n",
      "702.0 644.0 tmp/b_0.41180196492681487.py tmp/b_0.2274189914325433.py\n",
      "659.0 633.0 tmp/b_0.41180196492681487.py tmp/b_0.2274189914325433.py\n",
      "661.0 643.0 tmp/b_0.41180196492681487.py tmp/b_0.2274189914325433.py\n",
      "624.0 578.0 tmp/b_0.41180196492681487.py tmp/b_0.2274189914325433.py\n",
      "704.0 683.0 tmp/b_0.41180196492681487.py tmp/b_0.2274189914325433.py\n",
      "704.0 687.0 tmp/b_0.41180196492681487.py tmp/b_0.2274189914325433.py\n",
      "671.0 609.0 tmp/b_0.41180196492681487.py tmp/b_0.2274189914325433.py\n",
      "676.0 664.0 tmp/b_0.41180196492681487.py tmp/b_0.2274189914325433.py\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.989122719632633e-06,\n",
       " 29.7,\n",
       " 19.753733824267247,\n",
       " 0.9,\n",
       " 'tmp/b_0.41180196492681487.py',\n",
       " 'tmp/b_0.2274189914325433.py',\n",
       " array([607., 590., 702., 659., 661., 624., 704., 704., 671., 676.]),\n",
       " array([568., 592., 644., 633., 643., 578., 683., 687., 609., 664.]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# v22 vs neural with mean\n",
    "neural_with_nagiss = utils.bandits.neural.format(\"use_only_nagiss,use_mean=True,False\", \"\")\n",
    "neural_default = utils.bandits.neural.format(\"use_mean=True\", \"\")\n",
    "utils.bandits.compare(\n",
    "    utils.bandits.Agent(text=neural_with_nagiss),\n",
    "    utils.bandits.Agent(text=neural_default),\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
